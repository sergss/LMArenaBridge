# api_server.py
# æ–°ä¸€ä»£ LMArena Bridge åç«¯æœåŠ¡

import asyncio
import json
import logging
import os
import sys
import subprocess
import time
import uuid
import re
from contextlib import asynccontextmanager

import uvicorn
import requests
from packaging.version import parse as parse_version
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse, Response

# --- åŸºç¡€é…ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- å…¨å±€çŠ¶æ€ä¸é…ç½® ---
CONFIG = {} # å­˜å‚¨ä» config.jsonc åŠ è½½çš„é…ç½®
# browser_ws ç”¨äºå­˜å‚¨ä¸å•ä¸ªæ²¹çŒ´è„šæœ¬çš„ WebSocket è¿æ¥ã€‚
# æ³¨æ„ï¼šæ­¤æ¶æ„å‡å®šåªæœ‰ä¸€ä¸ªæµè§ˆå™¨æ ‡ç­¾é¡µåœ¨å·¥ä½œã€‚
# å¦‚æœéœ€è¦æ”¯æŒå¤šä¸ªå¹¶å‘æ ‡ç­¾é¡µï¼Œéœ€è¦å°†æ­¤æ‰©å±•ä¸ºå­—å…¸ç®¡ç†å¤šä¸ªè¿æ¥ã€‚
browser_ws: WebSocket | None = None
# response_channels ç”¨äºå­˜å‚¨æ¯ä¸ª API è¯·æ±‚çš„å“åº”é˜Ÿåˆ—ã€‚
# é”®æ˜¯ request_idï¼Œå€¼æ˜¯ asyncio.Queueã€‚
response_channels: dict[str, asyncio.Queue] = {}

# --- æ¨¡å‹æ˜ å°„ ---
MODEL_NAME_TO_ID_MAP = {}
DEFAULT_MODEL_ID = "f44e280a-7914-43ca-a25d-ecfcc5d48d09" # é»˜è®¤æ¨¡å‹: Claude 3.5 Sonnet

def load_config():
    """ä» config.jsonc åŠ è½½é…ç½®ï¼Œå¹¶å¤„ç† JSONC æ³¨é‡Šã€‚"""
    global CONFIG
    try:
        with open('config.jsonc', 'r', encoding='utf-8') as f:
            content = f.read()
            # ç§»é™¤ // è¡Œæ³¨é‡Šå’Œ /* */ å—æ³¨é‡Š
            json_content = re.sub(r'//.*', '', content)
            json_content = re.sub(r'/\*.*?\*/', '', json_content, flags=re.DOTALL)
            CONFIG = json.loads(json_content)
        logger.info("æˆåŠŸä» 'config.jsonc' åŠ è½½é…ç½®ã€‚")
        # æ‰“å°å…³é”®é…ç½®çŠ¶æ€
        logger.info(f"  - é…’é¦†æ¨¡å¼ (Tavern Mode): {'âœ… å¯ç”¨' if CONFIG.get('tavern_mode_enabled') else 'âŒ ç¦ç”¨'}")
        logger.info(f"  - ç»•è¿‡æ¨¡å¼ (Bypass Mode): {'âœ… å¯ç”¨' if CONFIG.get('bypass_enabled') else 'âŒ ç¦ç”¨'}")
    except (FileNotFoundError, json.JSONDecodeError) as e:
        logger.error(f"åŠ è½½æˆ–è§£æ 'config.jsonc' å¤±è´¥: {e}ã€‚å°†ä½¿ç”¨é»˜è®¤é…ç½®ã€‚")
        CONFIG = {}

def load_model_map():
    """ä» models.json åŠ è½½æ¨¡å‹æ˜ å°„ã€‚"""
    global MODEL_NAME_TO_ID_MAP
    try:
        with open('models.json', 'r', encoding='utf-8') as f:
            MODEL_NAME_TO_ID_MAP = json.load(f)
        logger.info(f"æˆåŠŸä» 'models.json' åŠ è½½äº† {len(MODEL_NAME_TO_ID_MAP)} ä¸ªæ¨¡å‹ã€‚")
    except (FileNotFoundError, json.JSONDecodeError) as e:
        logger.error(f"åŠ è½½ 'models.json' å¤±è´¥: {e}ã€‚å°†ä½¿ç”¨ç©ºæ¨¡å‹åˆ—è¡¨ã€‚")
        MODEL_NAME_TO_ID_MAP = {}

# --- æ›´æ–°æ£€æŸ¥ ---
GITHUB_REPO = "Lianues/LMArenaBridge"

def download_and_extract_update(version):
    """ä¸‹è½½å¹¶è§£å‹æœ€æ–°ç‰ˆæœ¬åˆ°ä¸´æ—¶æ–‡ä»¶å¤¹ã€‚"""
    update_dir = "update_temp"
    if not os.path.exists(update_dir):
        os.makedirs(update_dir)

    try:
        zip_url = f"https://github.com/{GITHUB_REPO}/archive/refs/heads/main.zip"
        logger.info(f"æ­£åœ¨ä» {zip_url} ä¸‹è½½æ–°ç‰ˆæœ¬...")
        response = requests.get(zip_url, timeout=60)
        response.raise_for_status()

        # éœ€è¦å¯¼å…¥ zipfile å’Œ io
        import zipfile
        import io
        with zipfile.ZipFile(io.BytesIO(response.content)) as z:
            z.extractall(update_dir)
        
        logger.info(f"æ–°ç‰ˆæœ¬å·²æˆåŠŸä¸‹è½½å¹¶è§£å‹åˆ° '{update_dir}' æ–‡ä»¶å¤¹ã€‚")
        return True
    except requests.RequestException as e:
        logger.error(f"ä¸‹è½½æ›´æ–°å¤±è´¥: {e}")
    except zipfile.BadZipFile:
        logger.error("ä¸‹è½½çš„æ–‡ä»¶ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„zipå‹ç¼©åŒ…ã€‚")
    except Exception as e:
        logger.error(f"è§£å‹æ›´æ–°æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    
    return False

def check_for_updates():
    """ä» GitHub æ£€æŸ¥æ–°ç‰ˆæœ¬ã€‚"""
    if not CONFIG.get("enable_auto_update", True):
        logger.info("è‡ªåŠ¨æ›´æ–°å·²ç¦ç”¨ï¼Œè·³è¿‡æ£€æŸ¥ã€‚")
        return

    current_version = CONFIG.get("version", "0.0.0")
    logger.info(f"å½“å‰ç‰ˆæœ¬: {current_version}ã€‚æ­£åœ¨ä» GitHub æ£€æŸ¥æ›´æ–°...")

    try:
        config_url = f"https://raw.githubusercontent.com/{GITHUB_REPO}/main/config.jsonc"
        response = requests.get(config_url, timeout=10)
        response.raise_for_status()

        jsonc_content = response.text
        json_content = re.sub(r'//.*', '', jsonc_content)
        json_content = re.sub(r'/\*.*?\*/', '', json_content, flags=re.DOTALL)
        remote_config = json.loads(json_content)
        
        remote_version_str = remote_config.get("version")
        if not remote_version_str:
            logger.warning("è¿œç¨‹é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ç‰ˆæœ¬å·ï¼Œè·³è¿‡æ›´æ–°æ£€æŸ¥ã€‚")
            return

        if parse_version(remote_version_str) > parse_version(current_version):
            logger.info("="*60)
            logger.info(f"ğŸ‰ å‘ç°æ–°ç‰ˆæœ¬! ğŸ‰")
            logger.info(f"  - å½“å‰ç‰ˆæœ¬: {current_version}")
            logger.info(f"  - æœ€æ–°ç‰ˆæœ¬: {remote_version_str}")
            if download_and_extract_update(remote_version_str):
                logger.info("å‡†å¤‡åº”ç”¨æ›´æ–°ã€‚æœåŠ¡å™¨å°†åœ¨5ç§’åå…³é—­å¹¶å¯åŠ¨æ›´æ–°è„šæœ¬ã€‚")
                time.sleep(5)
                update_script_path = os.path.join("modules", "update_script.py")
                # ä½¿ç”¨ Popen å¯åŠ¨ç‹¬ç«‹è¿›ç¨‹
                subprocess.Popen([sys.executable, update_script_path])
                # ä¼˜é›…åœ°é€€å‡ºå½“å‰æœåŠ¡å™¨è¿›ç¨‹
                os._exit(0)
            else:
                logger.error(f"è‡ªåŠ¨æ›´æ–°å¤±è´¥ã€‚è¯·è®¿é—® https://github.com/{GITHUB_REPO}/releases/latest æ‰‹åŠ¨ä¸‹è½½ã€‚")
            logger.info("="*60)
        else:
            logger.info("æ‚¨çš„ç¨‹åºå·²æ˜¯æœ€æ–°ç‰ˆæœ¬ã€‚")

    except requests.RequestException as e:
        logger.error(f"æ£€æŸ¥æ›´æ–°å¤±è´¥: {e}")
    except json.JSONDecodeError:
        logger.error("è§£æè¿œç¨‹é…ç½®æ–‡ä»¶å¤±è´¥ã€‚")
    except Exception as e:
        logger.error(f"æ£€æŸ¥æ›´æ–°æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

# --- æ¨¡å‹æ›´æ–° ---
def extract_models_from_html(html_content):
    """
    ä» HTML å†…å®¹ä¸­æå–æ¨¡å‹æ•°æ®ï¼Œé‡‡ç”¨æ›´å¥å£®çš„è§£ææ–¹æ³•ã€‚
    """
    script_contents = re.findall(r'<script>(.*?)</script>', html_content, re.DOTALL)
    
    for script_content in script_contents:
        if 'self.__next_f.push' in script_content and 'initialState' in script_content and 'publicName' in script_content:
            match = re.search(r'self\.__next_f\.push\(\[1,"(.*?)"\]\)', script_content, re.DOTALL)
            if not match:
                continue
            
            full_payload = match.group(1)
            
            payload_string = full_payload.split('\\n')[0]
            
            json_start_index = payload_string.find(':')
            if json_start_index == -1:
                continue
            
            json_string_with_escapes = payload_string[json_start_index + 1:]
            json_string = json_string_with_escapes.replace('\\"', '"')
            
            try:
                data = json.loads(json_string)
                
                def find_initial_state(obj):
                    if isinstance(obj, dict):
                        for key, value in obj.items():
                            if key == 'initialState' and isinstance(value, list):
                                if value and isinstance(value[0], dict) and 'publicName' in value[0]:
                                    return value
                            result = find_initial_state(value)
                            if result is not None:
                                return result
                    elif isinstance(obj, list):
                        for item in obj:
                            result = find_initial_state(item)
                            if result is not None:
                                return result
                    return None

                models = find_initial_state(data)
                if models:
                    logger.info(f"æˆåŠŸä»è„šæœ¬å—ä¸­æå–åˆ° {len(models)} ä¸ªæ¨¡å‹ã€‚")
                    return models
            except json.JSONDecodeError as e:
                logger.error(f"è§£ææå–çš„JSONå­—ç¬¦ä¸²æ—¶å‡ºé”™: {e}")
                continue

    logger.error("é”™è¯¯ï¼šåœ¨HTMLå“åº”ä¸­æ‰¾ä¸åˆ°åŒ…å«æœ‰æ•ˆæ¨¡å‹æ•°æ®çš„è„šæœ¬å—ã€‚")
    return None

def compare_and_update_models(new_models_list, models_path):
    """
    æ¯”è¾ƒæ–°æ—§æ¨¡å‹åˆ—è¡¨ï¼Œæ‰“å°å·®å¼‚ï¼Œå¹¶ç”¨æ–°åˆ—è¡¨æ›´æ–°æœ¬åœ° models.json æ–‡ä»¶ã€‚
    """
    try:
        with open(models_path, 'r', encoding='utf-8') as f:
            old_models = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        old_models = {}

    new_models_dict = {model['publicName']: model for model in new_models_list if 'publicName' in model}
    old_models_set = set(old_models.keys())
    new_models_set = set(new_models_dict.keys())

    added_models = new_models_set - old_models_set
    removed_models = old_models_set - new_models_set
    
    logger.info("--- æ¨¡å‹åˆ—è¡¨æ›´æ–°æ£€æŸ¥ ---")
    has_changes = False

    if added_models:
        has_changes = True
        logger.info("\n[+] æ–°å¢æ¨¡å‹:")
        for name in sorted(list(added_models)):
            model = new_models_dict[name]
            logger.info(f"  - åç§°: {name}, ID: {model.get('id')}, ç»„ç»‡: {model.get('organization', 'N/A')}")

    if removed_models:
        has_changes = True
        logger.info("\n[-] åˆ é™¤æ¨¡å‹:")
        for name in sorted(list(removed_models)):
            logger.info(f"  - åç§°: {name}, ID: {old_models.get(name)}")

    logger.info("\n[*] å…±åŒæ¨¡å‹æ£€æŸ¥:")
    changed_models = 0
    for name in sorted(list(new_models_set.intersection(old_models_set))):
        new_id = new_models_dict[name].get('id')
        old_id = old_models.get(name)
        if new_id != old_id:
            has_changes = True
            changed_models += 1
            logger.info(f"  - ID å˜æ›´: '{name}' æ—§ID: {old_id} -> æ–°ID: {new_id}")
    
    if changed_models == 0:
        logger.info("  - å…±åŒæ¨¡å‹çš„IDæ— å˜åŒ–ã€‚")

    if not has_changes:
        logger.info("\nç»“è®º: æ¨¡å‹åˆ—è¡¨æ— ä»»ä½•å˜åŒ–ï¼Œæ— éœ€æ›´æ–°æ–‡ä»¶ã€‚")
        logger.info("--- æ£€æŸ¥å®Œæ¯• ---")
        return

    logger.info("\nç»“è®º: æ£€æµ‹åˆ°æ¨¡å‹å˜æ›´ï¼Œæ­£åœ¨æ›´æ–° 'models.json'...")
    updated_model_map = {model['publicName']: model.get('id') for model in new_models_list if 'publicName' in model and 'id' in model}
    try:
        with open(models_path, 'w', encoding='utf-8') as f:
            json.dump(updated_model_map, f, indent=4, ensure_ascii=False)
        logger.info(f"'{models_path}' å·²æˆåŠŸæ›´æ–°ï¼ŒåŒ…å« {len(updated_model_map)} ä¸ªæ¨¡å‹ã€‚")
        load_model_map()
    except IOError as e:
        logger.error(f"å†™å…¥ '{models_path}' æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    logger.info("--- æ£€æŸ¥ä¸æ›´æ–°å®Œæ¯• ---")

# --- FastAPI ç”Ÿå‘½å‘¨æœŸäº‹ä»¶ ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    """åœ¨æœåŠ¡å™¨å¯åŠ¨æ—¶è¿è¡Œçš„ç”Ÿå‘½å‘¨æœŸå‡½æ•°ã€‚"""
    load_config() # é¦–å…ˆåŠ è½½é…ç½®
    check_for_updates() # æ£€æŸ¥ç¨‹åºæ›´æ–°
    load_model_map() # åŠ è½½æ¨¡å‹æ˜ å°„
    logger.info("æœåŠ¡å™¨å¯åŠ¨å®Œæˆã€‚ç­‰å¾…æ²¹çŒ´è„šæœ¬è¿æ¥...")
    yield
    logger.info("æœåŠ¡å™¨æ­£åœ¨å…³é—­ã€‚")

app = FastAPI(lifespan=lifespan)

# --- CORS ä¸­é—´ä»¶é…ç½® ---
# å…è®¸æ‰€æœ‰æ¥æºã€æ‰€æœ‰æ–¹æ³•ã€æ‰€æœ‰è¯·æ±‚å¤´ï¼Œè¿™å¯¹äºæœ¬åœ°å¼€å‘å·¥å…·æ˜¯å®‰å…¨çš„ã€‚
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- è¾…åŠ©å‡½æ•° ---
def save_config():
    """å°†å½“å‰çš„ CONFIG å¯¹è±¡å†™å› config.jsonc æ–‡ä»¶ï¼Œä¿ç•™æ³¨é‡Šã€‚"""
    try:
        # è¯»å–åŸå§‹æ–‡ä»¶ä»¥ä¿ç•™æ³¨é‡Šç­‰
        with open('config.jsonc', 'r', encoding='utf-8') as f:
            lines = f.readlines()

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å®‰å…¨åœ°æ›¿æ¢å€¼
        def replacer(key, value, content):
            # è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼ä¼šæ‰¾åˆ° keyï¼Œç„¶ååŒ¹é…å®ƒçš„ value éƒ¨åˆ†ï¼Œç›´åˆ°é€—å·æˆ–å³èŠ±æ‹¬å·
            pattern = re.compile(rf'("{key}"\s*:\s*").*?("?)(,?\s*)$', re.MULTILINE)
            replacement = rf'\g<1>{value}\g<2>\g<3>'
            if not pattern.search(content): # å¦‚æœ key ä¸å­˜åœ¨ï¼Œå°±æ·»åŠ åˆ°æ–‡ä»¶æœ«å°¾ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                 content = re.sub(r'}\s*$', f'  ,"{key}": "{value}"\n}}', content)
            else:
                 content = pattern.sub(replacement, content)
            return content

        content_str = "".join(lines)
        content_str = replacer("session_id", CONFIG["session_id"], content_str)
        content_str = replacer("message_id", CONFIG["message_id"], content_str)
        
        with open('config.jsonc', 'w', encoding='utf-8') as f:
            f.write(content_str)
        logger.info("âœ… æˆåŠŸå°†ä¼šè¯ä¿¡æ¯æ›´æ–°åˆ° config.jsoncã€‚")
    except Exception as e:
        logger.error(f"âŒ å†™å…¥ config.jsonc æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)


def _normalize_message_content(message: dict) -> dict:
    """
    å¤„ç†å’Œè§„èŒƒåŒ–æ¥è‡ª OpenAI è¯·æ±‚çš„å•æ¡æ¶ˆæ¯ã€‚
    - å°†å¤šæ¨¡æ€å†…å®¹åˆ—è¡¨è½¬æ¢ä¸ºçº¯æ–‡æœ¬ã€‚
    - ç¡®ä¿ user è§’è‰²çš„ç©ºå†…å®¹è¢«æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œä»¥é¿å… LMArena å‡ºé”™ã€‚
    """
    content = message.get("content")
    
    if isinstance(content, list):
        text_parts = [p.get("text", "") for p in content if isinstance(p, dict) and p.get("type") == "text"]
        message["content"] = "\n\n".join(text_parts)
        content = message["content"]

    if message.get("role") == "user" and content == "":
        message["content"] = " "
        
    return message

def convert_openai_to_lmarena_payload(openai_data: dict, session_id: str, message_id: str) -> dict:
    """
    å°† OpenAI è¯·æ±‚ä½“è½¬æ¢ä¸ºæ²¹çŒ´è„šæœ¬æ‰€éœ€çš„ç®€åŒ–è½½è·ï¼Œå¹¶åº”ç”¨é…’é¦†æ¨¡å¼å’Œç»•è¿‡æ¨¡å¼ã€‚
    """
    # 1. è§„èŒƒåŒ–æ‰€æœ‰æ¶ˆæ¯
    normalized_messages = [_normalize_message_content(msg.copy()) for msg in openai_data.get("messages", [])]

    # 2. åº”ç”¨é…’é¦†æ¨¡å¼ (Tavern Mode)
    if CONFIG.get("tavern_mode_enabled"):
        system_prompts = [msg['content'] for msg in normalized_messages if msg['role'] == 'system']
        other_messages = [msg for msg in normalized_messages if msg['role'] != 'system']
        
        merged_system_prompt = "\n\n".join(system_prompts)
        final_messages = []
        
        if merged_system_prompt:
            final_messages.append({"role": "system", "content": merged_system_prompt})
        
        final_messages.extend(other_messages)
        normalized_messages = final_messages

    # 3. ç¡®å®šç›®æ ‡æ¨¡å‹ ID
    model_name = openai_data.get("model", "claude-3-5-sonnet-20241022")
    target_model_id = MODEL_NAME_TO_ID_MAP.get(model_name, DEFAULT_MODEL_ID)
    
    # 4. æ„å»ºæ¶ˆæ¯æ¨¡æ¿ (åªä¿ç•™ role å’Œ content)
    message_templates = []
    for msg in normalized_messages:
        message_templates.append({"role": msg["role"], "content": msg.get("content", "")})

    # 5. åº”ç”¨ç»•è¿‡æ¨¡å¼ (Bypass Mode)
    if CONFIG.get("bypass_enabled"):
        message_templates.append({"role": "user", "content": " "})
    
    return {
        "message_templates": message_templates,
        "target_model_id": target_model_id,
        "session_id": session_id,
        "message_id": message_id
    }

# --- OpenAI æ ¼å¼åŒ–è¾…åŠ©å‡½æ•° (ç¡®ä¿JSONåºåˆ—åŒ–ç¨³å¥) ---
def format_openai_chunk(content: str, model: str, request_id: str) -> str:
    """æ ¼å¼åŒ–ä¸º OpenAI æµå¼å—ã€‚"""
    chunk = {
        "id": request_id, "object": "chat.completion.chunk",
        "created": int(time.time()), "model": model,
        "choices": [{"index": 0, "delta": {"content": content}, "finish_reason": None}]
    }
    return f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"

def format_openai_finish_chunk(model: str, request_id: str, reason: str = 'stop') -> str:
    """æ ¼å¼åŒ–ä¸º OpenAI ç»“æŸå—ã€‚"""
    chunk = {
        "id": request_id, "object": "chat.completion.chunk",
        "created": int(time.time()), "model": model,
        "choices": [{"index": 0, "delta": {}, "finish_reason": reason}]
    }
    return f"data: {json.dumps(chunk, ensure_ascii=False)}\n\ndata: [DONE]\n\n"

def format_openai_error_chunk(error_message: str, model: str, request_id: str) -> str:
    """æ ¼å¼åŒ–ä¸º OpenAI é”™è¯¯å—ã€‚"""
    content = f"\n\n[LMArena Bridge Error]: {error_message}"
    return format_openai_chunk(content, model, request_id)

def format_openai_non_stream_response(content: str, model: str, request_id: str, reason: str = 'stop') -> dict:
    """æ„å»ºç¬¦åˆ OpenAI è§„èŒƒçš„éæµå¼å“åº”ä½“ã€‚"""
    return {
        "id": request_id,
        "object": "chat.completion",
        "created": int(time.time()),
        "model": model,
        "choices": [{
            "index": 0,
            "message": {"role": "assistant", "content": content},
            "finish_reason": reason,
        }],
        "usage": {
            "prompt_tokens": 0,
            "completion_tokens": len(content) // 4,
            "total_tokens": len(content) // 4,
        },
    }

async def _process_lmarena_stream(request_id: str):
    """
    æ ¸å¿ƒå†…éƒ¨ç”Ÿæˆå™¨ï¼šå¤„ç†æ¥è‡ªæµè§ˆå™¨çš„åŸå§‹æ•°æ®æµï¼Œå¹¶äº§ç”Ÿç»“æ„åŒ–äº‹ä»¶ã€‚
    äº‹ä»¶ç±»å‹: ('content', str), ('finish', str), ('error', str)
    """
    queue = response_channels.get(request_id)
    if not queue:
        logger.error(f"PROCESSOR [ID: {request_id[:8]}]: æ— æ³•æ‰¾åˆ°å“åº”é€šé“ã€‚")
        yield 'error', 'Internal server error: response channel not found.'
        return

    buffer = ""
    timeout = CONFIG.get("stream_response_timeout_seconds", 120)
    text_pattern = re.compile(r'[ab]0:"((?:\\.|[^"\\])*)"')
    finish_pattern = re.compile(r'[ab]d:(\{.*?"finishReason".*?\})')
    error_pattern = re.compile(r'(\{\s*"error".*?\})', re.DOTALL)
    cloudflare_patterns = [r'<title>Just a moment...</title>', r'Enable JavaScript and cookies to continue']

    try:
        while True:
            try:
                raw_data = await asyncio.wait_for(queue.get(), timeout=timeout)
            except asyncio.TimeoutError:
                logger.warning(f"PROCESSOR [ID: {request_id[:8]}]: ç­‰å¾…æµè§ˆå™¨æ•°æ®è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰ã€‚")
                yield 'error', f'Response timed out after {timeout} seconds.'
                return

            # 1. æ£€æŸ¥æ¥è‡ª WebSocket ç«¯çš„ç›´æ¥é”™è¯¯æˆ–ç»ˆæ­¢ä¿¡å·
            if isinstance(raw_data, dict) and 'error' in raw_data:
                error_msg = raw_data.get('error', 'Unknown browser error')
                # å¢å¼ºï¼šæ£€æŸ¥é”™è¯¯æ¶ˆæ¯æœ¬èº«æ˜¯å¦åŒ…å«Cloudflareé¡µé¢
                if isinstance(error_msg, str) and any(re.search(p, error_msg, re.IGNORECASE) for p in cloudflare_patterns):
                    friendly_error_msg = "æ£€æµ‹åˆ° Cloudflare äººæœºéªŒè¯é¡µé¢ã€‚è¯·åœ¨æµè§ˆå™¨ä¸­åˆ·æ–° LMArena é¡µé¢å¹¶æ‰‹åŠ¨å®ŒæˆéªŒè¯ï¼Œç„¶åé‡è¯•è¯·æ±‚ã€‚"
                    if browser_ws:
                        try:
                            await browser_ws.send_text(json.dumps({"command": "refresh"}, ensure_ascii=False))
                            logger.info(f"PROCESSOR [ID: {request_id[:8]}]: åœ¨é”™è¯¯æ¶ˆæ¯ä¸­æ£€æµ‹åˆ°CFå¹¶å·²å‘é€åˆ·æ–°æŒ‡ä»¤ã€‚")
                        except Exception as e:
                            logger.error(f"PROCESSOR [ID: {request_id[:8]}]: å‘é€åˆ·æ–°æŒ‡ä»¤å¤±è´¥: {e}")
                    yield 'error', friendly_error_msg
                else:
                    yield 'error', error_msg
                return
            if raw_data == "[DONE]":
                break

            buffer += "".join(str(item) for item in raw_data) if isinstance(raw_data, list) else raw_data

            if any(re.search(p, buffer, re.IGNORECASE) for p in cloudflare_patterns):
                error_msg = "æ£€æµ‹åˆ° Cloudflare äººæœºéªŒè¯é¡µé¢ã€‚è¯·åœ¨æµè§ˆå™¨ä¸­åˆ·æ–° LMArena é¡µé¢å¹¶æ‰‹åŠ¨å®ŒæˆéªŒè¯ï¼Œç„¶åé‡è¯•è¯·æ±‚ã€‚"
                if browser_ws:
                    try:
                        await browser_ws.send_text(json.dumps({"command": "refresh"}, ensure_ascii=False))
                        logger.info(f"PROCESSOR [ID: {request_id[:8]}]: å·²å‘æµè§ˆå™¨å‘é€é¡µé¢åˆ·æ–°æŒ‡ä»¤ã€‚")
                    except Exception as e:
                        logger.error(f"PROCESSOR [ID: {request_id[:8]}]: å‘é€åˆ·æ–°æŒ‡ä»¤å¤±è´¥: {e}")
                yield 'error', error_msg
                return
            
            if (error_match := error_pattern.search(buffer)):
                try:
                    error_json = json.loads(error_match.group(1))
                    yield 'error', error_json.get("error", "æ¥è‡ª LMArena çš„æœªçŸ¥é”™è¯¯")
                    return
                except json.JSONDecodeError: pass

            while (match := text_pattern.search(buffer)):
                try:
                    text_content = json.loads(f'"{match.group(1)}"')
                    if text_content: yield 'content', text_content
                except (ValueError, json.JSONDecodeError): pass
                buffer = buffer[match.end():]

            if (finish_match := finish_pattern.search(buffer)):
                try:
                    finish_data = json.loads(finish_match.group(1))
                    yield 'finish', finish_data.get("finishReason", "stop")
                except (json.JSONDecodeError, IndexError): pass
                buffer = buffer[finish_match.end():]

    except asyncio.CancelledError:
        logger.info(f"PROCESSOR [ID: {request_id[:8]}]: ä»»åŠ¡è¢«å–æ¶ˆã€‚")
    finally:
        if request_id in response_channels:
            del response_channels[request_id]
            logger.info(f"PROCESSOR [ID: {request_id[:8]}]: å“åº”é€šé“å·²æ¸…ç†ã€‚")

async def stream_generator(request_id: str, model: str):
    """å°†å†…éƒ¨äº‹ä»¶æµæ ¼å¼åŒ–ä¸º OpenAI SSE å“åº”ã€‚"""
    response_id = f"chatcmpl-{uuid.uuid4()}"
    logger.info(f"STREAMER [ID: {request_id[:8]}]: æµå¼ç”Ÿæˆå™¨å¯åŠ¨ã€‚")
    
    async for event_type, data in _process_lmarena_stream(request_id):
        if event_type == 'content':
            yield format_openai_chunk(data, model, response_id)
        elif event_type == 'finish':
            if data == 'content-filter':
                warning_msg = "\n\nå“åº”è¢«ç»ˆæ­¢ï¼Œå¯èƒ½æ˜¯ä¸Šä¸‹æ–‡è¶…é™æˆ–è€…æ¨¡å‹å†…éƒ¨å®¡æŸ¥ï¼ˆå¤§æ¦‚ç‡ï¼‰çš„åŸå› "
                yield format_openai_chunk(warning_msg, model, response_id)
            yield format_openai_finish_chunk(model, response_id, reason=data)
            return
        elif event_type == 'error':
            logger.error(f"STREAMER [ID: {request_id[:8]}]: æµä¸­å‘ç”Ÿé”™è¯¯: {data}")
            yield format_openai_error_chunk(str(data), model, response_id)
            yield format_openai_finish_chunk(model, response_id, reason='stop')
            return
    
    yield format_openai_finish_chunk(model, response_id, reason='stop')
    logger.info(f"STREAMER [ID: {request_id[:8]}]: æµå¼ç”Ÿæˆå™¨æ­£å¸¸ç»“æŸã€‚")

async def non_stream_response(request_id: str, model: str):
    """èšåˆå†…éƒ¨äº‹ä»¶æµå¹¶è¿”å›å•ä¸ª OpenAI JSON å“åº”ã€‚"""
    response_id = f"chatcmpl-{uuid.uuid4()}"
    logger.info(f"NON-STREAM [ID: {request_id[:8]}]: å¼€å§‹å¤„ç†éæµå¼å“åº”ã€‚")
    
    full_content = []
    finish_reason = "stop"
    
    async for event_type, data in _process_lmarena_stream(request_id):
        if event_type == 'content':
            full_content.append(data)
        elif event_type == 'finish':
            finish_reason = data
            if data == 'content-filter':
                full_content.append("\n\nå“åº”è¢«ç»ˆæ­¢ï¼Œå¯èƒ½æ˜¯ä¸Šä¸‹æ–‡è¶…é™æˆ–è€…æ¨¡å‹å†…éƒ¨å®¡æŸ¥ï¼ˆå¤§æ¦‚ç‡ï¼‰çš„åŸå› ")
            break
        elif event_type == 'error':
            logger.error(f"NON-STREAM [ID: {request_id[:8]}]: å¤„ç†æ—¶å‘ç”Ÿé”™è¯¯: {data}")
            error_response = {
                "error": {
                    "message": f"[LMArena Bridge Error]: {data}",
                    "type": "bridge_error",
                    "code": "processing_error"
                }
            }
            return Response(content=json.dumps(error_response, ensure_ascii=False), status_code=500, media_type="application/json")

    final_content = "".join(full_content)
    response_data = format_openai_non_stream_response(final_content, model, response_id, reason=finish_reason)
    
    logger.info(f"NON-STREAM [ID: {request_id[:8]}]: å“åº”èšåˆå®Œæˆã€‚")
    return Response(content=json.dumps(response_data, ensure_ascii=False), media_type="application/json")

# --- WebSocket ç«¯ç‚¹ ---
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """å¤„ç†æ¥è‡ªæ²¹çŒ´è„šæœ¬çš„ WebSocket è¿æ¥ã€‚"""
    global browser_ws
    await websocket.accept()
    if browser_ws is not None:
        logger.warning("æ£€æµ‹åˆ°æ–°çš„æ²¹çŒ´è„šæœ¬è¿æ¥ï¼Œæ—§çš„è¿æ¥å°†è¢«æ›¿æ¢ã€‚")
    logger.info("âœ… æ²¹çŒ´è„šæœ¬å·²æˆåŠŸè¿æ¥ WebSocketã€‚")
    browser_ws = websocket
    try:
        while True:
            # ç­‰å¾…å¹¶æ¥æ”¶æ¥è‡ªæ²¹çŒ´è„šæœ¬çš„æ¶ˆæ¯
            message_str = await websocket.receive_text()
            message = json.loads(message_str)
            
            request_id = message.get("request_id")
            data = message.get("data")

            if not request_id or data is None:
                logger.warning(f"æ”¶åˆ°æ¥è‡ªæµè§ˆå™¨çš„æ— æ•ˆæ¶ˆæ¯: {message}")
                continue

            # å°†æ”¶åˆ°çš„æ•°æ®æ”¾å…¥å¯¹åº”çš„å“åº”é€šé“
            if request_id in response_channels:
                await response_channels[request_id].put(data)
            else:
                logger.warning(f"âš ï¸ æ”¶åˆ°æœªçŸ¥æˆ–å·²å…³é—­è¯·æ±‚çš„å“åº”: {request_id}")

    except WebSocketDisconnect:
        logger.warning("âŒ æ²¹çŒ´è„šæœ¬å®¢æˆ·ç«¯å·²æ–­å¼€è¿æ¥ã€‚")
    except Exception as e:
        logger.error(f"WebSocket å¤„ç†æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
    finally:
        browser_ws = None
        # æ¸…ç†æ‰€æœ‰ç­‰å¾…çš„å“åº”é€šé“ï¼Œä»¥é˜²è¯·æ±‚è¢«æŒ‚èµ·
        for queue in response_channels.values():
            await queue.put({"error": "Browser disconnected during operation"})
        response_channels.clear()
        logger.info("WebSocket è¿æ¥å·²æ¸…ç†ã€‚")

# --- æ¨¡å‹æ›´æ–°ç«¯ç‚¹ ---
@app.post("/update_models")
async def update_models_endpoint(request: Request):
    """
    æ¥æ”¶æ¥è‡ªæ²¹çŒ´è„šæœ¬çš„é¡µé¢ HTMLï¼Œæå–å¹¶æ›´æ–°æ¨¡å‹åˆ—è¡¨ã€‚
    """
    html_content = await request.body()
    if not html_content:
        logger.warning("æ¨¡å‹æ›´æ–°è¯·æ±‚æœªæ”¶åˆ°ä»»ä½• HTML å†…å®¹ã€‚")
        return JSONResponse(
            status_code=400,
            content={"status": "error", "message": "No HTML content received."}
        )
    
    logger.info("æ”¶åˆ°æ¥è‡ªæ²¹çŒ´è„šæœ¬çš„é¡µé¢å†…å®¹ï¼Œå¼€å§‹æ£€æŸ¥å¹¶æ›´æ–°æ¨¡å‹...")
    new_models_list = extract_models_from_html(html_content.decode('utf-8'))
    
    if new_models_list:
        compare_and_update_models(new_models_list, 'models.json')
        # load_model_map() is now called inside compare_and_update_models
        return JSONResponse({"status": "success", "message": "Model comparison and update complete."})
    else:
        logger.error("æœªèƒ½ä»æ²¹çŒ´è„šæœ¬æä¾›çš„ HTML ä¸­æå–æ¨¡å‹æ•°æ®ã€‚")
        return JSONResponse(
            status_code=400,
            content={"status": "error", "message": "Could not extract model data from HTML."}
        )

# --- OpenAI å…¼å®¹ API ç«¯ç‚¹ ---
@app.get("/v1/models")
async def get_models():
    """æä¾›å…¼å®¹ OpenAI çš„æ¨¡å‹åˆ—è¡¨ã€‚"""
    if not MODEL_NAME_TO_ID_MAP:
        return JSONResponse(
            status_code=404,
            content={"error": "æ¨¡å‹åˆ—è¡¨ä¸ºç©ºæˆ– 'models.json' æœªæ‰¾åˆ°ã€‚"}
        )
    
    return {
        "object": "list",
        "data": [
            {
                "id": model_name, 
                "object": "model",
                "created": int(asyncio.get_event_loop().time()), 
                "owned_by": "LMArenaBridge"
            }
            for model_name in MODEL_NAME_TO_ID_MAP.keys()
        ],
    }

@app.post("/v1/chat/completions")
async def chat_completions(request: Request):
    """
    å¤„ç†èŠå¤©è¡¥å…¨è¯·æ±‚ã€‚
    æ¥æ”¶ OpenAI æ ¼å¼çš„è¯·æ±‚ï¼Œå°†å…¶è½¬æ¢ä¸º LMArena æ ¼å¼ï¼Œ
    é€šè¿‡ WebSocket å‘é€ç»™æ²¹çŒ´è„šæœ¬ï¼Œç„¶åæµå¼è¿”å›ç»“æœã€‚
    """
    load_config()  # å®æ—¶åŠ è½½æœ€æ–°é…ç½®ï¼Œç¡®ä¿ä¼šè¯IDç­‰ä¿¡æ¯æ˜¯æœ€æ–°çš„
    # --- API Key éªŒè¯ ---
    api_key = CONFIG.get("api_key")
    if api_key:
        auth_header = request.headers.get('Authorization')
        if not auth_header or not auth_header.startswith('Bearer '):
            raise HTTPException(
                status_code=401,
                detail="æœªæä¾› API Keyã€‚è¯·åœ¨ Authorization å¤´éƒ¨ä¸­ä»¥ 'Bearer YOUR_KEY' æ ¼å¼æä¾›ã€‚"
            )
        
        provided_key = auth_header.split(' ')[1]
        if provided_key != api_key:
            raise HTTPException(
                status_code=401,
                detail="æä¾›çš„ API Key ä¸æ­£ç¡®ã€‚"
            )

    if not browser_ws:
        raise HTTPException(status_code=503, detail="æ²¹çŒ´è„šæœ¬å®¢æˆ·ç«¯æœªè¿æ¥ã€‚è¯·ç¡®ä¿ LMArena é¡µé¢å·²æ‰“å¼€å¹¶æ¿€æ´»è„šæœ¬ã€‚")

    try:
        openai_req = await request.json()
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="æ— æ•ˆçš„ JSON è¯·æ±‚ä½“")

    # --- ç¡®å®šå¹¶éªŒè¯ä¼šè¯ä¿¡æ¯ ---
    # ä¼˜å…ˆä½¿ç”¨è¯·æ±‚ä½“ä¸­æä¾›çš„IDï¼Œå¦åˆ™å›é€€åˆ°é…ç½®æ–‡ä»¶ä¸­çš„ID
    session_id = openai_req.get("session_id") or CONFIG.get("session_id")
    message_id = openai_req.get("message_id") or CONFIG.get("message_id")

    if not session_id or not message_id or "YOUR_" in session_id or "YOUR_" in message_id:
        raise HTTPException(
            status_code=400,
            detail="ä¼šè¯IDæˆ–æ¶ˆæ¯IDæ— æ•ˆã€‚è¯·åœ¨è¯·æ±‚ä½“ä¸­æä¾›ï¼Œæˆ–è¿è¡Œ `id_updater.py` æ¥è®¾ç½®é»˜è®¤å€¼ã€‚"
        )

    model_name = openai_req.get("model")
    if not model_name or model_name not in MODEL_NAME_TO_ID_MAP:
        logger.warning(f"è¯·æ±‚çš„æ¨¡å‹ '{model_name}' ä¸åœ¨ models.json ä¸­ï¼Œå°†ä½¿ç”¨é»˜è®¤æ¨¡å‹IDã€‚")

    request_id = str(uuid.uuid4())
    response_channels[request_id] = asyncio.Queue()
    logger.info(f"API CALL [ID: {request_id[:8]}]: å·²åˆ›å»ºå“åº”é€šé“ã€‚")

    try:
        # 1. è½¬æ¢è¯·æ±‚
        lmarena_payload = convert_openai_to_lmarena_payload(openai_req, session_id, message_id)
        
        # 2. åŒ…è£…æˆå‘é€ç»™æµè§ˆå™¨çš„æ¶ˆæ¯
        message_to_browser = {
            "request_id": request_id,
            "payload": lmarena_payload
        }
        
        # 3. é€šè¿‡ WebSocket å‘é€
        logger.info(f"API CALL [ID: {request_id[:8]}]: æ­£åœ¨é€šè¿‡ WebSocket å‘é€è½½è·åˆ°æ²¹çŒ´è„šæœ¬ã€‚")
        await browser_ws.send_text(json.dumps(message_to_browser))

        # 4. æ ¹æ® stream å‚æ•°å†³å®šè¿”å›ç±»å‹
        is_stream = openai_req.get("stream", True)

        if is_stream:
            # è¿”å›æµå¼å“åº”
            return StreamingResponse(
                stream_generator(request_id, model_name or "default_model"),
                media_type="text/event-stream"
            )
        else:
            # è¿”å›éæµå¼å“åº”
            return await non_stream_response(request_id, model_name or "default_model")
    except Exception as e:
        # å¦‚æœåœ¨è®¾ç½®è¿‡ç¨‹ä¸­å‡ºé”™ï¼Œæ¸…ç†é€šé“
        if request_id in response_channels:
            del response_channels[request_id]
        logger.error(f"API CALL [ID: {request_id[:8]}]: å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    # å»ºè®®ä» config.jsonc ä¸­è¯»å–ç«¯å£ï¼Œæ­¤å¤„ä¸ºä¸´æ—¶ç¡¬ç¼–ç 
    api_port = 5102
    logger.info(f"ğŸš€ LMArena Bridge v2.0 API æœåŠ¡å™¨æ­£åœ¨å¯åŠ¨...")
    logger.info(f"   - ç›‘å¬åœ°å€: http://127.0.0.1:{api_port}")
    logger.info(f"   - WebSocket ç«¯ç‚¹: ws://127.0.0.1:{api_port}/ws")
    
    uvicorn.run(app, host="0.0.0.0", port=api_port)