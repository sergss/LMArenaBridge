# local_openai_history_server.py
# v12.2 - Chinese Localization

import logging
import os
from flask import Flask, request, jsonify, Response
from flask_cors import CORS
from queue import Queue, Empty
import uuid
import threading
import time
import json
import re
from datetime import datetime

# --- å…¨å±€é…ç½® ---
CONFIG = {}
logger = logging.getLogger(__name__)

# --- Flask åº”ç”¨è®¾ç½® ---
app = Flask(__name__)
CORS(app)
werkzeug_logger = logging.getLogger('werkzeug')
werkzeug_logger.disabled = True

# --- æ•°æ®å­˜å‚¨ ---
MESSAGES_JOBS = Queue()
PROMPT_JOBS = Queue()
RESULTS = {}

# --- æ¨¡å‹æ˜ å°„ ---
MODEL_NAME_TO_ID_MAP = {}
DEFAULT_MODEL_ID = "f44e280a-7914-43ca-a25d-ecfcc5d48d09"

def load_model_map():
    global MODEL_NAME_TO_ID_MAP
    try:
        with open('models.json', 'r', encoding='utf-8') as f:
            MODEL_NAME_TO_ID_MAP = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        MODEL_NAME_TO_ID_MAP = {}

# --- API ç«¯ç‚¹ ---
@app.route('/get_config', methods=['GET'])
def get_config():
    try:
        with open('config.jsonc', 'r', encoding='utf-8') as f:
            jsonc_content = f.read()
            json_content = re.sub(r'//.*', '', jsonc_content)
            json_content = re.sub(r'/\*.*?\*/', '', json_content, flags=re.DOTALL)
            return jsonify(json.loads(json_content))
    except Exception as e:
        logger.error(f"è¯»å–æˆ–è§£æ config.jsonc å¤±è´¥: {e}")
        return jsonify({"error": "Config file issue"}), 500

@app.route('/')
def index():
    return "LMArena è‡ªåŠ¨åŒ–å·¥å…· v12.2 (ä¸­æ–‡æœ¬åœ°åŒ–) æ­£åœ¨è¿è¡Œã€‚"

@app.route('/log_from_client', methods=['POST'])
def log_from_client():
    log_data = request.json
    if log_data and 'message' in log_data:
        logger.info(f"[æ²¹çŒ´è„šæœ¬] {log_data.get('level', 'INFO')}: {log_data['message']}")
    return jsonify({"status": "logged"})

# --- æ ¸å¿ƒé€»è¾‘ (æ— å˜åŒ–) ---
def convert_openai_to_lmarena_templates(openai_data: dict) -> dict:
    model_name = openai_data.get("model", "claude-3-5-sonnet-20241022")
    target_model_id = MODEL_NAME_TO_ID_MAP.get(model_name, DEFAULT_MODEL_ID)
    message_templates = []
    for oai_msg in openai_data["messages"]:
        message_templates.append({"role": oai_msg["role"], "content": oai_msg.get("content", "")})
    if CONFIG.get("bypass_enabled"):
        message_templates.append({"role": "user", "content": " "})
    message_templates.append({"role": "assistant", "content": ""})
    return {"message_templates": message_templates, "target_model_id": target_model_id}

@app.route('/get_messages_job', methods=['GET'])
def get_messages_job():
    try: return jsonify({"status": "success", "job": MESSAGES_JOBS.get_nowait()})
    except Empty: return jsonify({"status": "empty"})

@app.route('/get_prompt_job', methods=['GET'])
def get_prompt_job():
    try: return jsonify({"status": "success", "job": PROMPT_JOBS.get_nowait()})
    except Empty: return jsonify({"status": "empty"})

@app.route('/stream_chunk', methods=['POST'])
def stream_chunk():
    data = request.json
    task_id = data.get('task_id')
    if task_id in RESULTS:
        RESULTS[task_id]['stream_queue'].put(data.get('chunk'))
        return jsonify({"status": "success"})
    return jsonify({"status": "error"}), 404

@app.route('/report_result', methods=['POST'])
def report_result():
    data = request.json
    task_id = data.get('task_id')
    if task_id in RESULTS:
        RESULTS[task_id]['status'] = data.get('status', 'completed')
        logger.info(f"ä»»åŠ¡ {task_id[:8]} å·²è¢«å®¢æˆ·ç«¯æŠ¥å‘Šä¸ºå®Œæˆã€‚")
        return jsonify({"status": "success"})
    return jsonify({"status": "error"}), 404

def format_openai_chunk(content: str, model: str, request_id: str):
    return f"data: {json.dumps({'id': request_id, 'object': 'chat.completion.chunk', 'created': int(time.time()), 'model': model, 'choices': [{'index': 0, 'delta': {'content': content}, 'finish_reason': None}]})}\n\n"

def format_openai_finish_chunk(model: str, request_id: str):
    return f"data: {json.dumps({'id': request_id, 'object': 'chat.completion.chunk', 'created': int(time.time()), 'model': model, 'choices': [{'index': 0, 'delta': {}, 'finish_reason': 'stop'}]})}\n\ndata: [DONE]\n\n"

def format_openai_non_stream_response(content: str, model: str, request_id: str):
    return {'id': request_id, 'object': 'chat.completion', 'created': int(time.time()), 'model': model, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': content}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}}

def _normalize_message_content(message: dict) -> dict:
    content = message.get("content")
    if isinstance(content, list):
        message["content"] = "\n\n".join([p.get("text", "") for p in content if isinstance(p, dict) and p.get("type") == "text"])
    return message

def _openai_response_generator(task_id: str):
    text_pattern = re.compile(r'a0:"((?:\\.|[^"\\])*)"')
    error_pattern = re.compile(r'(\{\s*"error".*?\})', re.DOTALL)
    finish_pattern = re.compile(r'"finishReason"\s*:\s*"stop"')
    buffer = ""
    while True:
        try:
            raw_chunk = RESULTS[task_id]['stream_queue'].get(timeout=60)
            buffer += raw_chunk
            error_match = error_pattern.search(buffer)
            if error_match:
                try:
                    error_json = json.loads(error_match.group(1))
                    error_message = error_json.get("error", "æ¥è‡ª LMArena çš„æœªçŸ¥é”™è¯¯")
                    logger.error(f"ä»»åŠ¡ {task_id[:8]} çš„æµå¼å“åº”ä¸­æ£€æµ‹åˆ°é”™è¯¯: {error_message}")
                    RESULTS[task_id]['error'] = str(error_message)
                    return
                except json.JSONDecodeError: pass
            while True:
                match = text_pattern.search(buffer)
                if not match: break
                try:
                    text_content = json.loads(f'"{match.group(1)}"')
                    if text_content: yield text_content
                except json.JSONDecodeError: pass
                buffer = buffer[match.end():]
            if finish_pattern.search(raw_chunk):
                logger.info(f"æ£€æµ‹åˆ°ä»»åŠ¡ {task_id[:8]} çš„ LMArena æµç»“æŸä¿¡å·ã€‚")
                return
        except Empty:
            logger.warning(f"ä»»åŠ¡ {task_id[:8]} çš„ç”Ÿæˆå™¨è¶…æ—¶ã€‚")
            RESULTS[task_id]['error'] = 'æµå¼å“åº”åœ¨60ç§’åè¶…æ—¶ã€‚'
            return

def _load_config():
    global CONFIG
    try:
        with open('config.jsonc', 'r', encoding='utf-8') as f:
            CONFIG = json.loads(re.sub(r'/\*.*?\*/', '', re.sub(r'//.*', '', f.read()), flags=re.DOTALL))
    except Exception as e:
        logging.error(f"æ— æ³•åŠ è½½ config.jsonc: {e}ã€‚å°†ä½¿ç”¨é»˜è®¤è®¾ç½®ã€‚")
        CONFIG = {"enable_comprehensive_logging": False}

@app.route('/v1/models', methods=['GET'])
def list_models():
    return jsonify({"object": "list", "data": [{"id": name, "object": "model", "owned_by": "local-server"} for name in MODEL_NAME_TO_ID_MAP.keys()]})

@app.route('/v1/chat/completions', methods=['POST'])
def chat_completions():
    request_data = request.json
    if CONFIG.get("log_server_requests"):
        logger.info(f"--- æ”¶åˆ° OpenAI è¯·æ±‚ ---\n{json.dumps(request_data, indent=2, ensure_ascii=False)}")
    if not request_data or "messages" not in request_data: return jsonify({"error": "è¯·æ±‚å¿…é¡»åŒ…å« 'messages'"}), 400
    request_data["messages"] = [_normalize_message_content(msg) for msg in request_data.get("messages", [])]
    if not request_data["messages"]: return jsonify({"error": "'messages' åˆ—è¡¨ä¸èƒ½ä¸ºç©º"}), 400
    if CONFIG.get("tavern_mode_enabled"):
        system_prompts = [msg['content'] for msg in request_data["messages"] if msg['role'] == 'system']
        other_messages = [msg for msg in request_data["messages"] if msg['role'] != 'system']
        merged_system_prompt = "\n\n".join(system_prompts)
        final_messages = []
        if merged_system_prompt: final_messages.append({"role": "system", "content": merged_system_prompt})
        final_messages.extend(other_messages)
        request_data["messages"] = final_messages
    messages_job = convert_openai_to_lmarena_templates(request_data)
    MESSAGES_JOBS.put(messages_job)
    task_id = str(uuid.uuid4())
    prompt_job = {"task_id": task_id, "prompt": f"[è¿™æ¡æ¶ˆæ¯ä»…èµ·å ä½ï¼Œè¯·ä»¥å¤–éƒ¨åº”ç”¨ä¸­æ˜¾ç¤ºçš„å†…å®¹ä¸ºå‡†ï¼š/{task_id}]"}
    PROMPT_JOBS.put(prompt_job)
    RESULTS[task_id] = {"status": "pending", "stream_queue": Queue(), "error": None}
    model = request_data.get("model", "default")
    use_stream = request_data.get("stream", False)
    request_id = f"chatcmpl-{uuid.uuid4()}"
    if use_stream:
        def stream_response():
            for chunk in _openai_response_generator(task_id):
                yield format_openai_chunk(chunk, model, request_id)
            if RESULTS[task_id].get('error'):
                yield format_openai_chunk(f"[LMArena è‡ªåŠ¨åŒ–å·¥å…·é”™è¯¯]: {RESULTS[task_id]['error']}", model, request_id)
            yield format_openai_finish_chunk(model, request_id)
        return Response(stream_response(), mimetype='text/event-stream')
    else:
        full_response_content = "".join(list(_openai_response_generator(task_id)))
        if RESULTS[task_id].get('error'):
            return jsonify({"error": {"message": f"[LMArena è‡ªåŠ¨åŒ–å·¥å…·é”™è¯¯]: {RESULTS[task_id]['error']}", "type": "automator_error"}}), 500
        return jsonify(format_openai_non_stream_response(full_response_content, model, request_id))

if __name__ == '__main__':
    _load_config()
    if CONFIG.get("enable_comprehensive_logging"):
        log_dir = "Debug"
        os.makedirs(log_dir, exist_ok=True)
        log_filename = os.path.join(log_dir, f"debug_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log")
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', handlers=[logging.FileHandler(log_filename, encoding='utf-8'), logging.StreamHandler()])
        logger.info(f"èšåˆæ—¥å¿—å·²å¯ç”¨ã€‚æ—¥å¿—æ–‡ä»¶ä¿å­˜è‡³: {os.path.abspath(log_filename)}")
    else:
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', handlers=[logging.StreamHandler()])
    
    load_model_map()
    logger.info("="*60)
    logger.info("  ğŸš€ LMArena è‡ªåŠ¨åŒ–å·¥å…· - v12.2 (ä¸­æ–‡æœ¬åœ°åŒ–)")
    logger.info(f"  - ç›‘å¬åœ°å€: http://127.0.0.1:5102")
    
    # ä½¿ç”¨ä¸€ä¸ªå­—å…¸æ¥æ˜ å°„é…ç½®é”®å’Œå®ƒä»¬çš„ä¸­æ–‡åç§°
    config_keys_in_chinese = {
        "bypass_enabled": "Bypass æ¨¡å¼",
        "tavern_mode_enabled": "é…’é¦†æ¨¡å¼",
        "log_server_requests": "æœåŠ¡å™¨è¯·æ±‚æ—¥å¿—",
        "log_tampermonkey_debug": "æ²¹çŒ´è„šæœ¬è°ƒè¯•æ—¥å¿—",
        "enable_comprehensive_logging": "èšåˆæ—¥å¿—"
    }
    
    logger.info("\n  å½“å‰é…ç½®:")
    for key, name in config_keys_in_chinese.items():
        status = 'âœ… å·²å¯ç”¨' if CONFIG.get(key) else 'âŒ å·²ç¦ç”¨'
        logger.info(f"  - {name}: {status}")
        
    logger.info("\n  è¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ä¸€ä¸ª LMArena çš„ Direct Chat é¡µé¢ä»¥æ¿€æ´»æ²¹çŒ´è„šæœ¬ã€‚")
    logger.info("="*60)
    
    app.run(host='0.0.0.0', port=5102, threaded=True)