# local_openai_history_server.py
# v12.4 - Server-Side Port Balancing

import logging
import os
from flask import Flask, request, jsonify, Response
from flask_cors import CORS
from werkzeug.serving import run_simple
from queue import Queue, Empty
import uuid
import threading
import time
import json
import re
import random
from datetime import datetime
import requests
from packaging.version import parse as parse_version
import zipfile
import io
import sys
import subprocess

# --- å…¨å±€é…ç½® ---
CONFIG = {}
logger = logging.getLogger(__name__)

# --- Flask åº”ç”¨è®¾ç½® ---
app = Flask(__name__)
CORS(app)
werkzeug_logger = logging.getLogger('werkzeug')
werkzeug_logger.disabled = True

# --- æ•°æ®å­˜å‚¨ ---
PENDING_JOBS = Queue()
# { "tab_id": {"status": "idle"|"busy", "job": {}, "last_seen": timestamp, "task_id": "...", "sse_queue": Queue(), "port": 5103} }
TAB_SESSIONS = {}
SESSION_LOCK = threading.Lock()
RESULTS = {}
PORT_CONNECTIONS = {} # {5103: 2, 5104: 5}
# é˜²äººæœºæ£€æµ‹æŒ‚æœºæ± 
HANGING_TAB_ID = None
NEXT_HANGING_JOB_TIME = 0

# --- å¸¸é‡å®šä¹‰ ---
TASK_TIMEOUT_SECONDS = 300  # ä»»åŠ¡è¶…æ—¶æ—¶é—´ï¼ˆ5åˆ†é’Ÿï¼‰

# --- æ¨¡å‹æ˜ å°„ ---
MODEL_NAME_TO_ID_MAP = {}
DEFAULT_MODEL_ID = "f44e280a-7914-43ca-a25d-ecfcc5d48d09"

def load_model_map():
    global MODEL_NAME_TO_ID_MAP
    try:
        with open('models.json', 'r', encoding='utf-8') as f:
            MODEL_NAME_TO_ID_MAP = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        MODEL_NAME_TO_ID_MAP = {}

# --- æ¨¡å‹æ›´æ–°æ£€æŸ¥é€»è¾‘ ---
def extract_models_from_html(html_content):
    """
    ä» HTML å†…å®¹ä¸­æå–æ¨¡å‹æ•°æ®ï¼Œé‡‡ç”¨æ›´å¥å£®çš„è§£ææ–¹æ³•ã€‚
    """
    script_contents = re.findall(r'<script>(.*?)</script>', html_content, re.DOTALL)
    
    for script_content in script_contents:
        if 'self.__next_f.push' in script_content and 'initialState' in script_content and 'publicName' in script_content:
            match = re.search(r'self\.__next_f\.push\(\[1,"(.*?)"\]\)', script_content, re.DOTALL)
            if not match:
                continue
            
            full_payload = match.group(1)
            
            payload_string = full_payload.split('\\n')[0]
            
            json_start_index = payload_string.find(':')
            if json_start_index == -1:
                continue
            
            json_string_with_escapes = payload_string[json_start_index + 1:]
            json_string = json_string_with_escapes.replace('\\"', '"')
            
            try:
                data = json.loads(json_string)
                
                def find_initial_state(obj):
                    if isinstance(obj, dict):
                        for key, value in obj.items():
                            if key == 'initialState' and isinstance(value, list):
                                if value and isinstance(value[0], dict) and 'publicName' in value[0]:
                                    return value
                            result = find_initial_state(value)
                            if result is not None:
                                return result
                    elif isinstance(obj, list):
                        for item in obj:
                            result = find_initial_state(item)
                            if result is not None:
                                return result
                    return None

                models = find_initial_state(data)
                if models:
                    logger.info(f"æˆåŠŸä»è„šæœ¬å—ä¸­æå–åˆ° {len(models)} ä¸ªæ¨¡å‹ã€‚")
                    return models
            except json.JSONDecodeError as e:
                logger.error(f"è§£ææå–çš„JSONå­—ç¬¦ä¸²æ—¶å‡ºé”™: {e}")
                continue

    logger.error("é”™è¯¯ï¼šåœ¨HTMLå“åº”ä¸­æ‰¾ä¸åˆ°åŒ…å«æœ‰æ•ˆæ¨¡å‹æ•°æ®çš„è„šæœ¬å—ã€‚")
    return None

def compare_and_update_models(new_models_list, models_path):
    """
    æ¯”è¾ƒæ–°æ—§æ¨¡å‹åˆ—è¡¨ï¼Œæ‰“å°å·®å¼‚ï¼Œå¹¶ç”¨æ–°åˆ—è¡¨æ›´æ–°æœ¬åœ° models.json æ–‡ä»¶ã€‚
    """
    try:
        with open(models_path, 'r', encoding='utf-8') as f:
            old_models = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        old_models = {}

    new_models_dict = {model['publicName']: model for model in new_models_list if 'publicName' in model}
    old_models_set = set(old_models.keys())
    new_models_set = set(new_models_dict.keys())

    added_models = new_models_set - old_models_set
    removed_models = old_models_set - new_models_set
    
    logger.info("--- æ¨¡å‹æ›´æ–°æ£€æŸ¥ ---")
    has_changes = False

    if added_models:
        has_changes = True
        logger.info("\n[+] æ–°å¢æ¨¡å‹:")
        for name in added_models:
            model = new_models_dict[name]
            logger.info(f"  - åç§°: {name}, ID: {model.get('id')}, ç»„ç»‡: {model.get('organization', 'N/A')}")

    if removed_models:
        has_changes = True
        logger.info("\n[-] åˆ é™¤æ¨¡å‹:")
        for name in removed_models:
            logger.info(f"  - åç§°: {name}, ID: {old_models.get(name)}")

    logger.info("\n[*] å…±åŒæ¨¡å‹æ£€æŸ¥:")
    changed_models = 0
    for name in new_models_set.intersection(old_models_set):
        new_id = new_models_dict[name].get('id')
        old_id = old_models.get(name)
        if new_id != old_id:
            has_changes = True
            changed_models += 1
            logger.info(f"  - ID å˜æ›´: '{name}' æ—§ID: {old_id} -> æ–°ID: {new_id}")
    
    if changed_models == 0:
        logger.info("  - å…±åŒæ¨¡å‹çš„IDæ— å˜åŒ–ã€‚")

    if not has_changes:
        logger.info("\nç»“è®º: æ¨¡å‹åˆ—è¡¨æ— ä»»ä½•å˜åŒ–ï¼Œæ— éœ€æ›´æ–°æ–‡ä»¶ã€‚")
        logger.info("--- æ£€æŸ¥å®Œæ¯• ---")
        return

    logger.info("\nç»“è®º: æ£€æµ‹åˆ°æ¨¡å‹å˜æ›´ï¼Œæ­£åœ¨æ›´æ–° 'models.json'...")
    updated_model_map = {model['publicName']: model.get('id') for model in new_models_list if 'publicName' in model and 'id' in model}
    try:
        with open(models_path, 'w', encoding='utf-8') as f:
            json.dump(updated_model_map, f, indent=4, ensure_ascii=False)
        logger.info(f"'{models_path}' å·²æˆåŠŸæ›´æ–°ï¼ŒåŒ…å« {len(updated_model_map)} ä¸ªæ¨¡å‹ã€‚")
        load_model_map()
    except IOError as e:
        logger.error(f"å†™å…¥ '{models_path}' æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    logger.info("--- æ£€æŸ¥ä¸æ›´æ–°å®Œæ¯• ---")


# --- æ›´æ–°æ£€æŸ¥ ---
GITHUB_REPO = "Lianues/LMArenaBridge"

def download_and_extract_update(version):
    """ä¸‹è½½å¹¶è§£å‹æœ€æ–°ç‰ˆæœ¬åˆ°ä¸´æ—¶æ–‡ä»¶å¤¹ã€‚"""
    update_dir = "update_temp"
    if not os.path.exists(update_dir):
        os.makedirs(update_dir)

    try:
        zip_url = f"https://github.com/{GITHUB_REPO}/archive/refs/heads/main.zip"
        logger.info(f"æ­£åœ¨ä» {zip_url} ä¸‹è½½æ–°ç‰ˆæœ¬...")
        response = requests.get(zip_url, timeout=60)
        response.raise_for_status()

        with zipfile.ZipFile(io.BytesIO(response.content)) as z:
            z.extractall(update_dir)
        
        logger.info(f"æ–°ç‰ˆæœ¬å·²æˆåŠŸä¸‹è½½å¹¶è§£å‹åˆ° '{update_dir}' æ–‡ä»¶å¤¹ã€‚")
        return True
    except requests.RequestException as e:
        logger.error(f"ä¸‹è½½æ›´æ–°å¤±è´¥: {e}")
    except zipfile.BadZipFile:
        logger.error("ä¸‹è½½çš„æ–‡ä»¶ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„zipå‹ç¼©åŒ…ã€‚")
    except Exception as e:
        logger.error(f"è§£å‹æ›´æ–°æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    
    return False

def check_for_updates():
    """ä» GitHub æ£€æŸ¥æ–°ç‰ˆæœ¬ã€‚"""
    if not CONFIG.get("enable_auto_update", True):
        logger.info("è‡ªåŠ¨æ›´æ–°å·²ç¦ç”¨ï¼Œè·³è¿‡æ£€æŸ¥ã€‚")
        return

    current_version = CONFIG.get("version", "0.0.0")
    logger.info(f"å½“å‰ç‰ˆæœ¬: {current_version}ã€‚æ­£åœ¨ä» GitHub æ£€æŸ¥æ›´æ–°...")

    try:
        config_url = f"https://raw.githubusercontent.com/{GITHUB_REPO}/main/config.jsonc"
        response = requests.get(config_url, timeout=10)
        response.raise_for_status()

        jsonc_content = response.text
        json_content = re.sub(r'//.*', '', jsonc_content)
        json_content = re.sub(r'/\*.*?\*/', '', json_content, flags=re.DOTALL)
        remote_config = json.loads(json_content)
        
        remote_version_str = remote_config.get("version")
        if not remote_version_str:
            logger.warning("è¿œç¨‹é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ç‰ˆæœ¬å·ï¼Œè·³è¿‡æ›´æ–°æ£€æŸ¥ã€‚")
            return

        if parse_version(remote_version_str) > parse_version(current_version):
            logger.info("="*60)
            logger.info(f"ğŸ‰ å‘ç°æ–°ç‰ˆæœ¬! ğŸ‰")
            logger.info(f"  - å½“å‰ç‰ˆæœ¬: {current_version}")
            logger.info(f"  - æœ€æ–°ç‰ˆæœ¬: {remote_version_str}")
            if download_and_extract_update(remote_version_str):
                logger.info("å‡†å¤‡åº”ç”¨æ›´æ–°ã€‚æœåŠ¡å™¨å°†åœ¨5ç§’åå…³é—­å¹¶å¯åŠ¨æ›´æ–°è„šæœ¬ã€‚")
                time.sleep(5)
                update_script_path = os.path.join("modules", "update_script.py")
                subprocess.Popen([sys.executable, update_script_path])
                sys.exit(0)
            else:
                logger.error(f"è‡ªåŠ¨æ›´æ–°å¤±è´¥ã€‚è¯·è®¿é—® https://github.com/{GITHUB_REPO}/releases/latest æ‰‹åŠ¨ä¸‹è½½ã€‚")
            logger.info("="*60)
        else:
            logger.info("æ‚¨çš„ç¨‹åºå·²æ˜¯æœ€æ–°ç‰ˆæœ¬ã€‚")

    except requests.RequestException as e:
        logger.error(f"æ£€æŸ¥æ›´æ–°å¤±è´¥: {e}")
    except json.JSONDecodeError:
        logger.error("è§£æè¿œç¨‹é…ç½®æ–‡ä»¶å¤±è´¥ã€‚")
    except Exception as e:
        logger.error(f"æ£€æŸ¥æ›´æ–°æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")


# --- API ç«¯ç‚¹ ---
@app.route('/update_models', methods=['POST'])
def update_models():
    html_content = request.data.decode('utf-8')
    if not html_content:
        return jsonify({"status": "error", "message": "No HTML content received."}), 400
    
    logger.info("æ”¶åˆ°æ¥è‡ªæ²¹çŒ´è„šæœ¬çš„é¡µé¢å†…å®¹ï¼Œå¼€å§‹æ£€æŸ¥å¹¶æ›´æ–°æ¨¡å‹...")
    new_models_list = extract_models_from_html(html_content)
    
    if new_models_list:
        compare_and_update_models(new_models_list, 'models.json')
        return jsonify({"status": "success", "message": "Model comparison and update complete."})
    else:
        return jsonify({"status": "error", "message": "Could not extract model data from HTML."}), 400

@app.route('/get_config', methods=['GET'])
def get_config():
    try:
        with open('config.jsonc', 'r', encoding='utf-8') as f:
            jsonc_content = f.read()
            json_content = re.sub(r'//.*', '', jsonc_content)
            json_content = re.sub(r'/\*.*?\*/', '', json_content, flags=re.DOTALL)
            config_data = json.loads(json_content)
            # ä»é…ç½®ä¸­ç§»é™¤ worker_portsï¼Œä¸éœ€è¦å‘é€ç»™å®¢æˆ·ç«¯
            config_data.pop('worker_ports', None)
            return jsonify(config_data)
    except Exception as e:
        logger.error(f"è¯»å–æˆ–è§£æ config.jsonc å¤±è´¥: {e}")
        return jsonify({"error": "Config file issue"}), 500

@app.route('/get_worker_port', methods=['GET'])
def get_worker_port():
    """ä¸ºæ–°çš„æ ‡ç­¾é¡µåˆ†é…ä¸€ä¸ªè´Ÿè½½æœ€ä½çš„ Worker ç«¯å£ã€‚"""
    with SESSION_LOCK:
        worker_ports = CONFIG.get("worker_ports", [])
        if not worker_ports:
            return jsonify({"status": "error", "message": "No worker ports configured."}), 500

        # æ‰¾åˆ°è¿æ¥æ•°æœ€å°‘çš„ç«¯å£
        best_port = -1
        min_connections = float('inf')

        for port in worker_ports:
            connections = PORT_CONNECTIONS.get(port, 0)
            if connections < min_connections:
                min_connections = connections
                best_port = port
        
        # æ£€æŸ¥é€‰å‡ºçš„æœ€ä½³ç«¯å£æ˜¯å¦å·²æ»¡ï¼ˆä¾‹å¦‚æ¯ä¸ªç«¯å£é™åˆ¶6ä¸ªè¿æ¥ï¼‰
        if min_connections < 6:
            logger.info(f"ä¸ºæ–°æ ‡ç­¾é¡µåˆ†é…äº†ç«¯å£ {best_port} (å½“å‰è¿æ¥æ•°: {min_connections})")
            return jsonify({"status": "success", "port": best_port})
        else:
            logger.error(f"æ‰€æœ‰ Worker ç«¯å£ {worker_ports} çš„è¿æ¥æ•°éƒ½å·²è¾¾åˆ°æˆ–è¶…è¿‡6ä¸ªã€‚æ— æ³•åˆ†é…æ–°ç«¯å£ã€‚")
            return jsonify({"status": "error", "message": "All worker ports are at maximum capacity."}), 503

@app.route('/')
def index():
    return "LMArena è‡ªåŠ¨åŒ–å·¥å…· v12.2 (ä¸­æ–‡æœ¬åœ°åŒ–) æ­£åœ¨è¿è¡Œã€‚"

@app.route('/log_from_client', methods=['POST'])
def log_from_client():
    log_data = request.json
    if log_data and 'message' in log_data:
        logger.info(f"[æ²¹çŒ´è„šæœ¬] {log_data.get('level', 'INFO')}: {log_data['message']}")
    return jsonify({"status": "logged"})

# --- æ ¸å¿ƒé€»è¾‘ ---
def convert_openai_to_lmarena_templates(openai_data: dict) -> dict:
    model_name = openai_data.get("model", "claude-3-5-sonnet-20241022")
    target_model_id = MODEL_NAME_TO_ID_MAP.get(model_name, DEFAULT_MODEL_ID)
    message_templates = []
    for oai_msg in openai_data["messages"]:
        message_templates.append({"role": oai_msg["role"], "content": oai_msg.get("content", "")})
    if CONFIG.get("bypass_enabled"):
        message_templates.append({"role": "user", "content": " "})
    message_templates.append({"role": "assistant", "content": ""})
    return {"message_templates": message_templates, "target_model_id": target_model_id}

@app.route('/get_messages_job', methods=['GET'])
def get_messages_job():
    tab_id = request.args.get('tab_id')
    if not tab_id:
        return jsonify({"status": "error", "message": "tab_id is required"}), 400
    
    with SESSION_LOCK:
        session = TAB_SESSIONS.get(tab_id)
        if session and session.get('status') == 'busy' and session.get('job'):
            job_data = session['job'].get('messages_job')
            if job_data:
                # Check if logging for hanging tasks is enabled
                is_hanging = session.get('job', {}).get('is_hanging_job', False)
                if not is_hanging or CONFIG.get("log_hanging_pool_activity", True):
                    logger.info(f"æä¾› messages_job ç»™æ ‡ç­¾é¡µ {tab_id[:8]} (ä»»åŠ¡ {session['task_id'][:8]})")
                session['job']['messages_job'] = None
                return jsonify({"status": "success", "job": job_data})
            
    return jsonify({"status": "empty"})

@app.route('/events', methods=['GET'])
def events():
    tab_id = request.args.get('tab_id')
    is_hanging = request.args.get('is_hanging') == 'true'
    
    # è·å–å½“å‰è¿æ¥çš„ç«¯å£ã€‚Werkzeug ä¼šå°†å®ƒæ”¾å…¥ environã€‚
    port_str = request.environ.get('SERVER_PORT')
    if not port_str:
        logger.error("æ— æ³•ç¡®å®šSSEè¿æ¥çš„æœåŠ¡å™¨ç«¯å£ã€‚")
        return Response("Could not determine server port", status=500)
    port = int(port_str)

    if not tab_id:
        return Response("tab_id is required", status=400)

    def stream():
        q = Queue()
        with SESSION_LOCK:
            if tab_id not in TAB_SESSIONS:
                logger.info(f"æ–°çš„SSEè¿æ¥åœ¨ç«¯å£ {port} ä¸Šå»ºç«‹: {tab_id[:8]} (æŠ¥å‘ŠæŒ‚æœºçŠ¶æ€: {is_hanging})")
                PORT_CONNECTIONS[port] = PORT_CONNECTIONS.get(port, 0) + 1
                TAB_SESSIONS[tab_id] = {
                    "status": "idle", "job": None, "task_id": None,
                    "last_seen": time.time(), "sse_queue": q,
                    "is_hanging_client": is_hanging, "port": port,
                    "refresh_requested": False
                }
            else:
                old_port = TAB_SESSIONS[tab_id].get('port')
                logger.info(f"æ ‡ç­¾é¡µ {tab_id[:8]} åœ¨ç«¯å£ {port} ä¸Šé‡æ–°å»ºç«‹äº†SSEè¿æ¥ã€‚")
                if old_port and old_port != port:
                    logger.warning(f"æ ‡ç­¾é¡µ {tab_id[:8]} ä»æ—§ç«¯å£ {old_port} ç§»åŠ¨åˆ°äº†æ–°ç«¯å£ {port}ã€‚")
                    # å‡å°‘æ—§ç«¯å£è¿æ¥æ•°ï¼Œå¢åŠ æ–°ç«¯å£è¿æ¥æ•°
                    PORT_CONNECTIONS[old_port] = max(0, PORT_CONNECTIONS.get(old_port, 1) - 1)
                    PORT_CONNECTIONS[port] = PORT_CONNECTIONS.get(port, 0) + 1
                
                TAB_SESSIONS[tab_id].update({
                    'sse_queue': q, 'last_seen': time.time(),
                    'is_hanging_client': is_hanging, 'port': port,
                    'refresh_requested': False
                })

            # ç«‹å³åŒæ­¥æŒ‚æœºçŠ¶æ€ï¼Œç¡®ä¿å®¢æˆ·ç«¯çŠ¶æ€ä¸æœåŠ¡å™¨ä¸€è‡´
            is_currently_hanging = (tab_id == HANGING_TAB_ID)
            q.put(f"event: set_hanging_status\ndata: {json.dumps({'is_hanging': is_currently_hanging})}\n\n")
            logger.info(f"æ ‡ç­¾é¡µ {tab_id[:8]} SSEè¿æ¥æ—¶åŒæ­¥æŒ‚æœºçŠ¶æ€: {is_currently_hanging}")

            if TAB_SESSIONS[tab_id]['status'] == 'idle':
                try:
                    job_package = PENDING_JOBS.get_nowait()
                    task_id = job_package['task_id']
                    TAB_SESSIONS[tab_id]['job'] = job_package
                    TAB_SESSIONS[tab_id]['status'] = 'busy'
                    TAB_SESSIONS[tab_id]['task_id'] = task_id
                    
                    prompt_job_data = job_package.get('prompt_job')
                    if prompt_job_data:
                        prompt_job_data['type'] = 'prompt'
                        logger.info(f"é€šè¿‡æ–°å»ºç«‹çš„SSEè¿æ¥ï¼Œå°†å¾…å¤„ç†ä»»åŠ¡ {task_id[:8]} æ¨é€ç»™æ ‡ç­¾é¡µ {tab_id[:8]}")
                        q.put(f"event: new_job\ndata: {json.dumps(prompt_job_data)}\n\n")

                except Empty:
                    pass

        try:
            while True:
                message = q.get()
                yield message
        except GeneratorExit:
            # port å˜é‡åœ¨ stream å‡½æ•°çš„é—­åŒ…ä¸­æ˜¯å¯ç”¨çš„
            logger.info(f"SSEè¿æ¥å·²ç”±å®¢æˆ·ç«¯å…³é—­: {tab_id[:8]} (ç«¯å£: {port})")
            with SESSION_LOCK:
                if tab_id in TAB_SESSIONS:
                    TAB_SESSIONS[tab_id]['sse_queue'] = None
                    # æ³¨æ„ï¼šåœ¨è¿™é‡Œä¸å‡å°‘è¿æ¥è®¡æ•°ã€‚è¿æ¥è®¡æ•°å°†åœ¨ cleanup_and_dispatch_thread ä¸­å¤„ç†ï¼Œ
                    # å› ä¸ºé‚£é‡Œæ˜¯å”¯ä¸€ç¡®å®šæ€§åœ°æ¸…ç†åƒµå°¸ä¼šè¯çš„åœ°æ–¹ã€‚

    return Response(stream(), mimetype='text/event-stream')

@app.route('/stream_chunk', methods=['POST'])
def stream_chunk():
    data = request.json
    task_id = data.get('task_id')
    tab_id = data.get('tab_id')
    if task_id in RESULTS:
        RESULTS[task_id]['stream_queue'].put(data.get('chunk'))
        return jsonify({"status": "success"})
    logger.warning(f"ä»æ ‡ç­¾é¡µ {tab_id[:8] if tab_id else 'N/A'} æ”¶åˆ°äº†æœªçŸ¥ä»»åŠ¡ {task_id[:8] if task_id else 'N/A'} çš„æ•°æ®å—ã€‚")
    return jsonify({"status": "error", "message": "Task ID not found"}), 404

@app.route('/report_result', methods=['POST'])
def report_result():
    data = request.json
    task_id = data.get('task_id')
    tab_id = data.get('tab_id')
    
    if not tab_id:
        return jsonify({"status": "error", "message": "tab_id is required"}), 400

    if task_id in RESULTS:
        RESULTS[task_id]['status'] = data.get('status', 'completed')
        
        is_hanging = task_id.startswith("hanging-")
        log_activity = CONFIG.get("log_hanging_pool_activity", True)

        if not is_hanging or log_activity:
            logger.info(f"ä»»åŠ¡ {task_id[:8]} (æ¥è‡ªæ ‡ç­¾é¡µ {tab_id[:8]}) å·²è¢«å®¢æˆ·ç«¯æŠ¥å‘Šä¸ºå®Œæˆã€‚")
        
        with SESSION_LOCK:
            session = TAB_SESSIONS.get(tab_id)
            if session and session.get('task_id') == task_id:
                if not is_hanging or log_activity:
                    logger.info(f"æ ‡ç­¾é¡µ {tab_id[:8]} å·²å®Œæˆä»»åŠ¡ï¼ŒçŠ¶æ€é‡ç½®ä¸ºç©ºé—²ã€‚")
                session['status'] = 'idle'
                session['job'] = None
                session['task_id'] = None
            else:
                logger.warning(f"æŠ¥å‘Šå®Œæˆæ—¶ï¼Œæ ‡ç­¾é¡µ {tab_id[:8]} çš„ä¼šè¯çŠ¶æ€å¼‚å¸¸æˆ–ä»»åŠ¡IDä¸åŒ¹é…ã€‚")

        return jsonify({"status": "success"})
        
    logger.warning(f"ä»æ ‡ç­¾é¡µ {tab_id[:8]} æ”¶åˆ°äº†æœªçŸ¥ä»»åŠ¡ {task_id[:8] if task_id else 'N/A'} çš„å®ŒæˆæŠ¥å‘Šã€‚")
    return jsonify({"status": "error", "message": "Task ID not found"}), 404

def format_openai_chunk(content: str, model: str, request_id: str):
    return f"data: {json.dumps({'id': request_id, 'object': 'chat.completion.chunk', 'created': int(time.time()), 'model': model, 'choices': [{'index': 0, 'delta': {'content': content}, 'finish_reason': None}]})}\n\n"

def format_openai_finish_chunk(model: str, request_id: str, reason: str = 'stop'):
    return f"data: {json.dumps({'id': request_id, 'object': 'chat.completion.chunk', 'created': int(time.time()), 'model': model, 'choices': [{'index': 0, 'delta': {}, 'finish_reason': reason}]})}\n\ndata: [DONE]\n\n"

def format_openai_non_stream_response(content: str, model: str, request_id: str, reason: str = 'stop'):
    return {'id': request_id, 'object': 'chat.completion', 'created': int(time.time()), 'model': model, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': content}, 'finish_reason': reason}], 'usage': {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}}

def _normalize_message_content(message: dict) -> dict:
    content = message.get("content")
    
    # 1. å¤„ç†åˆ—è¡¨å½¢å¼çš„å†…å®¹ (ä¾‹å¦‚å¤šæ¨¡æ€è¾“å…¥)
    if isinstance(content, list):
        # æå–æ–‡æœ¬éƒ¨åˆ†å¹¶è¿æ¥
        text_parts = [p.get("text", "") for p in content if isinstance(p, dict) and p.get("type") == "text"]
        message["content"] = "\n\n".join(text_parts)
        content = message["content"] # è§„èŒƒåŒ–åæ›´æ–° content å˜é‡

    # 2. æ£€æŸ¥ user è§’è‰²çš„ç©ºå†…å®¹å¹¶æ›¿æ¢ä¸ºç©ºæ ¼
    if message.get("role") == "user" and content == "":
        message["content"] = " "
        
    return message

def _openai_response_generator(task_id: str):
    text_pattern = re.compile(r'a0:"((?:\\.|[^"\\])*)"')
    error_pattern = re.compile(r'(\{\s*"error".*?\})', re.DOTALL)
    finish_pattern = re.compile(r'"finishReason"\s*:\s*"(stop|content-filter)"')
    # Cloudflare æ£€æµ‹ç‰¹å¾
    cloudflare_patterns = [
        r'<title>Just a moment...</title>',
        r'Enable JavaScript and cookies to continue'
    ]
    
    buffer = ""
    RESULTS[task_id]['finish_reason'] = None
    timeout = CONFIG.get("stream_response_timeout_seconds", 120)

    while True:
        try:
            raw_chunk = RESULTS[task_id]['stream_queue'].get(timeout=timeout)
            buffer += raw_chunk

            # 1. æ£€æµ‹ Cloudflare äººæœºéªŒè¯
            for pattern in cloudflare_patterns:
                if re.search(pattern, buffer, re.IGNORECASE):
                    error_message = "æ£€æµ‹åˆ° Cloudflare äººæœºéªŒè¯é¡µé¢ã€‚è¯·åœ¨æµè§ˆå™¨ä¸­åˆ·æ–° LMArena é¡µé¢å¹¶æ‰‹åŠ¨å®ŒæˆéªŒè¯ï¼Œç„¶åé‡è¯•è¯·æ±‚ã€‚"
                    logger.error(f"ä»»åŠ¡ {task_id[:8]} æ£€æµ‹åˆ° Cloudflare éªŒè¯: {error_message}")
                    RESULTS[task_id]['error'] = error_message
                    return

            # 2. æ£€æµ‹ LMArena è¿”å›çš„é”™è¯¯
            error_match = error_pattern.search(buffer)
            if error_match:
                try:
                    error_json = json.loads(error_match.group(1))
                    error_message = error_json.get("error", "æ¥è‡ª LMArena çš„æœªçŸ¥é”™è¯¯")
                    logger.error(f"ä»»åŠ¡ {task_id[:8]} çš„æµå¼å“åº”ä¸­æ£€æµ‹åˆ°é”™è¯¯: {error_message}")
                    RESULTS[task_id]['error'] = str(error_message)
                    return
                except json.JSONDecodeError: pass

            # 3. æå–æ–‡æœ¬å†…å®¹
            while True:
                match = text_pattern.search(buffer)
                if not match: break
                try:
                    text_content = json.loads(f'"{match.group(1)}"')
                    if text_content: yield text_content
                except json.JSONDecodeError: pass
                buffer = buffer[match.end():]
            
            finish_match = finish_pattern.search(raw_chunk)
            if finish_match:
                reason = finish_match.group(1)
                logger.info(f"æ£€æµ‹åˆ°ä»»åŠ¡ {task_id[:8]} çš„ LMArena æµç»“æŸä¿¡å·ï¼ŒåŸå› : {reason}ã€‚")
                RESULTS[task_id]['finish_reason'] = reason
                return
        except Empty:
            logger.warning(f"ä»»åŠ¡ {task_id[:8]} çš„ç”Ÿæˆå™¨è¶…æ—¶ã€‚")
            RESULTS[task_id]['error'] = f'æµå¼å“åº”åœ¨{timeout}ç§’åè¶…æ—¶ã€‚'
            return

def _load_config():
    global CONFIG
    try:
        with open('config.jsonc', 'r', encoding='utf-8') as f:
            CONFIG = json.loads(re.sub(r'/\*.*?\*/', '', re.sub(r'//.*', '', f.read()), flags=re.DOTALL))
        logger.info("æˆåŠŸä» 'config.jsonc' åŠ è½½é…ç½®ã€‚")
        timeout_val = CONFIG.get("stream_response_timeout_seconds")
        if timeout_val:
            logger.info(f"é…ç½®çš„å“åº”è¶…æ—¶æ—¶é—´: {timeout_val} ç§’ã€‚")
        else:
            logger.warning("'stream_response_timeout_seconds' æœªåœ¨é…ç½®ä¸­æ‰¾åˆ°ï¼Œå°†ä½¿ç”¨ä»£ç ä¸­çš„é»˜è®¤å€¼ã€‚")
    except Exception as e:
        logging.error(f"æ— æ³•åŠ è½½æˆ–è§£æ 'config.jsonc': {e}ã€‚å°†ä½¿ç”¨é»˜è®¤è®¾ç½®ã€‚")
        CONFIG = {}

@app.route('/v1/models', methods=['GET'])
def list_models():
    return jsonify({"object": "list", "data": [{"id": name, "object": "model", "owned_by": "local-server"} for name in MODEL_NAME_TO_ID_MAP.keys()]})

@app.route('/v1/chat/completions', methods=['POST'])
def chat_completions():
    # API Key éªŒè¯
    api_key = CONFIG.get("api_key")
    if api_key:
        auth_header = request.headers.get('Authorization')
        if not auth_header or not auth_header.startswith('Bearer '):
            logger.warning("è¯·æ±‚ç¼ºå°‘æœ‰æ•ˆçš„ Authorization Bearer å¤´éƒ¨")
            return jsonify({"error": {"message": "æœªæä¾› API Keyã€‚è¯·åœ¨ Authorization å¤´éƒ¨ä¸­ä»¥ 'Bearer YOUR_KEY' æ ¼å¼æä¾›ã€‚", "type": "invalid_request_error", "code": "invalid_api_key"}}), 401
        
        provided_key = auth_header.split(' ')[1]
        if provided_key != api_key:
            logger.warning("æä¾›çš„ API Key ä¸æ­£ç¡®")
            return jsonify({"error": {"message": "æä¾›çš„ API Key ä¸æ­£ç¡®ã€‚", "type": "invalid_request_error", "code": "invalid_api_key"}}), 401

    request_data = request.json
    if CONFIG.get("log_server_requests"):
        logger.info(f"--- æ”¶åˆ° OpenAI è¯·æ±‚ ---\n{json.dumps(request_data, indent=2, ensure_ascii=False)}")
    if not request_data or "messages" not in request_data: return jsonify({"error": "è¯·æ±‚å¿…é¡»åŒ…å« 'messages'"}), 400
    request_data["messages"] = [_normalize_message_content(msg) for msg in request_data.get("messages", [])]
    if not request_data["messages"]: return jsonify({"error": "'messages' åˆ—è¡¨ä¸èƒ½ä¸ºç©º"}), 400
    if CONFIG.get("tavern_mode_enabled"):
        system_prompts = [msg['content'] for msg in request_data["messages"] if msg['role'] == 'system']
        other_messages = [msg for msg in request_data["messages"] if msg['role'] != 'system']
        merged_system_prompt = "\n\n".join(system_prompts)
        final_messages = []
        if merged_system_prompt: final_messages.append({"role": "system", "content": merged_system_prompt})
        final_messages.extend(other_messages)
        request_data["messages"] = final_messages
    messages_job = convert_openai_to_lmarena_templates(request_data)
    task_id = str(uuid.uuid4())
    
    messages_job['task_id'] = task_id
    
    prompt_job = {"task_id": task_id, "prompt": f"[è¿™æ¡æ¶ˆæ¯ä»…èµ·å ä½ï¼Œè¯·ä»¥å¤–éƒ¨åº”ç”¨ä¸­æ˜¾ç¤ºçš„å†…å®¹ä¸ºå‡†ï¼š/{task_id}]"}

    job_package = {
        "task_id": task_id,
        "messages_job": messages_job,
        "prompt_job": prompt_job
    }

    RESULTS[task_id] = {"status": "pending", "stream_queue": Queue(), "error": None}

    with SESSION_LOCK:
        # The background dispatcher now handles all logic, so we just queue the job.
        PENDING_JOBS.put(job_package)
        logger.info(f"æ–°ä»»åŠ¡ {task_id[:8]} å·²æ”¶åˆ°å¹¶æ”¾å…¥å¾…å¤„ç†é˜Ÿåˆ—ã€‚è°ƒåº¦å™¨å°†åœ¨åå°å¤„ç†ã€‚")
    model = request_data.get("model", "default")
    use_stream = request_data.get("stream", False)
    request_id = f"chatcmpl-{uuid.uuid4()}"
    if use_stream:
        def stream_response():
            for chunk in _openai_response_generator(task_id):
                yield format_openai_chunk(chunk, model, request_id)

            if RESULTS[task_id].get('error'):
                error_info = {
                    "error": {
                        "message": f"[LMArena è‡ªåŠ¨åŒ–å·¥å…·é”™è¯¯]: {RESULTS[task_id]['error']}",
                        "type": "automator_error"
                    }
                }
                yield f"data: {json.dumps(error_info)}\n\n"
                yield "data: [DONE]\n\n"
                return

            finish_reason = RESULTS[task_id].get('finish_reason')
            if finish_reason == 'content-filter':
                yield format_openai_chunk("\n\nå“åº”è¢«ç»ˆæ­¢ï¼Œå¯èƒ½æ˜¯ä¸Šä¸‹æ–‡è¶…é™æˆ–è€…æ¨¡å‹å†…éƒ¨å®¡æŸ¥çš„åŸå› ", model, request_id)
            
            yield format_openai_finish_chunk(model, request_id, reason=finish_reason or 'stop')
        return Response(stream_response(), mimetype='text/event-stream')
    else:
        full_response_content = "".join(list(_openai_response_generator(task_id)))
        if RESULTS[task_id].get('error'):
            return jsonify({"error": {"message": f"[LMArena è‡ªåŠ¨åŒ–å·¥å…·é”™è¯¯]: {RESULTS[task_id]['error']}", "type": "automator_error"}}), 500
        
        finish_reason = RESULTS[task_id].get('finish_reason', 'stop')
        if finish_reason == 'content-filter':
            full_response_content += "\n\nå“åº”è¢«ç»ˆæ­¢ï¼Œå¯èƒ½æ˜¯ä¸Šä¸‹æ–‡è¶…é™æˆ–è€…æ¨¡å‹å†…éƒ¨å®¡æŸ¥çš„åŸå› "
            
        return jsonify(format_openai_non_stream_response(full_response_content, model, request_id, reason=finish_reason))

def create_hanging_job_package():
    """
    åˆ›å»ºä¸€ä¸ªå®Œå…¨æ¨¡æ‹Ÿ OpenAI è¯·æ±‚çš„æŒ‚æœºä»»åŠ¡åŒ…ã€‚
    """
    # æ¨¡æ‹Ÿä¸€ä¸ªå¤–éƒ¨åº”ç”¨å‘æ¥çš„è¯·æ±‚ä½“
    request_data = {
        "model": "claude-3-5-sonnet-20241022", # æˆ–è€…ä»»ä½•ä¸€ä¸ªæœ‰æ•ˆçš„é»˜è®¤æ¨¡å‹
        "messages": [{"role": "user", "content": "ä½ å¥½"}]
    }

    # ä½¿ç”¨ä¸ /v1/chat/completions ç«¯ç‚¹å®Œå…¨ç›¸åŒçš„é€»è¾‘æ¥åˆ›å»ºä»»åŠ¡
    messages_job = convert_openai_to_lmarena_templates(request_data)
    task_id = f"hanging-{uuid.uuid4()}"
    messages_job['task_id'] = task_id
    
    prompt_job = {
        "task_id": task_id,
        "prompt": f"[é˜²äººæœºæ£€æµ‹æŒ‚æœºä»»åŠ¡]"}

    job_package = {
        "task_id": task_id,
        "messages_job": messages_job,
        "prompt_job": prompt_job,
        "is_hanging_job": True  # æ ‡è®°ä¸ºæŒ‚æœºä»»åŠ¡
    }

    # æ³¨å†Œä»»åŠ¡ä»¥è·Ÿè¸ªå…¶ç»“æœ
    RESULTS[task_id] = {"status": "pending", "stream_queue": Queue(), "error": None}
    
    return job_package

def cleanup_and_dispatch_thread():
    """
    ä¸€ä¸ªåå°çº¿ç¨‹ï¼Œè´Ÿè´£æ¸…ç†åƒµå°¸è¿æ¥ã€è°ƒåº¦å¾…å¤„ç†ä»»åŠ¡ä»¥åŠç®¡ç†é˜²äººæœºæ£€æµ‹æŒ‚æœºæ± ã€‚
    """
    global HANGING_TAB_ID, NEXT_HANGING_JOB_TIME

    while True:
        try:
            # Increased responsiveness for higher concurrency
            # Reduced from 2s to 0.5s to allow faster dispatching when multiple workers are available
            time.sleep(0.5)

            # è¯»å–é…ç½®ï¼Œç¡®ä¿æ˜¯æœ€æ–°çš„
            enable_hanging = CONFIG.get("enable_anti_bot_hanging", False)
            hanging_interval = CONFIG.get("hanging_interval_seconds", 120)

            with SESSION_LOCK:
                # --- 0. çŠ¶æ€è¯Šæ–­ ---
                num_pending = PENDING_JOBS.qsize()
                num_sessions = len(TAB_SESSIONS)
                idle_sessions = [sid[:8] for sid, s in TAB_SESSIONS.items() if s.get('status') == 'idle']
                # ä»…åœ¨æœ‰ä»»åŠ¡æˆ–æœ‰æŒ‚æœºæ´»åŠ¨æ—¶è®°å½•å¿ƒè·³ï¼Œä»¥å‡å°‘å™ªéŸ³
                if num_pending > 0 or (enable_hanging and num_sessions > 0):
                    logger.info(f"è°ƒåº¦å™¨å¿ƒè·³: {num_pending}ä¸ªå¾…å¤„ç†, {num_sessions}ä¸ªä¼šè¯ (ç©ºé—²: {idle_sessions if idle_sessions else 'æ— '}), æŒ‚æœºæ± : {HANGING_TAB_ID[:8] if HANGING_TAB_ID else 'æ— '}")

                # --- 1. Active Ping & Cleanup Phase ---
                zombie_tabs = []
                active_sessions = list(TAB_SESSIONS.items())

                current_time_for_timeout = time.time()

                for tab_id, session in active_sessions:
                    # æ ‡è®°åƒµå°¸ä¼šè¯çš„æ¡ä»¶:
                    # 1. SSEé˜Ÿåˆ—ä¸å­˜åœ¨ (å®¢æˆ·ç«¯å·²æ­£å¸¸æ–­å¼€)
                    # 2. Pingå¤±è´¥ (å®¢æˆ·ç«¯å¼‚å¸¸æ–­å¼€)
                    # 3. ä»»åŠ¡è¶…æ—¶ (å®¢æˆ·ç«¯å¯èƒ½æ— å“åº”)
                    is_zombie = False

                    # æ¡ä»¶ 1 & 2: è¿æ¥æ£€æŸ¥
                    if not session.get('sse_queue'):
                        is_zombie = True
                    else:
                        try:
                            session['sse_queue'].put_nowait(": ping\n\n")
                        except Exception:
                            is_zombie = True
                    
                    # æ¡ä»¶ 3: ä»»åŠ¡è¶…æ—¶æ£€æŸ¥
                    if not is_zombie and session.get('status') == 'busy' and 'task_start_time' in session:
                        if current_time_for_timeout - session['task_start_time'] > TASK_TIMEOUT_SECONDS:
                            if not session.get('refresh_requested', False):
                                # é¦–æ¬¡è¶…æ—¶ï¼Œå°è¯•å‘é€åˆ·æ–°è¯·æ±‚
                                logger.warning(f"è°ƒåº¦å™¨ï¼šä»»åŠ¡è¶…æ—¶ä½†è¿æ¥æ´»è·ƒã€‚å‘æ ‡ç­¾é¡µ {tab_id[:8]} (ä»»åŠ¡ {session['task_id'][:8]}) å‘é€åˆ·æ–°è¯·æ±‚ã€‚")
                                try:
                                    session['sse_queue'].put_nowait(f"event: refresh\ndata: {{}}\n\n")
                                    session['refresh_requested'] = True
                                    # é‡ç½®å¼€å§‹æ—¶é—´ï¼Œç»™äºˆåˆ·æ–°åé‡æ–°å¤„ç†çš„æ—¶é—´
                                    session['task_start_time'] = current_time_for_timeout
                                except Exception:
                                    # å¦‚æœå‘é€å¤±è´¥ï¼Œåˆ™ç«‹å³æ ‡è®°ä¸ºåƒµå°¸
                                    logger.error(f"è°ƒåº¦å™¨ï¼šå‘è¶…æ—¶æ ‡ç­¾é¡µ {tab_id[:8]} å‘é€åˆ·æ–°è¯·æ±‚å¤±è´¥ã€‚æ ‡è®°ä¸ºåƒµå°¸ã€‚")
                                    is_zombie = True
                            else:
                                # å·²ç»è¯·æ±‚è¿‡åˆ·æ–°ä½†ä»ç„¶è¶…æ—¶ï¼Œæ ‡è®°ä¸ºåƒµå°¸
                                logger.warning(f"è°ƒåº¦å™¨ï¼šæ ‡ç­¾é¡µ {tab_id[:8]} åœ¨è¯·æ±‚åˆ·æ–°åä»ç„¶è¶…æ—¶ (ä»»åŠ¡ {session['task_id'][:8]})ã€‚æ ‡è®°ä¸ºåƒµå°¸ä¼šè¯ã€‚")
                                is_zombie = True

                    if is_zombie:
                        zombie_tabs.append(tab_id)

                if zombie_tabs:
                    logger.warning(f"è°ƒåº¦å™¨ï¼šæ£€æµ‹åˆ° {len(zombie_tabs)} ä¸ªåƒµå°¸ä¼šè¯: {[tid[:8] for tid in zombie_tabs]}ï¼Œæ­£åœ¨æ¸…ç†ã€‚")
                    for tab_id in zombie_tabs:
                        session = TAB_SESSIONS.pop(tab_id, None)
                        if session:
                            port = session.get('port')
                            if port:
                                PORT_CONNECTIONS[port] = max(0, PORT_CONNECTIONS.get(port, 1) - 1)
                                logger.info(f"æ¸…ç†åƒµå°¸ä¼šè¯ {tab_id[:8]}ï¼Œç«¯å£ {port} è¿æ¥æ•°å‡è‡³ {PORT_CONNECTIONS[port]}")
                            
                            if tab_id == HANGING_TAB_ID:
                                logger.info(f"è°ƒåº¦å™¨ï¼šæŒ‚æœºæ ‡ç­¾é¡µ {tab_id[:8]} æ˜¯åƒµå°¸ï¼Œæ­£åœ¨é‡ç½®ã€‚")
                                HANGING_TAB_ID = None
                            
                            if session.get('status') == 'busy' and session.get('job'):
                                if not session['job'].get("is_hanging_job"):
                                    requeued_job = session['job']
                                    PENDING_JOBS.put(requeued_job)
                                    logger.warning(f"è°ƒåº¦å™¨ï¼šæ¥è‡ªåƒµå°¸ä¼šè¯ {tab_id[:8]} çš„ä»»åŠ¡ {requeued_job['task_id'][:8]} å·²è¢«é‡æ–°æ’é˜Ÿã€‚")
                                else:
                                    logger.info(f"è°ƒåº¦å™¨ï¼šä¸¢å¼ƒæ¥è‡ªåƒµå°¸ä¼šè¯ {tab_id[:8]} çš„æŒ‚æœºä»»åŠ¡ {session['task_id'][:8]}ã€‚")

                # --- 2. Anti-Bot Hanging Management Phase ---
                previous_hanging_id = HANGING_TAB_ID

                if enable_hanging and len(TAB_SESSIONS) >= 2:
                    if HANGING_TAB_ID is None or HANGING_TAB_ID not in TAB_SESSIONS:
                        # ä¼˜å…ˆé€‰æ‹©é‚£äº›æŠ¥å‘Šè‡ªå·±æ˜¯æŒ‚æœºçŠ¶æ€çš„æ ‡ç­¾é¡µ
                        preferred_tabs = [tid for tid, s in TAB_SESSIONS.items() if s.get('is_hanging_client')]

                        if preferred_tabs:
                            HANGING_TAB_ID = random.choice(preferred_tabs)
                            logger.info(f"è°ƒåº¦å™¨ï¼šå·²ä» {len(preferred_tabs)} ä¸ªå‰æŒ‚æœºæ ‡ç­¾é¡µä¸­ï¼Œé‡æ–°é€‰æ‹© {HANGING_TAB_ID[:8]} ä½œä¸ºæŒ‚æœºæ± ã€‚")
                        else:
                            # å¦‚æœæ²¡æœ‰ï¼Œåˆ™ä»æ‰€æœ‰å¯ç”¨æ ‡ç­¾é¡µä¸­éšæœºé€‰æ‹©
                            available_tabs = list(TAB_SESSIONS.keys())
                            if available_tabs:
                                HANGING_TAB_ID = random.choice(available_tabs)
                                logger.info(f"è°ƒåº¦å™¨ï¼šæ²¡æœ‰æ‰¾åˆ°å‰æŒ‚æœºæ ‡ç­¾é¡µï¼Œå·²éšæœºé€‰æ‹©æ–°æ ‡ç­¾é¡µ {HANGING_TAB_ID[:8]} ä½œä¸ºæŒ‚æœºæ± ã€‚")

                        if HANGING_TAB_ID:
                            NEXT_HANGING_JOB_TIME = time.time()
                else:
                    if HANGING_TAB_ID is not None:
                         logger.info(f"è°ƒåº¦å™¨ï¼šå› æ¡ä»¶ä¸æ»¡è¶³ï¼ˆå¯ç”¨: {enable_hanging}, æ ‡ç­¾é¡µæ•°: {len(TAB_SESSIONS)}ï¼‰ï¼Œå–æ¶ˆæŒ‚æœºæ¨¡å¼ã€‚")
                    HANGING_TAB_ID = None

                # --- çŠ¶æ€å˜æ›´é€šçŸ¥ ---
                if previous_hanging_id != HANGING_TAB_ID:
                    # é€šçŸ¥æ—§çš„æŒ‚æœºæ ‡ç­¾é¡µå–æ¶ˆçŠ¶æ€
                    if previous_hanging_id and previous_hanging_id in TAB_SESSIONS:
                        try:
                            TAB_SESSIONS[previous_hanging_id]['sse_queue'].put(f"event: set_hanging_status\ndata: {json.dumps({'is_hanging': False})}\n\n")
                            logger.info(f"é€šçŸ¥æ ‡ç­¾é¡µ {previous_hanging_id[:8]} å·²å–æ¶ˆæŒ‚æœºçŠ¶æ€ã€‚")
                        except Exception: pass
                    # é€šçŸ¥æ–°çš„æŒ‚æœºæ ‡ç­¾é¡µè®¾ç½®çŠ¶æ€
                    if HANGING_TAB_ID and HANGING_TAB_ID in TAB_SESSIONS:
                        try:
                            TAB_SESSIONS[HANGING_TAB_ID]['sse_queue'].put(f"event: set_hanging_status\ndata: {json.dumps({'is_hanging': True})}\n\n")
                            logger.info(f"é€šçŸ¥æ ‡ç­¾é¡µ {HANGING_TAB_ID[:8]} å·²è®¾ä¸ºæŒ‚æœºçŠ¶æ€ã€‚")
                        except Exception: pass


                # --- 3. Hanging Job Creation Phase ---
                if enable_hanging and HANGING_TAB_ID:
                    current_time = time.time()
                    has_pending_hanging_job = any(job.get('is_hanging_job') for job in list(PENDING_JOBS.queue))

                    if current_time >= NEXT_HANGING_JOB_TIME and not has_pending_hanging_job:
                        if CONFIG.get("log_hanging_pool_activity", True):
                            logger.info(f"è°ƒåº¦å™¨ï¼šåˆ›å»ºæ–°çš„æŒ‚æœºä»»åŠ¡å¹¶æ”¾å…¥é˜Ÿåˆ—ã€‚")
                        hanging_job_package = create_hanging_job_package()
                        PENDING_JOBS.put(hanging_job_package)
                        NEXT_HANGING_JOB_TIME = current_time + hanging_interval

                # --- 4. Dispatch Phase (Optimized for High Concurrency) ---
                # æŒç»­è°ƒåº¦ï¼Œç›´åˆ°é˜Ÿåˆ—ä¸ºç©ºæˆ–æ²¡æœ‰å¯ç”¨çš„ Worker (æ»¡è¶³ FIFO åŸåˆ™)
                while not PENDING_JOBS.empty():
                    # 1. è¯†åˆ«æ‰€æœ‰ç©ºé—² Worker
                    idle_sessions = {tid: s for tid, s in TAB_SESSIONS.items() if s.get('status') == 'idle'}
                    
                    if not idle_sessions:
                        # æ²¡æœ‰ç©ºé—² Workerï¼Œåœæ­¢æœ¬æ¬¡è°ƒåº¦å¾ªç¯
                        break

                    # 2. æŸ¥çœ‹é˜Ÿåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªä»»åŠ¡ (Peek)
                    try:
                        job_package = PENDING_JOBS.queue[0]
                    except IndexError:
                        break # é˜Ÿåˆ—å˜ç©º

                    is_hanging_job = job_package.get("is_hanging_job", False)
                    target_session_id = None

                    # 3. å¯»æ‰¾åˆé€‚çš„ Worker
                    if is_hanging_job:
                        # æŒ‚æœºä»»åŠ¡å¿…é¡»åˆ†é…ç»™æŒ‚æœºæ± æ ‡ç­¾é¡µ
                        if HANGING_TAB_ID and HANGING_TAB_ID in idle_sessions:
                            target_session_id = HANGING_TAB_ID
                        # å¦‚æœæŒ‚æœºæ± ä¸å¯ç”¨ï¼ŒæŒ‚æœºä»»åŠ¡ï¼ˆä½œä¸ºé˜Ÿé¦–ï¼‰å°†é˜»å¡é˜Ÿåˆ—ï¼Œç­‰å¾…ä¸‹ä¸€æ¬¡å¾ªç¯
                        
                    else:
                        # æ™®é€šä»»åŠ¡
                        # ä¼˜å…ˆé€‰æ‹©éæŒ‚æœºæ± çš„ç©ºé—² Worker
                        idle_non_hanging = [tid for tid in idle_sessions.keys() if tid != HANGING_TAB_ID]
                        
                        if idle_non_hanging:
                            # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„éæŒ‚æœº Worker
                            target_session_id = idle_non_hanging[0]
                        elif HANGING_TAB_ID and HANGING_TAB_ID in idle_sessions:
                            # å¦‚æœæ²¡æœ‰æ™®é€š Workerï¼Œåˆ™ä½¿ç”¨æŒ‚æœºæ±  Worker
                            target_session_id = HANGING_TAB_ID

                    # 4. åˆ†é…ä»»åŠ¡
                    if target_session_id:
                        try:
                            session = TAB_SESSIONS.get(target_session_id)
                            # å†æ¬¡ç¡®è®¤ Worker çŠ¶æ€ï¼ˆè™½ç„¶åœ¨é”å†…ï¼Œä½†ä½œä¸ºé˜²å¾¡æ€§ç¼–ç¨‹ï¼‰
                            if session and session['status'] == 'idle':
                                # æ­£å¼ä»é˜Ÿåˆ—ä¸­å–å‡ºä»»åŠ¡
                                job_to_dispatch = PENDING_JOBS.get()
                                
                                # éªŒè¯ (å¯é€‰ï¼Œä½†åœ¨å¹¶å‘ç¯å¢ƒä¸­å¾ˆé‡è¦)
                                if job_to_dispatch['task_id'] != job_package['task_id']:
                                     logger.error("ä¸¥é‡é”™è¯¯ï¼šè°ƒåº¦å™¨å–å‡ºçš„ä»»åŠ¡ä¸é¢„æœŸçš„ä¸ä¸€è‡´ï¼")
                                     PENDING_JOBS.put(job_to_dispatch) # æ”¾å›å»
                                     break

                                dispatch_job(target_session_id, session, job_to_dispatch)
                                
                                # å¦‚æœæ™®é€šä»»åŠ¡ä½¿ç”¨äº†æŒ‚æœºæ± ï¼Œæ¨è¿Ÿä¸‹ä¸€æ¬¡æŒ‚æœºä»»åŠ¡
                                if not is_hanging_job and target_session_id == HANGING_TAB_ID:
                                    # ç¡®ä¿ hanging_interval å·²å®šä¹‰
                                    hanging_interval = CONFIG.get("hanging_interval_seconds", 120)
                                    NEXT_HANGING_JOB_TIME = time.time() + hanging_interval
                                    logger.info(f"æŒ‚æœºæ ‡ç­¾é¡µè¢«ç”¨äºæ‰§è¡Œæ™®é€šä»»åŠ¡ï¼Œä¸‹ä¸€æ¬¡æŒ‚æœºä»»åŠ¡æ¨è¿Ÿã€‚")
                            else:
                                # Worker çŠ¶æ€æ„å¤–æ”¹å˜
                                break
                        except Empty:
                            break # é˜Ÿåˆ—çªç„¶ç©ºäº†
                    else:
                        # é˜Ÿé¦–ä»»åŠ¡æ— æ³•è°ƒåº¦ï¼ˆä¾‹å¦‚æŒ‚æœºä»»åŠ¡ä½†æŒ‚æœºæ± å¿™ç¢Œï¼‰ï¼Œåœæ­¢æœ¬æ¬¡è°ƒåº¦å¾ªç¯ä»¥ä¿æŒ FIFO
                        break

        except Exception:
            logger.error("è°ƒåº¦å™¨åå°çº¿ç¨‹å‘ç”Ÿè‡´å‘½é”™è¯¯ï¼å°†ä¼šåœ¨10ç§’åé‡è¯•ã€‚", exc_info=True)
            # To prevent a fast spinning loop of death if the error is persistent
            time.sleep(10)

def dispatch_job(tab_id, session, job_package):
    """è¾…åŠ©å‡½æ•°ï¼Œç”¨äºå°†ä»»åŠ¡å‘é€åˆ°æŒ‡å®šçš„æ ‡ç­¾é¡µä¼šè¯ã€‚"""
    global HANGING_TAB_ID
    session['status'] = 'busy'
    session['job'] = job_package
    session['task_id'] = job_package['task_id']
    session['last_seen'] = time.time()
    session['task_start_time'] = time.time() # è®°å½•ä»»åŠ¡å¼€å§‹æ—¶é—´ç”¨äºè¶…æ—¶æ£€æµ‹

    prompt_job_data = job_package.get('prompt_job')
    if prompt_job_data:
        prompt_job_data['type'] = 'prompt'
        try:
            if session['sse_queue']:
                # ç¡®ä¿åœ¨åˆ†é…ä»»åŠ¡æ—¶ï¼ŒæŒ‚æœºçŠ¶æ€ï¼ˆå’Œæ ‡é¢˜ï¼‰æ˜¯æœ€æ–°çš„
                is_currently_hanging = (tab_id == HANGING_TAB_ID)
                session['sse_queue'].put(f"event: set_hanging_status\ndata: {json.dumps({'is_hanging': is_currently_hanging})}\n\n")

                session['sse_queue'].put(f"event: new_job\ndata: {json.dumps(prompt_job_data)}\n\n")
                
                # Check if logging for hanging tasks is enabled
                is_hanging = job_package.get("is_hanging_job", False)
                if not is_hanging or CONFIG.get("log_hanging_pool_activity", True):
                    logger.info(f"è°ƒåº¦å™¨ï¼šå°†ä»»åŠ¡ {job_package['task_id'][:8]} åˆ†é…ç»™äº†æ ‡ç­¾é¡µ {tab_id[:8]}")
            else:
                raise Exception("SSE Queue is None")
        except Exception as e:
            logger.error(f"è°ƒåº¦å™¨ï¼šåœ¨åˆ†é…ä»»åŠ¡ç»™ {tab_id[:8]} æ—¶è¿æ¥å¤±æ•ˆ: {e}")
            # å¦‚æœæ˜¯æ™®é€šä»»åŠ¡ï¼Œé‡æ–°æ’é˜Ÿ
            if not job_package.get("is_hanging_job"):
                PENDING_JOBS.put(job_package)
            TAB_SESSIONS.pop(tab_id, None)
            if tab_id == HANGING_TAB_ID:
                HANGING_TAB_ID = None

if __name__ == '__main__':
    _load_config()
    if CONFIG.get("enable_comprehensive_logging"):
        log_dir = "Debug"
        os.makedirs(log_dir, exist_ok=True)
        log_filename = os.path.join(log_dir, f"debug_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log")
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', handlers=[logging.FileHandler(log_filename, encoding='utf-8'), logging.StreamHandler()])
        logger.info(f"èšåˆæ—¥å¿—å·²å¯ç”¨ã€‚æ—¥å¿—æ–‡ä»¶ä¿å­˜è‡³: {os.path.abspath(log_filename)}")
    else:
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', handlers=[logging.StreamHandler()])
    
    load_model_map()
    
    check_for_updates()

    # å¯åŠ¨åå°è°ƒåº¦çº¿ç¨‹
    dispatcher_thread = threading.Thread(target=cleanup_and_dispatch_thread, daemon=True)
    dispatcher_thread.start()
    logger.info("åå°ä»»åŠ¡è°ƒåº¦å™¨å·²å¯åŠ¨ã€‚")

    logger.info("="*60)
    logger.info("  ğŸš€ LMArena è‡ªåŠ¨åŒ–å·¥å…· - v12.3 (å¤šç«¯å£å¹¶å‘)")
    # logger.info(f"  - ç›‘å¬åœ°å€: http://127.0.0.1:5102")
    
    config_keys_in_chinese = {
        "enable_auto_update": "è‡ªåŠ¨æ›´æ–°",
        "bypass_enabled": "Bypass æ¨¡å¼",
        "tavern_mode_enabled": "é…’é¦†æ¨¡å¼",
        "log_server_requests": "æœåŠ¡å™¨è¯·æ±‚æ—¥å¿—",
        "log_tampermonkey_debug": "æ²¹çŒ´è„šæœ¬è°ƒè¯•æ—¥å¿—",
        "enable_comprehensive_logging": "èšåˆæ—¥å¿—",
        "enable_anti_bot_hanging": "é˜²äººæœºæ£€æµ‹æŒ‚æœº",
        "log_hanging_pool_activity": "æŒ‚æœºæ± æ´»åŠ¨æ—¥å¿—",
        "api_key": "API Key ä¿æŠ¤"
    }
    
    logger.info("\n  å½“å‰é…ç½®:")
    for key, name in config_keys_in_chinese.items():
        status = 'âœ… å·²å¯ç”¨' if CONFIG.get(key) else 'âŒ å·²ç¦ç”¨'
        logger.info(f"  - {name}: {status}")
        
    logger.info("\n  è¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ä¸€ä¸ª LMArena çš„ Direct Chat é¡µé¢ä»¥æ¿€æ´»æ²¹çŒ´è„šæœ¬ã€‚")
    logger.info("="*60)

    # --- å¤šç«¯å£å¯åŠ¨é€»è¾‘ (v12.4) ---
    api_port = CONFIG.get("api_port", 5102)
    worker_ports = CONFIG.get("worker_ports", [])
    
    # åˆå§‹åŒ–æ‰€æœ‰ worker ç«¯å£çš„è¿æ¥è®¡æ•°
    for p in worker_ports:
        PORT_CONNECTIONS[p] = 0

    all_ports = sorted(list(set([api_port] + worker_ports)))
    
    logger.info(f"ğŸŒ å‡†å¤‡åœ¨ä»¥ä¸‹ {len(all_ports)} ä¸ªç«¯å£ä¸Šå¯åŠ¨æœåŠ¡å™¨: {all_ports}")
    logger.info(f"  - API å…¥å£ç«¯å£: {api_port}")
    logger.info(f"  - æµè§ˆå™¨ Worker ç«¯å£: {worker_ports}")

    threads = []
    host = '0.0.0.0'

    for port in all_ports:
        try:
            port_num = int(port)
            # Werkzeug çš„ run_simple åœ¨ä¸€ä¸ªçº¿ç¨‹ä¸­è¿è¡Œ Flask åº”ç”¨ã€‚
            # æˆ‘ä»¬ä¸ºæ¯ä¸ªç«¯å£åˆ›å»ºä¸€ä¸ªç‹¬ç«‹çš„çº¿ç¨‹æ¥è¿è¡Œä¸€ä¸ªæœåŠ¡å™¨å®ä¾‹ã€‚
            # æ‰€æœ‰çº¿ç¨‹å…±äº«åŒä¸€ä¸ª Flask app å¯¹è±¡å’Œå…¨å±€å˜é‡ï¼Œå®ç°äº†çŠ¶æ€å…±äº«ã€‚
            t = threading.Thread(target=run_simple, args=(host, port_num, app), kwargs={'use_reloader': False, 'use_debugger': False, 'threaded': True})
            t.daemon = True
            threads.append(t)
            t.start()
            logger.info(f"  âœ… æœåŠ¡å™¨å·²åœ¨ http://{host}:{port_num} å¯åŠ¨")
        except Exception as e:
            logger.error(f"  âŒ æ— æ³•åœ¨ç«¯å£ {port} å¯åŠ¨æœåŠ¡å™¨: {e}")

    if not threads:
        logger.error("æœªèƒ½å¯åŠ¨ä»»ä½•æœåŠ¡å™¨å®ä¾‹ã€‚ç¨‹åºå°†é€€å‡ºã€‚")
        sys.exit(1)

    # ä¸»çº¿ç¨‹ç­‰å¾…æ‰€æœ‰æœåŠ¡å™¨çº¿ç¨‹ (è™½ç„¶å®ƒä»¬æ˜¯å®ˆæŠ¤çº¿ç¨‹ï¼Œä½†è¿™æ ·å¯ä»¥ä¿æŒä¸»ç¨‹åºè¿è¡Œ)
    try:
        while True:
            # ä¿æŒä¸»çº¿ç¨‹æ´»è·ƒï¼Œä»¥ä¾¿æ¥æ”¶ Ctrl+C
            time.sleep(1)
    except KeyboardInterrupt:
        logger.info("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å…³é—­æœåŠ¡å™¨...")
        # æ³¨æ„ï¼šç”±äº Werkzeug æœåŠ¡å™¨è¿è¡Œåœ¨å®ˆæŠ¤çº¿ç¨‹ä¸­ï¼Œå½“ä¸»çº¿ç¨‹é€€å‡ºæ—¶å®ƒä»¬ä¼šè‡ªåŠ¨åœæ­¢ã€‚