# local_openai_history_server.py
# v12.2 - Chinese Localization

import logging
import os
from flask import Flask, request, jsonify, Response
from flask_cors import CORS
from queue import Queue, Empty
import uuid
import threading
import time
import json
import re
import random
from datetime import datetime
import requests
from packaging.version import parse as parse_version
import zipfile
import io
import sys
import subprocess

# --- å…¨å±€é…ç½® ---
CONFIG = {}
logger = logging.getLogger(__name__)

# --- Flask åº”ç”¨è®¾ç½® ---
app = Flask(__name__)
CORS(app)
werkzeug_logger = logging.getLogger('werkzeug')
werkzeug_logger.disabled = True

# --- æ•°æ®å­˜å‚¨ ---
PENDING_JOBS = Queue()
TAB_SESSIONS = {}  # { "tab_id": {"status": "idle"|"busy", "job": {}, "last_seen": timestamp, "task_id": "...", "sse_queue": Queue()} }
SESSION_LOCK = threading.Lock()
RESULTS = {}
# é˜²äººæœºæ£€æµ‹æŒ‚æœºæ± 
HANGING_TAB_ID = None
NEXT_HANGING_JOB_TIME = 0

# --- æ¨¡å‹æ˜ å°„ ---
MODEL_NAME_TO_ID_MAP = {}
DEFAULT_MODEL_ID = "f44e280a-7914-43ca-a25d-ecfcc5d48d09"

def load_model_map():
    global MODEL_NAME_TO_ID_MAP
    try:
        with open('models.json', 'r', encoding='utf-8') as f:
            MODEL_NAME_TO_ID_MAP = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        MODEL_NAME_TO_ID_MAP = {}

# --- æ¨¡å‹æ›´æ–°æ£€æŸ¥é€»è¾‘ ---
def extract_models_from_html(html_content):
    """
    ä» HTML å†…å®¹ä¸­æå–æ¨¡å‹æ•°æ®ï¼Œé‡‡ç”¨æ›´å¥å£®çš„è§£ææ–¹æ³•ã€‚
    """
    script_contents = re.findall(r'<script>(.*?)</script>', html_content, re.DOTALL)
    
    for script_content in script_contents:
        if 'self.__next_f.push' in script_content and 'initialState' in script_content and 'publicName' in script_content:
            match = re.search(r'self\.__next_f\.push\(\[1,"(.*?)"\]\)', script_content, re.DOTALL)
            if not match:
                continue
            
            full_payload = match.group(1)
            
            payload_string = full_payload.split('\\n')[0]
            
            json_start_index = payload_string.find(':')
            if json_start_index == -1:
                continue
            
            json_string_with_escapes = payload_string[json_start_index + 1:]
            json_string = json_string_with_escapes.replace('\\"', '"')
            
            try:
                data = json.loads(json_string)
                
                def find_initial_state(obj):
                    if isinstance(obj, dict):
                        for key, value in obj.items():
                            if key == 'initialState' and isinstance(value, list):
                                if value and isinstance(value[0], dict) and 'publicName' in value[0]:
                                    return value
                            result = find_initial_state(value)
                            if result is not None:
                                return result
                    elif isinstance(obj, list):
                        for item in obj:
                            result = find_initial_state(item)
                            if result is not None:
                                return result
                    return None

                models = find_initial_state(data)
                if models:
                    logger.info(f"æˆåŠŸä»è„šæœ¬å—ä¸­æå–åˆ° {len(models)} ä¸ªæ¨¡å‹ã€‚")
                    return models
            except json.JSONDecodeError as e:
                logger.error(f"è§£ææå–çš„JSONå­—ç¬¦ä¸²æ—¶å‡ºé”™: {e}")
                continue

    logger.error("é”™è¯¯ï¼šåœ¨HTMLå“åº”ä¸­æ‰¾ä¸åˆ°åŒ…å«æœ‰æ•ˆæ¨¡å‹æ•°æ®çš„è„šæœ¬å—ã€‚")
    return None

def compare_and_update_models(new_models_list, models_path):
    """
    æ¯”è¾ƒæ–°æ—§æ¨¡å‹åˆ—è¡¨ï¼Œæ‰“å°å·®å¼‚ï¼Œå¹¶ç”¨æ–°åˆ—è¡¨æ›´æ–°æœ¬åœ° models.json æ–‡ä»¶ã€‚
    """
    try:
        with open(models_path, 'r', encoding='utf-8') as f:
            old_models = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        old_models = {}

    new_models_dict = {model['publicName']: model for model in new_models_list if 'publicName' in model}
    old_models_set = set(old_models.keys())
    new_models_set = set(new_models_dict.keys())

    added_models = new_models_set - old_models_set
    removed_models = old_models_set - new_models_set
    
    logger.info("--- æ¨¡å‹æ›´æ–°æ£€æŸ¥ ---")
    has_changes = False

    if added_models:
        has_changes = True
        logger.info("\n[+] æ–°å¢æ¨¡å‹:")
        for name in added_models:
            model = new_models_dict[name]
            logger.info(f"  - åç§°: {name}, ID: {model.get('id')}, ç»„ç»‡: {model.get('organization', 'N/A')}")

    if removed_models:
        has_changes = True
        logger.info("\n[-] åˆ é™¤æ¨¡å‹:")
        for name in removed_models:
            logger.info(f"  - åç§°: {name}, ID: {old_models.get(name)}")

    logger.info("\n[*] å…±åŒæ¨¡å‹æ£€æŸ¥:")
    changed_models = 0
    for name in new_models_set.intersection(old_models_set):
        new_id = new_models_dict[name].get('id')
        old_id = old_models.get(name)
        if new_id != old_id:
            has_changes = True
            changed_models += 1
            logger.info(f"  - ID å˜æ›´: '{name}' æ—§ID: {old_id} -> æ–°ID: {new_id}")
    
    if changed_models == 0:
        logger.info("  - å…±åŒæ¨¡å‹çš„IDæ— å˜åŒ–ã€‚")

    if not has_changes:
        logger.info("\nç»“è®º: æ¨¡å‹åˆ—è¡¨æ— ä»»ä½•å˜åŒ–ï¼Œæ— éœ€æ›´æ–°æ–‡ä»¶ã€‚")
        logger.info("--- æ£€æŸ¥å®Œæ¯• ---")
        return

    logger.info("\nç»“è®º: æ£€æµ‹åˆ°æ¨¡å‹å˜æ›´ï¼Œæ­£åœ¨æ›´æ–° 'models.json'...")
    updated_model_map = {model['publicName']: model.get('id') for model in new_models_list if 'publicName' in model and 'id' in model}
    try:
        with open(models_path, 'w', encoding='utf-8') as f:
            json.dump(updated_model_map, f, indent=4, ensure_ascii=False)
        logger.info(f"'{models_path}' å·²æˆåŠŸæ›´æ–°ï¼ŒåŒ…å« {len(updated_model_map)} ä¸ªæ¨¡å‹ã€‚")
        load_model_map()
    except IOError as e:
        logger.error(f"å†™å…¥ '{models_path}' æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    logger.info("--- æ£€æŸ¥ä¸æ›´æ–°å®Œæ¯• ---")


# --- æ›´æ–°æ£€æŸ¥ ---
GITHUB_REPO = "Lianues/LMArenaBridge"

def download_and_extract_update(version):
    """ä¸‹è½½å¹¶è§£å‹æœ€æ–°ç‰ˆæœ¬åˆ°ä¸´æ—¶æ–‡ä»¶å¤¹ã€‚"""
    update_dir = "update_temp"
    if not os.path.exists(update_dir):
        os.makedirs(update_dir)

    try:
        zip_url = f"https://github.com/{GITHUB_REPO}/archive/refs/heads/main.zip"
        logger.info(f"æ­£åœ¨ä» {zip_url} ä¸‹è½½æ–°ç‰ˆæœ¬...")
        response = requests.get(zip_url, timeout=60)
        response.raise_for_status()

        with zipfile.ZipFile(io.BytesIO(response.content)) as z:
            z.extractall(update_dir)
        
        logger.info(f"æ–°ç‰ˆæœ¬å·²æˆåŠŸä¸‹è½½å¹¶è§£å‹åˆ° '{update_dir}' æ–‡ä»¶å¤¹ã€‚")
        return True
    except requests.RequestException as e:
        logger.error(f"ä¸‹è½½æ›´æ–°å¤±è´¥: {e}")
    except zipfile.BadZipFile:
        logger.error("ä¸‹è½½çš„æ–‡ä»¶ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„zipå‹ç¼©åŒ…ã€‚")
    except Exception as e:
        logger.error(f"è§£å‹æ›´æ–°æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    
    return False

def check_for_updates():
    """ä» GitHub æ£€æŸ¥æ–°ç‰ˆæœ¬ã€‚"""
    if not CONFIG.get("enable_auto_update", True):
        logger.info("è‡ªåŠ¨æ›´æ–°å·²ç¦ç”¨ï¼Œè·³è¿‡æ£€æŸ¥ã€‚")
        return

    current_version = CONFIG.get("version", "0.0.0")
    logger.info(f"å½“å‰ç‰ˆæœ¬: {current_version}ã€‚æ­£åœ¨ä» GitHub æ£€æŸ¥æ›´æ–°...")

    try:
        config_url = f"https://raw.githubusercontent.com/{GITHUB_REPO}/main/config.jsonc"
        response = requests.get(config_url, timeout=10)
        response.raise_for_status()

        jsonc_content = response.text
        json_content = re.sub(r'//.*', '', jsonc_content)
        json_content = re.sub(r'/\*.*?\*/', '', json_content, flags=re.DOTALL)
        remote_config = json.loads(json_content)
        
        remote_version_str = remote_config.get("version")
        if not remote_version_str:
            logger.warning("è¿œç¨‹é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ç‰ˆæœ¬å·ï¼Œè·³è¿‡æ›´æ–°æ£€æŸ¥ã€‚")
            return

        if parse_version(remote_version_str) > parse_version(current_version):
            logger.info("="*60)
            logger.info(f"ğŸ‰ å‘ç°æ–°ç‰ˆæœ¬! ğŸ‰")
            logger.info(f"  - å½“å‰ç‰ˆæœ¬: {current_version}")
            logger.info(f"  - æœ€æ–°ç‰ˆæœ¬: {remote_version_str}")
            if download_and_extract_update(remote_version_str):
                logger.info("å‡†å¤‡åº”ç”¨æ›´æ–°ã€‚æœåŠ¡å™¨å°†åœ¨5ç§’åå…³é—­å¹¶å¯åŠ¨æ›´æ–°è„šæœ¬ã€‚")
                time.sleep(5)
                update_script_path = os.path.join("modules", "update_script.py")
                subprocess.Popen([sys.executable, update_script_path])
                sys.exit(0)
            else:
                logger.error(f"è‡ªåŠ¨æ›´æ–°å¤±è´¥ã€‚è¯·è®¿é—® https://github.com/{GITHUB_REPO}/releases/latest æ‰‹åŠ¨ä¸‹è½½ã€‚")
            logger.info("="*60)
        else:
            logger.info("æ‚¨çš„ç¨‹åºå·²æ˜¯æœ€æ–°ç‰ˆæœ¬ã€‚")

    except requests.RequestException as e:
        logger.error(f"æ£€æŸ¥æ›´æ–°å¤±è´¥: {e}")
    except json.JSONDecodeError:
        logger.error("è§£æè¿œç¨‹é…ç½®æ–‡ä»¶å¤±è´¥ã€‚")
    except Exception as e:
        logger.error(f"æ£€æŸ¥æ›´æ–°æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")


# --- API ç«¯ç‚¹ ---
@app.route('/update_models', methods=['POST'])
def update_models():
    html_content = request.data.decode('utf-8')
    if not html_content:
        return jsonify({"status": "error", "message": "No HTML content received."}), 400
    
    logger.info("æ”¶åˆ°æ¥è‡ªæ²¹çŒ´è„šæœ¬çš„é¡µé¢å†…å®¹ï¼Œå¼€å§‹æ£€æŸ¥å¹¶æ›´æ–°æ¨¡å‹...")
    new_models_list = extract_models_from_html(html_content)
    
    if new_models_list:
        compare_and_update_models(new_models_list, 'models.json')
        return jsonify({"status": "success", "message": "Model comparison and update complete."})
    else:
        return jsonify({"status": "error", "message": "Could not extract model data from HTML."}), 400

@app.route('/get_config', methods=['GET'])
def get_config():
    try:
        with open('config.jsonc', 'r', encoding='utf-8') as f:
            jsonc_content = f.read()
            json_content = re.sub(r'//.*', '', jsonc_content)
            json_content = re.sub(r'/\*.*?\*/', '', json_content, flags=re.DOTALL)
            return jsonify(json.loads(json_content))
    except Exception as e:
        logger.error(f"è¯»å–æˆ–è§£æ config.jsonc å¤±è´¥: {e}")
        return jsonify({"error": "Config file issue"}), 500

@app.route('/')
def index():
    return "LMArena è‡ªåŠ¨åŒ–å·¥å…· v12.2 (ä¸­æ–‡æœ¬åœ°åŒ–) æ­£åœ¨è¿è¡Œã€‚"

@app.route('/log_from_client', methods=['POST'])
def log_from_client():
    log_data = request.json
    if log_data and 'message' in log_data:
        logger.info(f"[æ²¹çŒ´è„šæœ¬] {log_data.get('level', 'INFO')}: {log_data['message']}")
    return jsonify({"status": "logged"})

# --- æ ¸å¿ƒé€»è¾‘ ---
def convert_openai_to_lmarena_templates(openai_data: dict) -> dict:
    model_name = openai_data.get("model", "claude-3-5-sonnet-20241022")
    target_model_id = MODEL_NAME_TO_ID_MAP.get(model_name, DEFAULT_MODEL_ID)
    message_templates = []
    for oai_msg in openai_data["messages"]:
        message_templates.append({"role": oai_msg["role"], "content": oai_msg.get("content", "")})
    if CONFIG.get("bypass_enabled"):
        message_templates.append({"role": "user", "content": " "})
    message_templates.append({"role": "assistant", "content": ""})
    return {"message_templates": message_templates, "target_model_id": target_model_id}

@app.route('/get_messages_job', methods=['GET'])
def get_messages_job():
    tab_id = request.args.get('tab_id')
    if not tab_id:
        return jsonify({"status": "error", "message": "tab_id is required"}), 400
    
    with SESSION_LOCK:
        session = TAB_SESSIONS.get(tab_id)
        if session and session.get('status') == 'busy' and session.get('job'):
            job_data = session['job'].get('messages_job')
            if job_data:
                logger.info(f"æä¾› messages_job ç»™æ ‡ç­¾é¡µ {tab_id[:8]} (ä»»åŠ¡ {session['task_id'][:8]})")
                session['job']['messages_job'] = None
                return jsonify({"status": "success", "job": job_data})
            
    return jsonify({"status": "empty"})

@app.route('/events', methods=['GET'])
def events():
    tab_id = request.args.get('tab_id')
    if not tab_id:
        return Response("tab_id is required", status=400)

    def stream():
        q = Queue()
        with SESSION_LOCK:
            if tab_id not in TAB_SESSIONS:
                logger.info(f"æ–°çš„SSEè¿æ¥å·²å»ºç«‹: {tab_id[:8]}")
                TAB_SESSIONS[tab_id] = {"status": "idle", "job": None, "task_id": None, "last_seen": time.time(), "sse_queue": q}
            else:
                logger.info(f"æ ‡ç­¾é¡µ {tab_id[:8]} é‡æ–°å»ºç«‹äº†SSEè¿æ¥ã€‚")
                TAB_SESSIONS[tab_id]['sse_queue'] = q
                TAB_SESSIONS[tab_id]['last_seen'] = time.time()
            
            if TAB_SESSIONS[tab_id]['status'] == 'idle':
                try:
                    job_package = PENDING_JOBS.get_nowait()
                    task_id = job_package['task_id']
                    TAB_SESSIONS[tab_id]['job'] = job_package
                    TAB_SESSIONS[tab_id]['status'] = 'busy'
                    TAB_SESSIONS[tab_id]['task_id'] = task_id
                    
                    prompt_job_data = job_package.get('prompt_job')
                    if prompt_job_data:
                        prompt_job_data['type'] = 'prompt'
                        logger.info(f"é€šè¿‡æ–°å»ºç«‹çš„SSEè¿æ¥ï¼Œå°†å¾…å¤„ç†ä»»åŠ¡ {task_id[:8]} æ¨é€ç»™æ ‡ç­¾é¡µ {tab_id[:8]}")
                        q.put(f"event: new_job\ndata: {json.dumps(prompt_job_data)}\n\n")

                except Empty:
                    pass

        try:
            while True:
                message = q.get()
                yield message
        except GeneratorExit:
            logger.info(f"SSEè¿æ¥å·²ç”±å®¢æˆ·ç«¯å…³é—­: {tab_id[:8]}")
            with SESSION_LOCK:
                if tab_id in TAB_SESSIONS:
                    TAB_SESSIONS[tab_id]['sse_queue'] = None

    return Response(stream(), mimetype='text/event-stream')

@app.route('/stream_chunk', methods=['POST'])
def stream_chunk():
    data = request.json
    task_id = data.get('task_id')
    tab_id = data.get('tab_id')
    if task_id in RESULTS:
        RESULTS[task_id]['stream_queue'].put(data.get('chunk'))
        return jsonify({"status": "success"})
    logger.warning(f"ä»æ ‡ç­¾é¡µ {tab_id[:8] if tab_id else 'N/A'} æ”¶åˆ°äº†æœªçŸ¥ä»»åŠ¡ {task_id[:8] if task_id else 'N/A'} çš„æ•°æ®å—ã€‚")
    return jsonify({"status": "error", "message": "Task ID not found"}), 404

@app.route('/report_result', methods=['POST'])
def report_result():
    data = request.json
    task_id = data.get('task_id')
    tab_id = data.get('tab_id')
    
    if not tab_id:
        return jsonify({"status": "error", "message": "tab_id is required"}), 400

    if task_id in RESULTS:
        RESULTS[task_id]['status'] = data.get('status', 'completed')
        logger.info(f"ä»»åŠ¡ {task_id[:8]} (æ¥è‡ªæ ‡ç­¾é¡µ {tab_id[:8]}) å·²è¢«å®¢æˆ·ç«¯æŠ¥å‘Šä¸ºå®Œæˆã€‚")
        
        with SESSION_LOCK:
            session = TAB_SESSIONS.get(tab_id)
            if session and session.get('task_id') == task_id:
                logger.info(f"æ ‡ç­¾é¡µ {tab_id[:8]} å·²å®Œæˆä»»åŠ¡ï¼ŒçŠ¶æ€é‡ç½®ä¸ºç©ºé—²ã€‚")
                session['status'] = 'idle'
                session['job'] = None
                session['task_id'] = None
            else:
                logger.warning(f"æŠ¥å‘Šå®Œæˆæ—¶ï¼Œæ ‡ç­¾é¡µ {tab_id[:8]} çš„ä¼šè¯çŠ¶æ€å¼‚å¸¸æˆ–ä»»åŠ¡IDä¸åŒ¹é…ã€‚")

        return jsonify({"status": "success"})
        
    logger.warning(f"ä»æ ‡ç­¾é¡µ {tab_id[:8]} æ”¶åˆ°äº†æœªçŸ¥ä»»åŠ¡ {task_id[:8] if task_id else 'N/A'} çš„å®ŒæˆæŠ¥å‘Šã€‚")
    return jsonify({"status": "error", "message": "Task ID not found"}), 404

def format_openai_chunk(content: str, model: str, request_id: str):
    return f"data: {json.dumps({'id': request_id, 'object': 'chat.completion.chunk', 'created': int(time.time()), 'model': model, 'choices': [{'index': 0, 'delta': {'content': content}, 'finish_reason': None}]})}\n\n"

def format_openai_finish_chunk(model: str, request_id: str, reason: str = 'stop'):
    return f"data: {json.dumps({'id': request_id, 'object': 'chat.completion.chunk', 'created': int(time.time()), 'model': model, 'choices': [{'index': 0, 'delta': {}, 'finish_reason': reason}]})}\n\ndata: [DONE]\n\n"

def format_openai_non_stream_response(content: str, model: str, request_id: str, reason: str = 'stop'):
    return {'id': request_id, 'object': 'chat.completion', 'created': int(time.time()), 'model': model, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': content}, 'finish_reason': reason}], 'usage': {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}}

def _normalize_message_content(message: dict) -> dict:
    content = message.get("content")
    if isinstance(content, list):
        message["content"] = "\n\n".join([p.get("text", "") for p in content if isinstance(p, dict) and p.get("type") == "text"])
    return message

def _openai_response_generator(task_id: str):
    text_pattern = re.compile(r'a0:"((?:\\.|[^"\\])*)"')
    error_pattern = re.compile(r'(\{\s*"error".*?\})', re.DOTALL)
    finish_pattern = re.compile(r'"finishReason"\s*:\s*"(stop|content-filter)"')
    buffer = ""
    RESULTS[task_id]['finish_reason'] = None
    timeout = CONFIG.get("stream_response_timeout_seconds", 120)

    while True:
        try:
            raw_chunk = RESULTS[task_id]['stream_queue'].get(timeout=timeout)
            buffer += raw_chunk
            error_match = error_pattern.search(buffer)
            if error_match:
                try:
                    error_json = json.loads(error_match.group(1))
                    error_message = error_json.get("error", "æ¥è‡ª LMArena çš„æœªçŸ¥é”™è¯¯")
                    logger.error(f"ä»»åŠ¡ {task_id[:8]} çš„æµå¼å“åº”ä¸­æ£€æµ‹åˆ°é”™è¯¯: {error_message}")
                    RESULTS[task_id]['error'] = str(error_message)
                    return
                except json.JSONDecodeError: pass
            while True:
                match = text_pattern.search(buffer)
                if not match: break
                try:
                    text_content = json.loads(f'"{match.group(1)}"')
                    if text_content: yield text_content
                except json.JSONDecodeError: pass
                buffer = buffer[match.end():]
            
            finish_match = finish_pattern.search(raw_chunk)
            if finish_match:
                reason = finish_match.group(1)
                logger.info(f"æ£€æµ‹åˆ°ä»»åŠ¡ {task_id[:8]} çš„ LMArena æµç»“æŸä¿¡å·ï¼ŒåŸå› : {reason}ã€‚")
                RESULTS[task_id]['finish_reason'] = reason
                return
        except Empty:
            logger.warning(f"ä»»åŠ¡ {task_id[:8]} çš„ç”Ÿæˆå™¨è¶…æ—¶ã€‚")
            RESULTS[task_id]['error'] = f'æµå¼å“åº”åœ¨{timeout}ç§’åè¶…æ—¶ã€‚'
            return

def _load_config():
    global CONFIG
    try:
        with open('config.jsonc', 'r', encoding='utf-8') as f:
            CONFIG = json.loads(re.sub(r'/\*.*?\*/', '', re.sub(r'//.*', '', f.read()), flags=re.DOTALL))
    except Exception as e:
        logging.error(f"æ— æ³•åŠ è½½ config.jsonc: {e}ã€‚å°†ä½¿ç”¨é»˜è®¤è®¾ç½®ã€‚")
        CONFIG = {"enable_comprehensive_logging": False}

@app.route('/v1/models', methods=['GET'])
def list_models():
    return jsonify({"object": "list", "data": [{"id": name, "object": "model", "owned_by": "local-server"} for name in MODEL_NAME_TO_ID_MAP.keys()]})

@app.route('/v1/chat/completions', methods=['POST'])
def chat_completions():
    # API Key éªŒè¯
    api_key = CONFIG.get("api_key")
    if api_key:
        auth_header = request.headers.get('Authorization')
        if not auth_header or not auth_header.startswith('Bearer '):
            logger.warning("è¯·æ±‚ç¼ºå°‘æœ‰æ•ˆçš„ Authorization Bearer å¤´éƒ¨")
            return jsonify({"error": {"message": "æœªæä¾› API Keyã€‚è¯·åœ¨ Authorization å¤´éƒ¨ä¸­ä»¥ 'Bearer YOUR_KEY' æ ¼å¼æä¾›ã€‚", "type": "invalid_request_error", "code": "invalid_api_key"}}), 401
        
        provided_key = auth_header.split(' ')[1]
        if provided_key != api_key:
            logger.warning("æä¾›çš„ API Key ä¸æ­£ç¡®")
            return jsonify({"error": {"message": "æä¾›çš„ API Key ä¸æ­£ç¡®ã€‚", "type": "invalid_request_error", "code": "invalid_api_key"}}), 401

    request_data = request.json
    if CONFIG.get("log_server_requests"):
        logger.info(f"--- æ”¶åˆ° OpenAI è¯·æ±‚ ---\n{json.dumps(request_data, indent=2, ensure_ascii=False)}")
    if not request_data or "messages" not in request_data: return jsonify({"error": "è¯·æ±‚å¿…é¡»åŒ…å« 'messages'"}), 400
    request_data["messages"] = [_normalize_message_content(msg) for msg in request_data.get("messages", [])]
    if not request_data["messages"]: return jsonify({"error": "'messages' åˆ—è¡¨ä¸èƒ½ä¸ºç©º"}), 400
    if CONFIG.get("tavern_mode_enabled"):
        system_prompts = [msg['content'] for msg in request_data["messages"] if msg['role'] == 'system']
        other_messages = [msg for msg in request_data["messages"] if msg['role'] != 'system']
        merged_system_prompt = "\n\n".join(system_prompts)
        final_messages = []
        if merged_system_prompt: final_messages.append({"role": "system", "content": merged_system_prompt})
        final_messages.extend(other_messages)
        request_data["messages"] = final_messages
    messages_job = convert_openai_to_lmarena_templates(request_data)
    task_id = str(uuid.uuid4())
    
    messages_job['task_id'] = task_id
    
    prompt_job = {"task_id": task_id, "prompt": f"[è¿™æ¡æ¶ˆæ¯ä»…èµ·å ä½ï¼Œè¯·ä»¥å¤–éƒ¨åº”ç”¨ä¸­æ˜¾ç¤ºçš„å†…å®¹ä¸ºå‡†ï¼š/{task_id}]"}

    job_package = {
        "task_id": task_id,
        "messages_job": messages_job,
        "prompt_job": prompt_job
    }

    RESULTS[task_id] = {"status": "pending", "stream_queue": Queue(), "error": None}

    with SESSION_LOCK:
        # The background dispatcher now handles all logic, so we just queue the job.
        PENDING_JOBS.put(job_package)
        logger.info(f"æ–°ä»»åŠ¡ {task_id[:8]} å·²æ”¶åˆ°å¹¶æ”¾å…¥å¾…å¤„ç†é˜Ÿåˆ—ã€‚è°ƒåº¦å™¨å°†åœ¨åå°å¤„ç†ã€‚")
    model = request_data.get("model", "default")
    use_stream = request_data.get("stream", False)
    request_id = f"chatcmpl-{uuid.uuid4()}"
    if use_stream:
        def stream_response():
            for chunk in _openai_response_generator(task_id):
                yield format_openai_chunk(chunk, model, request_id)

            if RESULTS[task_id].get('error'):
                yield format_openai_chunk(f"[LMArena è‡ªåŠ¨åŒ–å·¥å…·é”™è¯¯]: {RESULTS[task_id]['error']}", model, request_id)
                yield format_openai_finish_chunk(model, request_id)
                return

            finish_reason = RESULTS[task_id].get('finish_reason')
            if finish_reason == 'content-filter':
                yield format_openai_chunk("\n\nå“åº”è¢«ç»ˆæ­¢ï¼Œå¯èƒ½æ˜¯ä¸Šä¸‹æ–‡è¶…é™æˆ–è€…æ¨¡å‹å†…éƒ¨å®¡æŸ¥çš„åŸå› ", model, request_id)
            
            yield format_openai_finish_chunk(model, request_id, reason=finish_reason or 'stop')
        return Response(stream_response(), mimetype='text/event-stream')
    else:
        full_response_content = "".join(list(_openai_response_generator(task_id)))
        if RESULTS[task_id].get('error'):
            return jsonify({"error": {"message": f"[LMArena è‡ªåŠ¨åŒ–å·¥å…·é”™è¯¯]: {RESULTS[task_id]['error']}", "type": "automator_error"}}), 500
        
        finish_reason = RESULTS[task_id].get('finish_reason', 'stop')
        if finish_reason == 'content-filter':
            full_response_content += "\n\nå“åº”è¢«ç»ˆæ­¢ï¼Œå¯èƒ½æ˜¯ä¸Šä¸‹æ–‡è¶…é™æˆ–è€…æ¨¡å‹å†…éƒ¨å®¡æŸ¥çš„åŸå› "
            
        return jsonify(format_openai_non_stream_response(full_response_content, model, request_id, reason=finish_reason))

def create_hanging_job_package():
    """
    åˆ›å»ºä¸€ä¸ªå®Œå…¨æ¨¡æ‹Ÿ OpenAI è¯·æ±‚çš„æŒ‚æœºä»»åŠ¡åŒ…ã€‚
    """
    # æ¨¡æ‹Ÿä¸€ä¸ªå¤–éƒ¨åº”ç”¨å‘æ¥çš„è¯·æ±‚ä½“
    request_data = {
        "model": "claude-3-5-sonnet-20241022", # æˆ–è€…ä»»ä½•ä¸€ä¸ªæœ‰æ•ˆçš„é»˜è®¤æ¨¡å‹
        "messages": [{"role": "user", "content": "ä½ å¥½"}]
    }

    # ä½¿ç”¨ä¸ /v1/chat/completions ç«¯ç‚¹å®Œå…¨ç›¸åŒçš„é€»è¾‘æ¥åˆ›å»ºä»»åŠ¡
    messages_job = convert_openai_to_lmarena_templates(request_data)
    task_id = f"hanging-{uuid.uuid4()}"
    messages_job['task_id'] = task_id
    
    prompt_job = {
        "task_id": task_id,
        "prompt": f"[é˜²äººæœºæ£€æµ‹æŒ‚æœºä»»åŠ¡]"}

    job_package = {
        "task_id": task_id,
        "messages_job": messages_job,
        "prompt_job": prompt_job,
        "is_hanging_job": True  # æ ‡è®°ä¸ºæŒ‚æœºä»»åŠ¡
    }

    # æ³¨å†Œä»»åŠ¡ä»¥è·Ÿè¸ªå…¶ç»“æœ
    RESULTS[task_id] = {"status": "pending", "stream_queue": Queue(), "error": None}
    
    return job_package

def cleanup_and_dispatch_thread():
    """
    ä¸€ä¸ªåå°çº¿ç¨‹ï¼Œè´Ÿè´£æ¸…ç†åƒµå°¸è¿æ¥ã€è°ƒåº¦å¾…å¤„ç†ä»»åŠ¡ä»¥åŠç®¡ç†é˜²äººæœºæ£€æµ‹æŒ‚æœºæ± ã€‚
    """
    global HANGING_TAB_ID, NEXT_HANGING_JOB_TIME

    while True:
        time.sleep(2) # Run every 2 seconds for high responsiveness
        
        # è¯»å–é…ç½®ï¼Œç¡®ä¿æ˜¯æœ€æ–°çš„
        enable_hanging = CONFIG.get("enable_anti_bot_hanging", False)
        hanging_interval = CONFIG.get("hanging_interval_seconds", 120)

        with SESSION_LOCK:
            # --- 1. Active Ping & Cleanup Phase ---
            zombie_tabs = []
            active_sessions = list(TAB_SESSIONS.items())
            
            for tab_id, session in active_sessions:
                try:
                    if session['sse_queue']:
                        session['sse_queue'].put_nowait(": ping\n\n")
                except (AttributeError, Exception):
                    zombie_tabs.append(tab_id)

            for tab_id in zombie_tabs:
                logger.warning(f"è°ƒåº¦å™¨ï¼šé€šè¿‡Pingæ£€æµ‹åˆ°åƒµå°¸ä¼šè¯: {tab_id[:8]}ï¼Œæ­£åœ¨æ¸…ç†ã€‚")
                session = TAB_SESSIONS.pop(tab_id, None)
                
                if tab_id == HANGING_TAB_ID:
                    logger.info("è°ƒåº¦å™¨ï¼šæŒ‚æœºæ ‡ç­¾é¡µå·²æ–­å¼€ï¼Œå°†é‡æ–°é€‰æ‹©ã€‚")
                    HANGING_TAB_ID = None
                
                if session and session.get('status') == 'busy' and session.get('job'):
                    if not session['job'].get("is_hanging_job"):
                        requeued_job = session['job']
                        PENDING_JOBS.put(requeued_job)
                        logger.warning(f"è°ƒåº¦å™¨ï¼šæ¥è‡ªåƒµå°¸ä¼šè¯çš„ä»»åŠ¡ {requeued_job['task_id'][:8]} å·²è¢«é‡æ–°æ’é˜Ÿã€‚")
                    else:
                        logger.info(f"è°ƒåº¦å™¨ï¼šä¸¢å¼ƒæ¥è‡ªåƒµå°¸ä¼šè¯çš„æŒ‚æœºä»»åŠ¡ {session['task_id'][:8]}ã€‚")

            # --- 2. Anti-Bot Hanging Management Phase ---
            previous_hanging_id = HANGING_TAB_ID
            
            if enable_hanging and len(TAB_SESSIONS) >= 2:
                if HANGING_TAB_ID is None or HANGING_TAB_ID not in TAB_SESSIONS:
                    available_tabs = list(TAB_SESSIONS.keys())
                    if available_tabs:
                        HANGING_TAB_ID = random.choice(available_tabs)
                        NEXT_HANGING_JOB_TIME = time.time()
                        logger.info(f"è°ƒåº¦å™¨ï¼šå·²é€‰æ‹©æ–°æ ‡ç­¾é¡µ {HANGING_TAB_ID[:8]} ä½œä¸ºé˜²äººæœºæ£€æµ‹æŒ‚æœºæ± ã€‚")
            else:
                if HANGING_TAB_ID is not None:
                     logger.info(f"è°ƒåº¦å™¨ï¼šå› æ¡ä»¶ä¸æ»¡è¶³ï¼ˆå¯ç”¨: {enable_hanging}, æ ‡ç­¾é¡µæ•°: {len(TAB_SESSIONS)}ï¼‰ï¼Œå–æ¶ˆæŒ‚æœºæ¨¡å¼ã€‚")
                HANGING_TAB_ID = None

            # --- çŠ¶æ€å˜æ›´é€šçŸ¥ ---
            if previous_hanging_id != HANGING_TAB_ID:
                # é€šçŸ¥æ—§çš„æŒ‚æœºæ ‡ç­¾é¡µå–æ¶ˆçŠ¶æ€
                if previous_hanging_id and previous_hanging_id in TAB_SESSIONS:
                    try:
                        TAB_SESSIONS[previous_hanging_id]['sse_queue'].put(f"event: set_hanging_status\ndata: {json.dumps({'is_hanging': False})}\n\n")
                        logger.info(f"é€šçŸ¥æ ‡ç­¾é¡µ {previous_hanging_id[:8]} å·²å–æ¶ˆæŒ‚æœºçŠ¶æ€ã€‚")
                    except Exception: pass
                # é€šçŸ¥æ–°çš„æŒ‚æœºæ ‡ç­¾é¡µè®¾ç½®çŠ¶æ€
                if HANGING_TAB_ID and HANGING_TAB_ID in TAB_SESSIONS:
                    try:
                        TAB_SESSIONS[HANGING_TAB_ID]['sse_queue'].put(f"event: set_hanging_status\ndata: {json.dumps({'is_hanging': True})}\n\n")
                        logger.info(f"é€šçŸ¥æ ‡ç­¾é¡µ {HANGING_TAB_ID[:8]} å·²è®¾ä¸ºæŒ‚æœºçŠ¶æ€ã€‚")
                    except Exception: pass


            # --- 3. Hanging Job Creation Phase ---
            if enable_hanging and HANGING_TAB_ID:
                current_time = time.time()
                has_pending_hanging_job = any(job.get('is_hanging_job') for job in list(PENDING_JOBS.queue))
                
                if current_time >= NEXT_HANGING_JOB_TIME and not has_pending_hanging_job:
                    logger.info(f"è°ƒåº¦å™¨ï¼šåˆ›å»ºæ–°çš„æŒ‚æœºä»»åŠ¡å¹¶æ”¾å…¥é˜Ÿåˆ—ã€‚")
                    hanging_job_package = create_hanging_job_package()
                    PENDING_JOBS.put(hanging_job_package)
                    NEXT_HANGING_JOB_TIME = current_time + hanging_interval

            # --- 4. Dispatch Phase ---
            if not PENDING_JOBS.empty():
                job_package = PENDING_JOBS.queue[0]
                is_hanging = job_package.get("is_hanging_job", False)
                
                target_session_id = None
                
                if is_hanging:
                    if HANGING_TAB_ID and HANGING_TAB_ID in TAB_SESSIONS and TAB_SESSIONS[HANGING_TAB_ID].get('status') == 'idle':
                        target_session_id = HANGING_TAB_ID
                else:
                    idle_non_hanging_sessions = {
                        tid: s for tid, s in TAB_SESSIONS.items()
                        if s.get('status') == 'idle' and tid != HANGING_TAB_ID
                    }
                    if idle_non_hanging_sessions:
                        target_session_id = list(idle_non_hanging_sessions.keys())[0]
                    elif HANGING_TAB_ID and HANGING_TAB_ID in TAB_SESSIONS and TAB_SESSIONS[HANGING_TAB_ID].get('status') == 'idle':
                        target_session_id = HANGING_TAB_ID

                if target_session_id:
                    job_to_dispatch = PENDING_JOBS.get()
                    session = TAB_SESSIONS[target_session_id]
                    dispatch_job(target_session_id, session, job_to_dispatch)
                    if not is_hanging and target_session_id == HANGING_TAB_ID:
                        NEXT_HANGING_JOB_TIME = time.time() + hanging_interval
                        logger.info(f"æŒ‚æœºæ ‡ç­¾é¡µè¢«ç”¨äºæ‰§è¡Œæ™®é€šä»»åŠ¡ï¼Œä¸‹ä¸€æ¬¡æŒ‚æœºä»»åŠ¡æ¨è¿Ÿã€‚")

def dispatch_job(tab_id, session, job_package):
    """è¾…åŠ©å‡½æ•°ï¼Œç”¨äºå°†ä»»åŠ¡å‘é€åˆ°æŒ‡å®šçš„æ ‡ç­¾é¡µä¼šè¯ã€‚"""
    global HANGING_TAB_ID
    session['status'] = 'busy'
    session['job'] = job_package
    session['task_id'] = job_package['task_id']
    session['last_seen'] = time.time()

    prompt_job_data = job_package.get('prompt_job')
    if prompt_job_data:
        prompt_job_data['type'] = 'prompt'
        try:
            if session['sse_queue']:
                session['sse_queue'].put(f"event: new_job\ndata: {json.dumps(prompt_job_data)}\n\n")
                logger.info(f"è°ƒåº¦å™¨ï¼šå°†ä»»åŠ¡ {job_package['task_id'][:8]} åˆ†é…ç»™äº†æ ‡ç­¾é¡µ {tab_id[:8]}")
            else:
                raise Exception("SSE Queue is None")
        except Exception as e:
            logger.error(f"è°ƒåº¦å™¨ï¼šåœ¨åˆ†é…ä»»åŠ¡ç»™ {tab_id[:8]} æ—¶è¿æ¥å¤±æ•ˆ: {e}")
            # å¦‚æœæ˜¯æ™®é€šä»»åŠ¡ï¼Œé‡æ–°æ’é˜Ÿ
            if not job_package.get("is_hanging_job"):
                PENDING_JOBS.put(job_package)
            TAB_SESSIONS.pop(tab_id, None)
            if tab_id == HANGING_TAB_ID:
                HANGING_TAB_ID = None

if __name__ == '__main__':
    _load_config()
    if CONFIG.get("enable_comprehensive_logging"):
        log_dir = "Debug"
        os.makedirs(log_dir, exist_ok=True)
        log_filename = os.path.join(log_dir, f"debug_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log")
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', handlers=[logging.FileHandler(log_filename, encoding='utf-8'), logging.StreamHandler()])
        logger.info(f"èšåˆæ—¥å¿—å·²å¯ç”¨ã€‚æ—¥å¿—æ–‡ä»¶ä¿å­˜è‡³: {os.path.abspath(log_filename)}")
    else:
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', handlers=[logging.StreamHandler()])
    
    load_model_map()
    
    check_for_updates()

    # å¯åŠ¨åå°è°ƒåº¦çº¿ç¨‹
    dispatcher_thread = threading.Thread(target=cleanup_and_dispatch_thread, daemon=True)
    dispatcher_thread.start()
    logger.info("åå°ä»»åŠ¡è°ƒåº¦å™¨å·²å¯åŠ¨ã€‚")

    logger.info("="*60)
    logger.info("  ğŸš€ LMArena è‡ªåŠ¨åŒ–å·¥å…· - v12.2 (ä¸­æ–‡æœ¬åœ°åŒ–)")
    logger.info(f"  - ç›‘å¬åœ°å€: http://127.0.0.1:5102")
    
    config_keys_in_chinese = {
        "enable_auto_update": "è‡ªåŠ¨æ›´æ–°",
        "bypass_enabled": "Bypass æ¨¡å¼",
        "tavern_mode_enabled": "é…’é¦†æ¨¡å¼",
        "log_server_requests": "æœåŠ¡å™¨è¯·æ±‚æ—¥å¿—",
        "log_tampermonkey_debug": "æ²¹çŒ´è„šæœ¬è°ƒè¯•æ—¥å¿—",
        "enable_comprehensive_logging": "èšåˆæ—¥å¿—",
        "enable_anti_bot_hanging": "é˜²äººæœºæ£€æµ‹æŒ‚æœº",
        "api_key": "API Key ä¿æŠ¤"
    }
    
    logger.info("\n  å½“å‰é…ç½®:")
    for key, name in config_keys_in_chinese.items():
        status = 'âœ… å·²å¯ç”¨' if CONFIG.get(key) else 'âŒ å·²ç¦ç”¨'
        logger.info(f"  - {name}: {status}")
        
    logger.info("\n  è¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ä¸€ä¸ª LMArena çš„ Direct Chat é¡µé¢ä»¥æ¿€æ´»æ²¹çŒ´è„šæœ¬ã€‚")
    logger.info("="*60)
    
    app.run(host='0.0.0.0', port=5102, threaded=True)