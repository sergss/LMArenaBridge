# local_openai_history_server.py
# v7.0 - OpenAI History Injection Ready

from flask import Flask, request, jsonify, Response
from flask_cors import CORS # å¯¼å…¥ CORS
from queue import Queue, Empty
import logging
import uuid
import threading
import time
import json
import re
from datetime import datetime, timezone

# --- é…ç½® ---
log = logging.getLogger('werkzeug')
log.setLevel(logging.ERROR)
app = Flask(__name__)
CORS(app) # ä¸ºæ•´ä¸ªåº”ç”¨å¯ç”¨ CORS

# --- æ•°æ®å­˜å‚¨ (ä¸ v6.0 ä¿æŒä¸€è‡´) ---
INJECTION_JOBS = Queue()
PROMPT_JOBS = Queue()
TOOL_RESULT_JOBS = Queue()
MODEL_FETCH_JOBS = Queue()
RESULTS = {}
REPORTED_MODELS_CACHE = {
    "data": None,
    "timestamp": 0,
    "event": threading.Event()
}

# --- ã€æ–°ã€‘æ³¨å…¥å®Œæˆä¿¡å· ---
INJECTION_EVENTS = {}


# --- ã€æ–°ã€‘æ¨¡å‹è‡ªåŠ¨æ›´æ–°é€»è¾‘ ---
def extract_models_from_html(html_content: str) -> list:
    """ä» LMArena é¡µé¢çš„ HTML å†…å®¹ä¸­æå–æ¨¡å‹åˆ—è¡¨ã€‚"""
    # æ­£åˆ™è¡¨è¾¾å¼å¯»æ‰¾åŒ…å«æ¨¡å‹åˆ—è¡¨çš„ 'initialState' JSON å¯¹è±¡
    match = re.search(r'"initialState":(\[.*?\]),', html_content)
    if not match:
        # å°è¯•å¤‡ç”¨æ¨¡å¼ï¼Œå¤„ç†è½¬ä¹‰åçš„å¼•å·
        match = re.search(r'initialState\\":(\[.*?\]),', html_content)
    
    if not match:
        print("â„¹ï¸ [Model Updater] åœ¨HTMLå†…å®¹ä¸­æœªæ‰¾åˆ° 'initialState' æ¨¡å‹åˆ—è¡¨ã€‚")
        return []

    models_json_str = match.group(1)
    
    # æ¸…ç†å¯èƒ½å­˜åœ¨çš„è½¬ä¹‰å­—ç¬¦
    if '\\"' in models_json_str:
        models_json_str = models_json_str.replace('\\"', '"')

    try:
        models_list = json.loads(models_json_str)
        extracted_models = []
        for model in models_list:
            if 'publicName' in model and 'id' in model:
                extracted_models.append({
                    'name': model['publicName'],
                    'id': model['id']
                })
        print(f"âœ… [Model Updater] ä»é¡µé¢æˆåŠŸæå– {len(extracted_models)} ä¸ªæ¨¡å‹ã€‚")
        return extracted_models
    except json.JSONDecodeError as e:
        print(f"âŒ [Model Updater] è§£ææ¨¡å‹ JSON å¤±è´¥: {e}")
        print(f"   > é—®é¢˜ç‰‡æ®µ: {models_json_str[:250]}...")
        return []

def update_models_json_file(new_models: list):
    """ä½¿ç”¨æå–çš„æ–°æ¨¡å‹æ›´æ–° models.json æ–‡ä»¶ã€‚"""
    if not new_models:
        return

    try:
        with open('models.json', 'r+', encoding='utf-8') as f:
            try:
                existing_models = json.load(f)
            except json.JSONDecodeError:
                print("âš ï¸ [Model Updater] 'models.json' æ–‡ä»¶å·²æŸåæˆ–ä¸ºç©ºã€‚å°†åˆ›å»ºæ–°å†…å®¹ã€‚")
                existing_models = {}

            added_count = 0
            newly_added_names = []
            
            for model in new_models:
                model_name = model['name']
                model_id = model['id']
                if model_name not in existing_models:
                    existing_models[model_name] = model_id
                    added_count += 1
                    newly_added_names.append(model_name)

            if added_count > 0:
                print(f"âœ¨ [Model Updater] å‘ç° {added_count} ä¸ªæ–°æ¨¡å‹ï¼æ­£åœ¨æ›´æ–° 'models.json'...")
                for name in newly_added_names:
                    print(f"  -> æ–°å¢: {name}")
                
                f.seek(0)
                json.dump(existing_models, f, indent=4)
                f.truncate()
                print("âœ… [Model Updater] 'models.json' æ–‡ä»¶æ›´æ–°æˆåŠŸã€‚")
            else:
                print("âœ… [Model Updater] æ£€æŸ¥å®Œæ¯•ï¼Œæ‰€æœ‰æ¨¡å‹å‡å·²å­˜åœ¨äº 'models.json'ã€‚æ— éœ€æ›´æ–°ã€‚")

    except FileNotFoundError:
        print("âš ï¸ [Model Updater] 'models.json' æ–‡ä»¶æœªæ‰¾åˆ°ã€‚æ­£åœ¨åˆ›å»ºæ–°æ–‡ä»¶...")
        with open('models.json', 'w', encoding='utf-8') as f:
            models_to_write = {model['name']: model['id'] for model in new_models}
            json.dump(models_to_write, f, indent=4)
            print(f"âœ… [Model Updater] æˆåŠŸåˆ›å»º 'models.json' å¹¶æ·»åŠ äº† {len(models_to_write)} ä¸ªæ¨¡å‹ã€‚")


# --- å…¨å±€ä¼šè¯ç¼“å­˜ ---
LAST_CONVERSATION_STATE = None


# --- API ç«¯ç‚¹ ---

@app.route('/reset_state', methods=['POST'])
def reset_state():
    """æ‰‹åŠ¨é‡ç½®ä¼šè¯ç¼“å­˜"""
    global LAST_CONVERSATION_STATE
    LAST_CONVERSATION_STATE = None
    print("ğŸ”„ [Cache] ä¼šè¯ç¼“å­˜å·²è¢«æ‰‹åŠ¨é‡ç½®ã€‚")
    return jsonify({"status": "success", "message": "Conversation cache has been reset."})


@app.route('/')
def index():
    return "LMArena è‡ªåŠ¨åŒ–ä»£ç†æœåŠ¡å™¨ v8.0 (OpenAI History Injection Ready) æ­£åœ¨è¿è¡Œã€‚"

# --- æ¨¡å‹æ˜ å°„è¡¨ ---
def load_model_map():
    """ä» models.json åŠ è½½æ¨¡å‹æ˜ å°„"""
    try:
        with open('models.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print("âŒ é”™è¯¯: 'models.json' æ–‡ä»¶æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿è¯¥æ–‡ä»¶å­˜åœ¨ã€‚")
        return {}
    except json.JSONDecodeError:
        print("âŒ é”™è¯¯: 'models.json' æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ã€‚")
        return {}

MODEL_NAME_TO_ID_MAP = load_model_map()
DEFAULT_MODEL_ID = "f44e280a-7914-43ca-a25d-ecfcc5d48d09" # é»˜è®¤ Claude 3.5 Sonnet

# --- æ ¼å¼è½¬æ¢é€»è¾‘ (v2) ---
def convert_openai_to_lmarena(openai_data):
    """å°† OpenAI æ ¼å¼çš„å¯¹è¯å†å²è½¬æ¢ä¸º LMArena å†…éƒ¨æ ¼å¼ï¼Œå¹¶æ³¨å…¥æ­£ç¡®çš„æ¨¡å‹ ID"""
    session_id = f"c{str(uuid.uuid4())[1:]}"
    user_id = f"u{str(uuid.uuid4())[1:]}"
    evaluation_id = f"e{str(uuid.uuid4())[1:]}"
    
    # æ ¹æ®æ¨¡å‹åç§°æŸ¥æ‰¾æ¨¡å‹ ID
    model_name = openai_data.get("model", "claude-3-5-sonnet-20241022")
    target_model_id = MODEL_NAME_TO_ID_MAP.get(model_name, DEFAULT_MODEL_ID)
    print(f"ğŸ¤– æ¨¡å‹æ˜ å°„: '{model_name}' -> '{target_model_id}'")

    lmarena_messages = []
    parent_msg_id = None

    for i, oai_msg in enumerate(openai_data["messages"]):
        msg_id = str(uuid.uuid4())
        created_at = datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z')
        
        lm_msg = {
            "id": msg_id,
            "evaluationSessionId": session_id,
            "evaluationId": evaluation_id,
            "parentMessageIds": [parent_msg_id] if parent_msg_id else [],
            "content": oai_msg.get("content", ""),
            "modelId": None if oai_msg["role"] in ("user", "system") else target_model_id, # ç”¨æˆ·å’Œç³»ç»Ÿæ¶ˆæ¯ä¸æŒ‡å®šæ¨¡å‹ID
            "status": "success",
            "failureReason": None,
            "metadata": None,
            "createdAt": created_at,
            "updatedAt": created_at,
            "role": oai_msg["role"],
            "experimental_attachments": [],
            "participantPosition": "a"
        }
        lmarena_messages.append(lm_msg)
        parent_msg_id = msg_id

    title = "New Conversation"
    if openai_data["messages"]:
        title = openai_data["messages"][0].get("content", "New Conversation")[:50]

    history_data = {
        "id": session_id,
        "userId": user_id,
        "title": title,
        "mode": "direct",
        "visibility": "public",
        "lastMessageIds": [parent_msg_id] if parent_msg_id else [],
        "createdAt": datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z'),
        "updatedAt": datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z'),
        "messages": lmarena_messages,
        "pairwiseFeedbacks": [],
        "pointwiseFeedbacks": [],
        "maskedEvaluations": [
            {
                "id": evaluation_id,
                "modality": "chat",
                "arenaId": "4c249f58-2f34-4859-bbdb-4233a8313340"
            }
        ],
        # ã€ã€ã€æ ¸å¿ƒæ–°å¢ã€‘ã€‘ã€‘å°†ç›®æ ‡æ¨¡å‹ ID ä¼ é€’ç»™æ²¹çŒ´è„šæœ¬
        "targetModelId": target_model_id
    }
    return history_data

# --- ã€ã€ã€æ–°ã€‘ã€‘ã€‘OpenAI æ ¼å¼å†å²æ³¨å…¥ API (å·²å‡çº§) ---
@app.route('/inject_openai_history', methods=['POST'])
def inject_openai_history():
    """æ¥æ”¶ OpenAI æ ¼å¼çš„å†å²ï¼Œè½¬æ¢åæ”¾å…¥æ³¨å…¥é˜Ÿåˆ—"""
    openai_job_data = request.json
    if not openai_job_data or 'messages' not in openai_job_data:
        return jsonify({"status": "error", "message": "è¯·æ±‚ä½“éœ€è¦åŒ…å« 'messages' å­—æ®µã€‚"}), 400
    
    print("ğŸ”„ æ¥æ”¶åˆ° OpenAI æ ¼å¼æ³¨å…¥ä»»åŠ¡ï¼Œå¼€å§‹è½¬æ¢ä¸º LMarena æ ¼å¼...")
    lmarena_job_data = convert_openai_to_lmarena(openai_job_data)
    
    INJECTION_JOBS.put(lmarena_job_data)
    print(f"âœ… è½¬æ¢å®Œæˆï¼å·²å°†ã€LMarena æ ¼å¼ä»»åŠ¡ã€‘æ”¾å…¥æ³¨å…¥é˜Ÿåˆ—ã€‚é˜Ÿåˆ—ç°æœ‰ä»»åŠ¡: {INJECTION_JOBS.qsize()}ã€‚")
    return jsonify({"status": "success", "message": "OpenAI history converted and submitted"}), 200


# --- åŸæœ‰æ³¨å…¥ API (ä¿æŒåŠŸèƒ½ï¼Œç”¨äºå‘åå…¼å®¹æˆ–ç‰¹å®šåœºæ™¯) ---
@app.route('/submit_injection_job', methods=['POST'])
def submit_injection_job():
    job_data = request.json
    INJECTION_JOBS.put(job_data)
    print(f"âœ… å·²æ¥æ”¶åˆ°æ–°çš„ã€æ ‡å‡†æ³¨å…¥ä»»åŠ¡ã€‘ã€‚æ³¨å…¥é˜Ÿåˆ—ç°æœ‰ä»»åŠ¡: {INJECTION_JOBS.qsize()}ã€‚")
    return jsonify({"status": "success", "message": "Injection job submitted"}), 200

@app.route('/get_injection_job', methods=['GET'])
def get_injection_job():
    try:
        job = INJECTION_JOBS.get_nowait()
        print(f"ğŸš€ Automator è„šæœ¬å·²å–èµ°æ³¨å…¥ä»»åŠ¡ã€‚é˜Ÿåˆ—å‰©ä½™: {INJECTION_JOBS.qsize()}ã€‚")
        return jsonify({"status": "success", "job": job}), 200
    except Empty:
        return jsonify({"status": "empty"}), 200

@app.route('/signal_injection_complete', methods=['POST'])
def signal_injection_complete():
    """æ¥æ”¶æ²¹çŒ´è„šæœ¬çš„æ³¨å…¥å®Œæˆä¿¡å·ï¼Œå¹¶å¯é€‰æ‹©æ€§åœ°å¤„ç†é¡µé¢å†…å®¹ä»¥æ›´æ–°æ¨¡å‹ã€‚"""
    data = request.json
    injection_id = data.get('injection_id')
    html_content = data.get('page_html')  # æ¥æ”¶å¯é€‰çš„ HTML å†…å®¹

    # å¦‚æœæ¥æ”¶åˆ° HTML å†…å®¹ï¼Œåˆ™è§¦å‘æ¨¡å‹æ›´æ–°æµç¨‹
    if html_content:
        print("â„¹ï¸ [Model Updater] æ¥æ”¶åˆ°é¡µé¢ HTMLï¼Œå¼€å§‹è‡ªåŠ¨æ›´æ–°æ¨¡å‹åº“...")
        extracted_models = extract_models_from_html(html_content)
        update_models_json_file(extracted_models)

    # å…¼å®¹ä»…æ›´æ–°æ¨¡å‹è€Œä¸å¤„ç†æ³¨å…¥ä¿¡å·çš„æƒ…å†µ
    if not injection_id:
        if html_content:
            return jsonify({"status": "success", "message": "Models updated, no injection ID provided."}), 200
        return jsonify({"status": "error", "message": "éœ€è¦ 'injection_id' å­—æ®µã€‚"}), 400

    if injection_id in INJECTION_EVENTS:
        INJECTION_EVENTS[injection_id].set()  # è§¦å‘äº‹ä»¶
        del INJECTION_EVENTS[injection_id]  # æ¸…ç†
        print(f"âœ”ï¸ æ¥æ”¶åˆ°æ³¨å…¥ä»»åŠ¡ {injection_id} çš„å®Œæˆä¿¡å·ã€‚")
        return jsonify({"status": "success"}), 200
    else:
        print(f"âš ï¸ æ¥æ”¶åˆ°æœªçŸ¥æˆ–å·²è¿‡æœŸçš„æ³¨å…¥ä»»åŠ¡ä¿¡å·: {injection_id}")
        return jsonify({"status": "error", "message": "æœªçŸ¥çš„æ³¨å…¥ IDã€‚"}), 404


# --- äº¤äº’å¼å¯¹è¯ API ---
@app.route('/submit_prompt', methods=['POST'])
def submit_prompt():
    data = request.json
    if not data or 'prompt' not in data:
        return jsonify({"status": "error", "message": "éœ€è¦ 'prompt' å­—æ®µã€‚"}), 400
    
    task_id = str(uuid.uuid4())
    # ç¡®ä¿å°† task_id åŒ…å«åœ¨ä»»åŠ¡æ•°æ®ä¸­
    job = {"task_id": task_id, "prompt": data['prompt']}
    PROMPT_JOBS.put(job)
    
    # ä¸ºè¿™ä¸ªæ–°ä»»åŠ¡åˆå§‹åŒ–ç»“æœå­˜å‚¨ï¼Œè¿™æ˜¯æ¥æ”¶æµå¼å“åº”æ‰€å¿…éœ€çš„
    RESULTS[task_id] = {
        "status": "pending",
        "stream_queue": Queue(),
        "full_response": None
    }
    
    print(f"âœ… å·²æ¥æ”¶åˆ°æ–°çš„ã€å¯¹è¯ä»»åŠ¡ã€‘(ID: {task_id[:8]})ã€‚å¯¹è¯é˜Ÿåˆ—ç°æœ‰ä»»åŠ¡: {PROMPT_JOBS.qsize()}ã€‚")
    return jsonify({"status": "success", "task_id": task_id}), 200

@app.route('/get_prompt_job', methods=['GET'])
def get_prompt_job():
    try:
        job = PROMPT_JOBS.get_nowait()
        print(f"ğŸš€ Automator è„šæœ¬å·²å–èµ°å¯¹è¯ä»»åŠ¡ (ID: {job['task_id'][:8]})ã€‚é˜Ÿåˆ—å‰©ä½™: {PROMPT_JOBS.qsize()}ã€‚")
        return jsonify({"status": "success", "job": job}), 200
    except Empty:
        return jsonify({"status": "empty"}), 200

# --- æµå¼æ•°æ® API (æ— å˜åŒ–) ---
@app.route('/stream_chunk', methods=['POST'])
def stream_chunk():
    data = request.json
    task_id = data.get('task_id')
    chunk = data.get('chunk')
    if task_id in RESULTS:
        RESULTS[task_id]['stream_queue'].put(chunk)
        return jsonify({"status": "success"}), 200
    return jsonify({"status": "error", "message": "æ— æ•ˆçš„ä»»åŠ¡ ID"}), 404

@app.route('/get_chunk/<task_id>', methods=['GET'])
def get_chunk(task_id):
    if task_id in RESULTS:
        try:
            chunk = RESULTS[task_id]['stream_queue'].get_nowait()
            return jsonify({"status": "ok", "chunk": chunk}), 200
        except Empty:
            if RESULTS[task_id]['status'] in ['completed', 'failed']:
                return jsonify({"status": "done"}), 200
            else:
                return jsonify({"status": "empty"}), 200
    return jsonify({"status": "not_found"}), 404
    
@app.route('/report_result', methods=['POST'])
def report_result():
    data = request.json
    task_id = data.get('task_id')
    if task_id and task_id in RESULTS:
        RESULTS[task_id]['status'] = data.get('status', 'completed')
        RESULTS[task_id]['full_response'] = data.get('content', '')
        print(f"âœ”ï¸ ä»»åŠ¡ {task_id[:8]} å·²å®Œæˆã€‚çŠ¶æ€: {RESULTS[task_id]['status']}ã€‚")
        return jsonify({"status": "success"}), 200
    return jsonify({"status": "error", "message": "æ— æ•ˆçš„ä»»åŠ¡ IDã€‚"}), 404

# --- å·¥å…·å‡½æ•°ç»“æœ API (æ— å˜åŒ–) ---
@app.route('/submit_tool_result', methods=['POST'])
def submit_tool_result():
    data = request.json
    if not data or 'task_id' not in data or 'result' not in data:
        return jsonify({"status": "error", "message": "éœ€è¦ 'task_id' å’Œ 'result' å­—æ®µã€‚"}), 400
    
    task_id = data['task_id']
    job = {"task_id": task_id, "result": data['result']}
    TOOL_RESULT_JOBS.put(job)
    RESULTS[task_id] = {
        "status": "pending",
        "stream_queue": Queue(),
        "full_response": None
    }
    print(f"âœ… å·²æ¥æ”¶åˆ°æ–°çš„ã€å·¥å…·è¿”å›ä»»åŠ¡ã€‘(ID: {task_id[:8]})ã€‚å·¥å…·é˜Ÿåˆ—ç°æœ‰ä»»åŠ¡: {TOOL_RESULT_JOBS.qsize()}ã€‚")
    return jsonify({"status": "success"}), 200

@app.route('/get_tool_result_job', methods=['GET'])
def get_tool_result_job():
    try:
        job = TOOL_RESULT_JOBS.get_nowait()
        print(f"ğŸš€ Automator å·²å–èµ°å·¥å…·è¿”å›ä»»åŠ¡ (ID: {job['task_id'][:8]})ã€‚é˜Ÿåˆ—å‰©ä½™: {TOOL_RESULT_JOBS.qsize()}ã€‚")
        return jsonify({"status": "success", "job": job}), 200
    except Empty:
        return jsonify({"status": "empty"}), 200

# --- æ¨¡å‹è·å– API (æ— å˜åŒ–) ---
@app.route('/submit_model_fetch_job', methods=['POST'])
def submit_model_fetch_job():
    if not MODEL_FETCH_JOBS.empty():
        return jsonify({"status": "success", "message": "A fetch job is already pending."}), 200
    
    task_id = str(uuid.uuid4())
    job = {"task_id": task_id, "type": "FETCH_MODELS"}
    MODEL_FETCH_JOBS.put(job)
    REPORTED_MODELS_CACHE['event'].clear()
    REPORTED_MODELS_CACHE['data'] = None
    print(f"âœ… å·²æ¥æ”¶åˆ°æ–°çš„ã€æ¨¡å‹è·å–ä»»åŠ¡ã€‘(ID: {task_id[:8]})ã€‚")
    return jsonify({"status": "success", "task_id": task_id})

@app.route('/get_model_fetch_job', methods=['GET'])
def get_model_fetch_job():
    try:
        job = MODEL_FETCH_JOBS.queue[0]
        return jsonify({"status": "success", "job": job}), 200
    except IndexError:
        return jsonify({"status": "empty"}), 200

@app.route('/acknowledge_model_fetch_job', methods=['POST'])
def acknowledge_model_fetch_job():
    try:
        job = MODEL_FETCH_JOBS.get_nowait()
        print(f"ğŸš€ Model Fetcher å·²ç¡®è®¤å¹¶å–èµ°æ¨¡å‹è·å–ä»»åŠ¡ (ID: {job['task_id'][:8]})ã€‚")
        return jsonify({"status": "success"}), 200
    except Empty:
        return jsonify({"status": "error", "message": "No job to acknowledge."}), 400

@app.route('/report_models', methods=['POST'])
def report_models():
    data = request.json
    models_json = data.get('models_json')
    if models_json:
        REPORTED_MODELS_CACHE['data'] = models_json
        REPORTED_MODELS_CACHE['timestamp'] = uuid.uuid4().int
        REPORTED_MODELS_CACHE['event'].set()
        print(f"âœ”ï¸ æˆåŠŸæ¥æ”¶å¹¶ç¼“å­˜äº†æ–°çš„æ¨¡å‹åˆ—è¡¨æ•°æ®ã€‚")
        return jsonify({"status": "success"}), 200
    return jsonify({"status": "error", "message": "éœ€è¦ 'models_json' å­—æ®µã€‚"}), 400

@app.route('/get_reported_models', methods=['GET'])
def get_reported_models():
    wait_result = REPORTED_MODELS_CACHE['event'].wait(timeout=60)
    if not wait_result:
        return jsonify({"status": "error", "message": "ç­‰å¾…æ¨¡å‹æ•°æ®è¶…æ—¶ (60 ç§’)ã€‚"}), 408
    if REPORTED_MODELS_CACHE['data']:
        return jsonify({
            "status": "success",
            "data": REPORTED_MODELS_CACHE['data'],
            "timestamp": REPORTED_MODELS_CACHE['timestamp']
        }), 200
    else:
        return jsonify({"status": "error", "message": "æ•°æ®è·å–å¤±è´¥ï¼Œå³ä½¿äº‹ä»¶å·²è§¦å‘ã€‚"}), 500


# --- ã€ã€ã€æ–°ã€‘ã€‘ã€‘OpenAI å…¼å®¹ API ---

def format_openai_chunk(content: str, model: str, request_id: str):
    """æ ¼å¼åŒ– OpenAI æµå¼å“åº”çš„æ–‡æœ¬å—"""
    chunk_data = {
        "id": request_id,
        "object": "chat.completion.chunk",
        "created": int(time.time()),
        "model": model,
        "choices": [{"index": 0, "delta": {"content": content}, "finish_reason": None}]
    }
    return f"data: {json.dumps(chunk_data)}\n\n"

def format_openai_finish_chunk(model: str, request_id: str, finish_reason: str = "stop"):
    """æ ¼å¼åŒ– OpenAI æµå¼å“åº”çš„ç»“æŸå—"""
    chunk_data = {
        "id": request_id,
        "object": "chat.completion.chunk",
        "created": int(time.time()),
        "model": model,
        "choices": [{"index": 0, "delta": {}, "finish_reason": finish_reason}]
    }
    return f"data: {json.dumps(chunk_data)}\n\n"

def format_openai_non_stream_response(content: str, model: str, request_id: str, finish_reason: str = "stop"):
    """æ ¼å¼åŒ– OpenAI éæµå¼å“åº”"""
    response_data = {
        "id": request_id,
        "object": "chat.completion",
        "created": int(time.time()),
        "model": model,
        "choices": [{
            "index": 0,
            "message": {"role": "assistant", "content": content},
            "finish_reason": finish_reason
        }],
        "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
    }
    return response_data

def _normalize_message_content(message: dict) -> dict:
    """
    ç¡®ä¿æ¶ˆæ¯å†…å®¹æ˜¯å­—ç¬¦ä¸²ï¼Œå¤„ç† OpenAI å®¢æˆ·ç«¯å¯èƒ½å‘é€çš„ content åˆ—è¡¨ã€‚
    """
    content = message.get("content")
    # ä»…å½“ content æ˜¯åˆ—è¡¨æ—¶æ‰è¿›è¡Œå¤„ç†
    if isinstance(content, list):
        # å°†æ‰€æœ‰ text éƒ¨åˆ†è¿æ¥èµ·æ¥
        message["content"] = "\n\n".join(
            [p.get("text", "") for p in content if isinstance(p, dict) and p.get("type") == "text"]
        )
    return message

def _openai_response_generator(task_id: str):
    """
    ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œä»å†…éƒ¨é˜Ÿåˆ—ä¸­æ‹‰å–ç»“æœå—ï¼Œè§£æå¹¶æå–çº¯æ–‡æœ¬å†…å®¹ã€‚
    è¿™ä¸ªç”Ÿæˆå™¨æ˜¯æµå¼å’Œéæµå¼å“åº”çš„åŸºç¡€ã€‚
    """
    # æ­£åˆ™è¡¨è¾¾å¼ç”¨äºä»åŸå§‹æ•°æ®å—ä¸­æå– "a0:..." æ ¼å¼çš„æ–‡æœ¬å†…å®¹
    text_pattern = re.compile(r'a0:"((?:\\.|[^"\\])*)"')

    while True:
        try:
            # ä»å†…éƒ¨é˜Ÿåˆ—è·å–ä¸‹ä¸€ä¸ªåŸå§‹æ•°æ®å—
            raw_chunk = RESULTS[task_id]['stream_queue'].get(timeout=1)
            
            # ä»åŸå§‹æµä¸­æå– a0:"..." çš„å†…å®¹
            matches = text_pattern.findall(raw_chunk)
            for match in matches:
                # ä½¿ç”¨ json.loads æ¥æ­£ç¡®å¤„ç†å¯èƒ½çš„è½¬ä¹‰å­—ç¬¦ (e.g., \n, \")
                try:
                    text_content = json.loads(f'"{match}"')
                    if text_content: # ç¡®ä¿ä¸ yield ç©ºå­—ç¬¦ä¸²
                        yield text_content
                except json.JSONDecodeError:
                    # å¦‚æœè§£æå¤±è´¥ï¼Œè·³è¿‡è¿™ä¸ªåŒ¹é…é¡¹
                    continue

        except Empty:
            # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²ç”± Automator è„šæœ¬æ ‡è®°ä¸ºå®Œæˆ
            if RESULTS.get(task_id, {}).get('status') in ['completed', 'failed']:
                return # ç»“æŸç”Ÿæˆå™¨

def _update_conversation_state(request_base, new_messages: list):
    """
    é€šç”¨çŠ¶æ€æ›´æ–°å‡½æ•°ã€‚
    - request_base: ä¸åŒ…å«æ–°æ¶ˆæ¯çš„åŸºç¡€è¯·æ±‚ã€‚
    - new_messages: ä¸€ä¸ªåŒ…å« 'user' å’Œ 'assistant' æ¶ˆæ¯çš„åˆ—è¡¨ã€‚
    """
    global LAST_CONVERSATION_STATE
    new_state = request_base.copy()
    if "messages" not in new_state:
        new_state["messages"] = []
    
    # ã€ã€ã€æ–°ï¼šè¿‡æ»¤å ä½æ¶ˆæ¯ã€‘ã€‘ã€‘åœ¨æ›´æ–°ç¼“å­˜å‰ï¼Œè¿‡æ»¤æ‰æˆ‘ä»¬è‡ªå·±æ·»åŠ çš„å ä½æ¶ˆæ¯
    final_messages_to_add = [
        msg for msg in new_messages
        if not (msg.get("role") == "user" and msg.get("content", "").strip() == "")
    ]

    new_state["messages"].extend(final_messages_to_add)
    LAST_CONVERSATION_STATE = new_state
    print(f"âœ… [Cache] ä¼šè¯çŠ¶æ€å·²æ›´æ–°ï¼Œå½“å‰å…± {len(new_state['messages'])} æ¡æ¶ˆæ¯ã€‚")

@app.route('/v1/models', methods=['GET'])
def list_models():
    """å…¼å®¹ OpenAI çš„ /v1/models ç«¯ç‚¹ï¼Œè¿”å› models.json ä¸­çš„æ¨¡å‹åˆ—è¡¨ã€‚"""
    print("ğŸ”„ [API] æ¥æ”¶åˆ° /v1/models è¯·æ±‚...")
    model_map = load_model_map()
    if not model_map:
        return jsonify({"error": "æ— æ³•åŠ è½½ 'models.json'ã€‚"}), 500

    openai_models = []
    # The client uses the 'name' (e.g., 'claude-3-5-sonnet-20241022') as the model ID in requests.
    for model_name in model_map.keys():
        openai_models.append({
            "id": model_name,
            "object": "model",
            "created": int(time.time()),
            "owned_by": "local-history-server"
        })

    response_data = {
      "object": "list",
      "data": openai_models
    }
    
    return jsonify(response_data)


@app.route('/v1/chat/completions', methods=['POST'])
def chat_completions():
    """
    å…¼å®¹ OpenAI çš„ chat completions ç«¯ç‚¹ï¼ˆå¸¦ä¼šè¯ç¼“å­˜ï¼‰ã€‚
    """
    global LAST_CONVERSATION_STATE
    request_data = request.json
    if not request_data or "messages" not in request_data:
        return jsonify({"error": "è¯·æ±‚ä½“éœ€è¦åŒ…å« 'messages' å­—æ®µã€‚"}), 400

    try:
        # åœ¨è¿›è¡Œä»»ä½•å¤„ç†ä¹‹å‰ï¼Œå…ˆè§„èŒƒåŒ–æ¶ˆæ¯å†…å®¹
        messages = [_normalize_message_content(msg) for msg in request_data.get("messages", [])]
        # ã€ã€ã€æ ¸å¿ƒä¿®å¤ã€‘ã€‘ã€‘ç”¨è§„èŒƒåŒ–åçš„æ¶ˆæ¯åˆ—è¡¨æ›´æ–°åŸå§‹è¯·æ±‚æ•°æ®
        request_data["messages"] = messages
    except Exception as e:
        return jsonify({"error": f"å¤„ç†æ¶ˆæ¯å†…å®¹æ—¶å¤±è´¥: {e}"}), 400

    if not messages:
        return jsonify({"error": "'messages' åˆ—è¡¨ä¸èƒ½ä¸ºç©ºã€‚"}), 400

    model = request_data.get("model", "claude-3-5-sonnet-20241022")
    use_stream = request_data.get("stream", False)
    request_id = f"chatcmpl-{uuid.uuid4()}"

    # --- å¯¹è¯è¿ç»­æ€§æ£€æµ‹ ---
    is_continuation = False
    if LAST_CONVERSATION_STATE:
        cached_messages = LAST_CONVERSATION_STATE.get("messages", [])
        new_messages_base = messages[:-1]
        if json.dumps(cached_messages, sort_keys=True) == json.dumps(new_messages_base, sort_keys=True):
            is_continuation = True

    last_message = messages[-1]
    prompt_content = last_message.get("content", "")
    request_base_for_update = request_data.copy()
    request_base_for_update["messages"] = messages[:-1]

    # --- è·¯å¾„é€‰æ‹©ï¼šå¿«é€Ÿé€šé“ vs å®Œæ•´æ³¨å…¥ ---
    if is_continuation:
        print(f"âš¡ï¸ [Fast Path] æ£€æµ‹åˆ°è¿ç»­å¯¹è¯ (è¯·æ±‚ {request_id[:8]})ï¼Œè·³è¿‡å†å²æ³¨å…¥ã€‚")
    else:
        print(f"ğŸ”„ [Full Injection] æ£€æµ‹åˆ°æ–°å¯¹è¯æˆ–çŠ¶æ€ä¸ä¸€è‡´ (è¯·æ±‚ {request_id[:8]})ï¼Œæ‰§è¡Œå®Œæ•´å†å²æ³¨å…¥ã€‚")
        LAST_CONVERSATION_STATE = None # é‡ç½®çŠ¶æ€
        
        # å‡†å¤‡è¦æ³¨å…¥çš„å†å²è®°å½• (é™¤æœ€åä¸€æ¡æ¶ˆæ¯å¤–çš„æ‰€æœ‰å†…å®¹)
        history_messages = messages[:-1]
        
        # ã€ã€ã€æ ¸å¿ƒé‡æ„ï¼šä½¿ç”¨äº‹ä»¶ä¿¡å·æœºåˆ¶æ›¿ä»£ time.sleepã€‘ã€‘ã€‘
        injection_id = str(uuid.uuid4())
        
        # 1. å…ˆåˆ›å»ºå¹¶å­˜å‚¨äº‹ä»¶
        event = threading.Event()
        INJECTION_EVENTS[injection_id] = event
        print(f"  > å·²ä¸ºæ³¨å…¥ä»»åŠ¡ {injection_id} åˆ›å»ºç­‰å¾…ä¿¡å·ã€‚")

        # 2. å‡†å¤‡ä»»åŠ¡å¹¶æ”¾å…¥é˜Ÿåˆ—
        history_messages = messages[:-1]
        
        # ã€ã€ã€æ ¸å¿ƒä¿®å¤ï¼šå¤„ç†ç©ºå†å²å’Œç³»ç»Ÿæç¤ºè¯ã€‘ã€‘ã€‘
        # å¦‚æœå†å²è®°å½•ä¸ºç©ºï¼Œæˆ‘ä»¬éœ€è¦å†³å®šæ³¨å…¥ä»€ä¹ˆã€‚
        if not history_messages:
            # æŸ¥æ‰¾åŸå§‹è¯·æ±‚ä¸­æ˜¯å¦æœ‰ system prompt
            system_prompt = next((msg for msg in messages if msg['role'] == 'system'), None)
            if system_prompt:
                print("  > æ£€æµ‹åˆ°ç©ºå†å²è®°å½•ï¼Œä½†æœ‰ç³»ç»Ÿæç¤ºè¯ã€‚å°†æ³¨å…¥ç³»ç»Ÿæç¤ºè¯ã€‚")
                history_messages.append(system_prompt)
            else:
                print("  > æ£€æµ‹åˆ°ç©ºå†å²è®°å½•ï¼Œä¸”æ— ç³»ç»Ÿæç¤ºè¯ã€‚æ³¨å…¥ä¸€ä¸ªå¸¦ç©ºæ ¼çš„ç³»ç»Ÿæç¤ºè¯ä»¥åˆå§‹åŒ–ã€‚")
                history_messages.append({"role": "system", "content": " "})

        history_data = {"model": model, "messages": history_messages}
        lmarena_history_job = convert_openai_to_lmarena(history_data)
        lmarena_history_job["injection_id"] = injection_id
        INJECTION_JOBS.put(lmarena_history_job)
        print(f"  > å·²æäº¤æ³¨å…¥ä»»åŠ¡ {injection_id}ã€‚ç­‰å¾…æ²¹çŒ´è„šæœ¬çš„å®Œæˆä¿¡å·...")

        # 3. ç°åœ¨å¯ä»¥å®‰å…¨åœ°ç­‰å¾…äº‹ä»¶äº†
        completed_in_time = event.wait(timeout=60.0)
        if completed_in_time:
            print(f"  > æ³¨å…¥ä»»åŠ¡ {injection_id} å·²ç¡®è®¤å®Œæˆã€‚ç»§ç»­æ‰§è¡Œã€‚")
        else:
            print(f"  > è­¦å‘Šï¼šç­‰å¾…æ³¨å…¥ä»»åŠ¡ {injection_id} å®Œæˆè¶…æ—¶ï¼ˆ60ç§’ï¼‰ã€‚å¯èƒ½å‡ºç°é—®é¢˜ã€‚")
            # è¶…æ—¶åä¹Ÿæ¸…ç†æ‰äº‹ä»¶ï¼Œé¿å…å†…å­˜æ³„æ¼
            if injection_id in INJECTION_EVENTS:
                del INJECTION_EVENTS[injection_id]

    # --- ä»»åŠ¡æäº¤ä¸å“åº”ç”Ÿæˆ ---
    task_id = str(uuid.uuid4())
    prompt_job = {"task_id": task_id, "prompt": prompt_content}
    PROMPT_JOBS.put(prompt_job)
    RESULTS[task_id] = {"status": "pending", "stream_queue": Queue(), "full_response": None}
    print(f"âœ… å·²ä¸ºè¯·æ±‚ {request_id[:8]} åˆ›å»ºæ–°çš„å¯¹è¯ä»»åŠ¡ (ID: {task_id[:8]})ã€‚")

    if use_stream:
        def stream_response():
            print(f"ğŸŸ¢ å¼€å§‹ä¸ºè¯·æ±‚ {request_id[:8]} (ä»»åŠ¡ID: {task_id[:8]}) è¿›è¡Œæµå¼ä¼ è¾“...")
            
            full_ai_response_text = []
            # ç›´æ¥è¿­ä»£ç”Ÿæˆå™¨ï¼Œå®ç°çœŸæ­£çš„æµå¼ä¼ è¾“
            for chunk in _openai_response_generator(task_id):
                full_ai_response_text.append(chunk)
                yield format_openai_chunk(chunk, model, request_id)
            
            # æµç»“æŸåï¼Œç»„åˆå®Œæ•´å“åº”å¹¶æ›´æ–°ä¼šè¯çŠ¶æ€
            final_text = "".join(full_ai_response_text)
            assistant_message = {"role": "assistant", "content": final_text}
            _update_conversation_state(request_base_for_update, [last_message, assistant_message])
            
            # å‘é€ç»“æŸä¿¡å·
            yield format_openai_finish_chunk(model, request_id)
            yield "data: [DONE]\n\n"
            print(f"ğŸŸ¡ è¯·æ±‚ {request_id[:8]} (ä»»åŠ¡ID: {task_id[:8]}) æµå¼ä¼ è¾“ç»“æŸã€‚")

        return Response(stream_response(), mimetype='text/event-stream')
    else:
        # éæµå¼å“åº”
        print(f"ğŸŸ¢ å¼€å§‹ä¸ºè¯·æ±‚ {request_id[:8]} (ä»»åŠ¡ID: {task_id[:8]}) åœ¨åå°æ”¶é›†å“åº”...")
        
        full_response_content = "".join(list(_openai_response_generator(task_id)))
        
        # æ›´æ–°ä¼šè¯çŠ¶æ€
        assistant_message = {"role": "assistant", "content": full_response_content}
        _update_conversation_state(request_base_for_update, [last_message, assistant_message])

        final_json = format_openai_non_stream_response(full_response_content, model, request_id)
        print(f"ğŸŸ¡ è¯·æ±‚ {request_id[:8]} (ä»»åŠ¡ID: {task_id[:8]}) å“åº”æ”¶é›†å®Œæˆã€‚")
        return jsonify(final_json)


if __name__ == '__main__':
    print("======================================================================")
    print("  LMArena è‡ªåŠ¨åŒ–ä»£ç†æœåŠ¡å™¨ v8.0 (OpenAI API Ready)")
    print("  âœ¨ æ–°å¢: /v1/chat/completions (å…¼å®¹ OpenAI çš„å¯¹è¯æ¥å£)")
    print("  âœ¨ æ–°å¢: /inject_openai_history (ç”¨äº OpenAI æ ¼å¼å†å²æ³¨å…¥)")
    print("  âœ¨ æ–°å¢: /submit_prompt, /get_prompt_job (ç”¨äºå‘èµ·å¯¹è¯)")
    print("  âœ¨ æ–°å¢: /stream_chunk, /get_chunk, /report_result (ç”¨äºæµå¼ä¼ è¾“)")
    print("  - /submit_injection_job, /get_injection_job (æ ‡å‡†æ³¨å…¥)")
    print("  - /submit_tool_result, /get_tool_result_job (è¿”å›å·¥å…·ç»“æœ)")
    print("  - /submit_model_fetch_job, /get_model_fetch_job (è·å–æ¨¡å‹)")
    print("  å·²åœ¨ http://127.0.0.1:5102 å¯åŠ¨")
    print("======================================================================")
    app.run(host='0.0.0.0', port=5102, threaded=True)