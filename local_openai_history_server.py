# local_openai_history_server.py
# v12.2 - Chinese Localization

import logging
import os
from flask import Flask, request, jsonify, Response
from flask_cors import CORS
from queue import Queue, Empty
import uuid
import threading
import time
import json
import re
from datetime import datetime
import requests
from packaging.version import parse as parse_version
import zipfile
import io
import sys
import subprocess

# --- å…¨å±€é…ç½® ---
CONFIG = {}
logger = logging.getLogger(__name__)

# --- Flask åº”ç”¨è®¾ç½® ---
app = Flask(__name__)
CORS(app)
werkzeug_logger = logging.getLogger('werkzeug')
werkzeug_logger.disabled = True

# --- æ•°æ®å­˜å‚¨ ---
MESSAGES_JOBS = Queue()
PROMPT_JOBS = Queue()
RESULTS = {}

# --- æ¨¡å‹æ˜ å°„ ---
MODEL_NAME_TO_ID_MAP = {}
DEFAULT_MODEL_ID = "f44e280a-7914-43ca-a25d-ecfcc5d48d09"

def load_model_map():
    global MODEL_NAME_TO_ID_MAP
    try:
        with open('models.json', 'r', encoding='utf-8') as f:
            MODEL_NAME_TO_ID_MAP = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        MODEL_NAME_TO_ID_MAP = {}

# --- æ¨¡å‹æ›´æ–°æ£€æŸ¥é€»è¾‘ ---
def extract_models_from_html(html_content):
    """
    ä» HTML å†…å®¹ä¸­æå–æ¨¡å‹æ•°æ®ï¼Œé‡‡ç”¨æ›´å¥å£®çš„è§£ææ–¹æ³•ã€‚
    """
    script_contents = re.findall(r'<script>(.*?)</script>', html_content, re.DOTALL)
    
    for script_content in script_contents:
        if 'self.__next_f.push' in script_content and 'initialState' in script_content and 'publicName' in script_content:
            match = re.search(r'self\.__next_f\.push\(\[1,"(.*?)"\]\)', script_content, re.DOTALL)
            if not match:
                continue
            
            full_payload = match.group(1)
            
            # è½½è·å¯èƒ½åŒ…å«ç”± \n åˆ†éš”çš„å¤šä¸ªéƒ¨åˆ†ã€‚æˆ‘ä»¬åªå…³å¿ƒç¬¬ä¸€ä¸ªä¸»è¦çš„æ•°æ®å—ã€‚
            # æˆ‘ä»¬æŒ‰å­—é¢ä¸Šçš„ '\\n' åˆ†å‰²
            payload_string = full_payload.split('\\n')[0]
            
            json_start_index = payload_string.find(':')
            if json_start_index == -1:
                continue
            
            json_string_with_escapes = payload_string[json_start_index + 1:]
            # ç§»é™¤è½¬ä¹‰çš„å¼•å·
            json_string = json_string_with_escapes.replace('\\"', '"')
            
            try:
                data = json.loads(json_string)
                
                # é€’å½’æŸ¥æ‰¾ initialState é”®
                def find_initial_state(obj):
                    if isinstance(obj, dict):
                        for key, value in obj.items():
                            if key == 'initialState' and isinstance(value, list):
                                # ç¡®ä¿åˆ—è¡¨ä¸ä¸ºç©ºä¸”åŒ…å«æ¨¡å‹å­—å…¸
                                if value and isinstance(value[0], dict) and 'publicName' in value[0]:
                                    return value
                            # å³ä½¿æ‰¾åˆ°ä¸€ä¸ªï¼Œä¹Ÿè¦ç»§ç»­æœç´¢ï¼Œä»¥é˜²æœ‰åµŒå¥—
                            result = find_initial_state(value)
                            if result is not None:
                                return result
                    elif isinstance(obj, list):
                        for item in obj:
                            result = find_initial_state(item)
                            if result is not None:
                                return result
                    return None

                models = find_initial_state(data)
                if models:
                    logger.info(f"æˆåŠŸä»è„šæœ¬å—ä¸­æå–åˆ° {len(models)} ä¸ªæ¨¡å‹ã€‚")
                    return models
            except json.JSONDecodeError as e:
                logger.error(f"è§£ææå–çš„JSONå­—ç¬¦ä¸²æ—¶å‡ºé”™: {e}")
                continue

    logger.error("é”™è¯¯ï¼šåœ¨HTMLå“åº”ä¸­æ‰¾ä¸åˆ°åŒ…å«æœ‰æ•ˆæ¨¡å‹æ•°æ®çš„è„šæœ¬å—ã€‚")
    return None

def compare_and_update_models(new_models_list, models_path):
    """
    æ¯”è¾ƒæ–°æ—§æ¨¡å‹åˆ—è¡¨ï¼Œæ‰“å°å·®å¼‚ï¼Œå¹¶ç”¨æ–°åˆ—è¡¨æ›´æ–°æœ¬åœ° models.json æ–‡ä»¶ã€‚
    """
    try:
        with open(models_path, 'r', encoding='utf-8') as f:
            old_models = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        old_models = {}

    new_models_dict = {model['publicName']: model for model in new_models_list if 'publicName' in model}
    old_models_set = set(old_models.keys())
    new_models_set = set(new_models_dict.keys())

    added_models = new_models_set - old_models_set
    removed_models = old_models_set - new_models_set
    
    logger.info("--- æ¨¡å‹æ›´æ–°æ£€æŸ¥ ---")
    has_changes = False

    if added_models:
        has_changes = True
        logger.info("\n[+] æ–°å¢æ¨¡å‹:")
        for name in added_models:
            model = new_models_dict[name]
            logger.info(f"  - åç§°: {name}, ID: {model.get('id')}, ç»„ç»‡: {model.get('organization', 'N/A')}")

    if removed_models:
        has_changes = True
        logger.info("\n[-] åˆ é™¤æ¨¡å‹:")
        for name in removed_models:
            logger.info(f"  - åç§°: {name}, ID: {old_models.get(name)}")

    logger.info("\n[*] å…±åŒæ¨¡å‹æ£€æŸ¥:")
    changed_models = 0
    for name in new_models_set.intersection(old_models_set):
        new_id = new_models_dict[name].get('id')
        old_id = old_models.get(name)
        if new_id != old_id:
            has_changes = True
            changed_models += 1
            logger.info(f"  - ID å˜æ›´: '{name}' æ—§ID: {old_id} -> æ–°ID: {new_id}")
    
    if changed_models == 0:
        logger.info("  - å…±åŒæ¨¡å‹çš„IDæ— å˜åŒ–ã€‚")

    if not has_changes:
        logger.info("\nç»“è®º: æ¨¡å‹åˆ—è¡¨æ— ä»»ä½•å˜åŒ–ï¼Œæ— éœ€æ›´æ–°æ–‡ä»¶ã€‚")
        logger.info("--- æ£€æŸ¥å®Œæ¯• ---")
        return

    # æ›´æ–°æ–‡ä»¶
    logger.info("\nç»“è®º: æ£€æµ‹åˆ°æ¨¡å‹å˜æ›´ï¼Œæ­£åœ¨æ›´æ–° 'models.json'...")
    updated_model_map = {model['publicName']: model.get('id') for model in new_models_list if 'publicName' in model and 'id' in model}
    try:
        with open(models_path, 'w', encoding='utf-8') as f:
            json.dump(updated_model_map, f, indent=4, ensure_ascii=False)
        logger.info(f"'{models_path}' å·²æˆåŠŸæ›´æ–°ï¼ŒåŒ…å« {len(updated_model_map)} ä¸ªæ¨¡å‹ã€‚")
        # æ›´æ–°åé‡æ–°åŠ è½½åˆ°å†…å­˜ä¸­
        load_model_map()
    except IOError as e:
        logger.error(f"å†™å…¥ '{models_path}' æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    logger.info("--- æ£€æŸ¥ä¸æ›´æ–°å®Œæ¯• ---")


# --- æ›´æ–°æ£€æŸ¥ ---
GITHUB_REPO = "Lianues/LMArenaBridge"

def download_and_extract_update(version):
    """ä¸‹è½½å¹¶è§£å‹æœ€æ–°ç‰ˆæœ¬åˆ°ä¸´æ—¶æ–‡ä»¶å¤¹ã€‚"""
    update_dir = "update_temp"
    if not os.path.exists(update_dir):
        os.makedirs(update_dir)

    try:
        zip_url = f"https://github.com/{GITHUB_REPO}/archive/refs/heads/main.zip"
        logger.info(f"æ­£åœ¨ä» {zip_url} ä¸‹è½½æ–°ç‰ˆæœ¬...")
        response = requests.get(zip_url, timeout=60)
        response.raise_for_status()

        with zipfile.ZipFile(io.BytesIO(response.content)) as z:
            # è§£å‹æ‰€æœ‰æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
            z.extractall(update_dir)
        
        logger.info(f"æ–°ç‰ˆæœ¬å·²æˆåŠŸä¸‹è½½å¹¶è§£å‹åˆ° '{update_dir}' æ–‡ä»¶å¤¹ã€‚")
        return True
    except requests.RequestException as e:
        logger.error(f"ä¸‹è½½æ›´æ–°å¤±è´¥: {e}")
    except zipfile.BadZipFile:
        logger.error("ä¸‹è½½çš„æ–‡ä»¶ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„zipå‹ç¼©åŒ…ã€‚")
    except Exception as e:
        logger.error(f"è§£å‹æ›´æ–°æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    
    return False

def check_for_updates():
    """ä» GitHub æ£€æŸ¥æ–°ç‰ˆæœ¬ã€‚"""
    if not CONFIG.get("enable_auto_update", True):
        logger.info("è‡ªåŠ¨æ›´æ–°å·²ç¦ç”¨ï¼Œè·³è¿‡æ£€æŸ¥ã€‚")
        return

    current_version = CONFIG.get("version", "0.0.0")
    logger.info(f"å½“å‰ç‰ˆæœ¬: {current_version}ã€‚æ­£åœ¨ä» GitHub æ£€æŸ¥æ›´æ–°...")

    try:
        config_url = f"https://raw.githubusercontent.com/{GITHUB_REPO}/main/config.jsonc"
        response = requests.get(config_url, timeout=10)
        response.raise_for_status()

        # ç§»é™¤æ³¨é‡Šä»¥è§£æ JSON
        jsonc_content = response.text
        json_content = re.sub(r'//.*', '', jsonc_content)
        json_content = re.sub(r'/\*.*?\*/', '', json_content, flags=re.DOTALL)
        remote_config = json.loads(json_content)
        
        remote_version_str = remote_config.get("version")
        if not remote_version_str:
            logger.warning("è¿œç¨‹é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ç‰ˆæœ¬å·ï¼Œè·³è¿‡æ›´æ–°æ£€æŸ¥ã€‚")
            return

        if parse_version(remote_version_str) > parse_version(current_version):
            logger.info("="*60)
            logger.info(f"ğŸ‰ å‘ç°æ–°ç‰ˆæœ¬! ğŸ‰")
            logger.info(f"  - å½“å‰ç‰ˆæœ¬: {current_version}")
            logger.info(f"  - æœ€æ–°ç‰ˆæœ¬: {remote_version_str}")
            if download_and_extract_update(remote_version_str):
                logger.info("å‡†å¤‡åº”ç”¨æ›´æ–°ã€‚æœåŠ¡å™¨å°†åœ¨5ç§’åå…³é—­å¹¶å¯åŠ¨æ›´æ–°è„šæœ¬ã€‚")
                time.sleep(5)
                # å¯åŠ¨æ›´æ–°è„šæœ¬å¹¶åˆ†ç¦»
                update_script_path = os.path.join("modules", "update_script.py")
                subprocess.Popen([sys.executable, update_script_path])
                # å¹²å‡€åœ°é€€å‡ºå½“å‰ç¨‹åº
                sys.exit(0)
            else:
                logger.error(f"è‡ªåŠ¨æ›´æ–°å¤±è´¥ã€‚è¯·è®¿é—® https://github.com/{GITHUB_REPO}/releases/latest æ‰‹åŠ¨ä¸‹è½½ã€‚")
            logger.info("="*60)
        else:
            logger.info("æ‚¨çš„ç¨‹åºå·²æ˜¯æœ€æ–°ç‰ˆæœ¬ã€‚")

    except requests.RequestException as e:
        logger.error(f"æ£€æŸ¥æ›´æ–°å¤±è´¥: {e}")
    except json.JSONDecodeError:
        logger.error("è§£æè¿œç¨‹é…ç½®æ–‡ä»¶å¤±è´¥ã€‚")
    except Exception as e:
        logger.error(f"æ£€æŸ¥æ›´æ–°æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")


# --- API ç«¯ç‚¹ ---
@app.route('/update_models', methods=['POST'])
def update_models():
    html_content = request.data.decode('utf-8')
    if not html_content:
        return jsonify({"status": "error", "message": "No HTML content received."}), 400
    
    logger.info("æ”¶åˆ°æ¥è‡ªæ²¹çŒ´è„šæœ¬çš„é¡µé¢å†…å®¹ï¼Œå¼€å§‹æ£€æŸ¥å¹¶æ›´æ–°æ¨¡å‹...")
    new_models_list = extract_models_from_html(html_content)
    
    if new_models_list:
        compare_and_update_models(new_models_list, 'models.json')
        return jsonify({"status": "success", "message": "Model comparison and update complete."})
    else:
        return jsonify({"status": "error", "message": "Could not extract model data from HTML."}), 400

@app.route('/get_config', methods=['GET'])
def get_config():
    try:
        with open('config.jsonc', 'r', encoding='utf-8') as f:
            jsonc_content = f.read()
            json_content = re.sub(r'//.*', '', jsonc_content)
            json_content = re.sub(r'/\*.*?\*/', '', json_content, flags=re.DOTALL)
            return jsonify(json.loads(json_content))
    except Exception as e:
        logger.error(f"è¯»å–æˆ–è§£æ config.jsonc å¤±è´¥: {e}")
        return jsonify({"error": "Config file issue"}), 500

@app.route('/')
def index():
    return "LMArena è‡ªåŠ¨åŒ–å·¥å…· v12.2 (ä¸­æ–‡æœ¬åœ°åŒ–) æ­£åœ¨è¿è¡Œã€‚"

@app.route('/log_from_client', methods=['POST'])
def log_from_client():
    log_data = request.json
    if log_data and 'message' in log_data:
        logger.info(f"[æ²¹çŒ´è„šæœ¬] {log_data.get('level', 'INFO')}: {log_data['message']}")
    return jsonify({"status": "logged"})

# --- æ ¸å¿ƒé€»è¾‘ (æ— å˜åŒ–) ---
def convert_openai_to_lmarena_templates(openai_data: dict) -> dict:
    model_name = openai_data.get("model", "claude-3-5-sonnet-20241022")
    target_model_id = MODEL_NAME_TO_ID_MAP.get(model_name, DEFAULT_MODEL_ID)
    message_templates = []
    for oai_msg in openai_data["messages"]:
        message_templates.append({"role": oai_msg["role"], "content": oai_msg.get("content", "")})
    if CONFIG.get("bypass_enabled"):
        message_templates.append({"role": "user", "content": " "})
    message_templates.append({"role": "assistant", "content": ""})
    return {"message_templates": message_templates, "target_model_id": target_model_id}

@app.route('/get_messages_job', methods=['GET'])
def get_messages_job():
    try: return jsonify({"status": "success", "job": MESSAGES_JOBS.get_nowait()})
    except Empty: return jsonify({"status": "empty"})

@app.route('/get_prompt_job', methods=['GET'])
def get_prompt_job():
    try: return jsonify({"status": "success", "job": PROMPT_JOBS.get_nowait()})
    except Empty: return jsonify({"status": "empty"})

@app.route('/stream_chunk', methods=['POST'])
def stream_chunk():
    data = request.json
    task_id = data.get('task_id')
    if task_id in RESULTS:
        RESULTS[task_id]['stream_queue'].put(data.get('chunk'))
        return jsonify({"status": "success"})
    return jsonify({"status": "error"}), 404

@app.route('/report_result', methods=['POST'])
def report_result():
    data = request.json
    task_id = data.get('task_id')
    if task_id in RESULTS:
        RESULTS[task_id]['status'] = data.get('status', 'completed')
        logger.info(f"ä»»åŠ¡ {task_id[:8]} å·²è¢«å®¢æˆ·ç«¯æŠ¥å‘Šä¸ºå®Œæˆã€‚")
        return jsonify({"status": "success"})
    return jsonify({"status": "error"}), 404

def format_openai_chunk(content: str, model: str, request_id: str):
    return f"data: {json.dumps({'id': request_id, 'object': 'chat.completion.chunk', 'created': int(time.time()), 'model': model, 'choices': [{'index': 0, 'delta': {'content': content}, 'finish_reason': None}]})}\n\n"

def format_openai_finish_chunk(model: str, request_id: str, reason: str = 'stop'):
    return f"data: {json.dumps({'id': request_id, 'object': 'chat.completion.chunk', 'created': int(time.time()), 'model': model, 'choices': [{'index': 0, 'delta': {}, 'finish_reason': reason}]})}\n\ndata: [DONE]\n\n"

def format_openai_non_stream_response(content: str, model: str, request_id: str, reason: str = 'stop'):
    return {'id': request_id, 'object': 'chat.completion', 'created': int(time.time()), 'model': model, 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': content}, 'finish_reason': reason}], 'usage': {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}}

def _normalize_message_content(message: dict) -> dict:
    content = message.get("content")
    if isinstance(content, list):
        message["content"] = "\n\n".join([p.get("text", "") for p in content if isinstance(p, dict) and p.get("type") == "text"])
    return message

def _openai_response_generator(task_id: str):
    text_pattern = re.compile(r'a0:"((?:\\.|[^"\\])*)"')
    error_pattern = re.compile(r'(\{\s*"error".*?\})', re.DOTALL)
    finish_pattern = re.compile(r'"finishReason"\s*:\s*"(stop|content-filter)"')
    buffer = ""
    RESULTS[task_id]['finish_reason'] = None

    while True:
        try:
            raw_chunk = RESULTS[task_id]['stream_queue'].get(timeout=60)
            buffer += raw_chunk
            error_match = error_pattern.search(buffer)
            if error_match:
                try:
                    error_json = json.loads(error_match.group(1))
                    error_message = error_json.get("error", "æ¥è‡ª LMArena çš„æœªçŸ¥é”™è¯¯")
                    logger.error(f"ä»»åŠ¡ {task_id[:8]} çš„æµå¼å“åº”ä¸­æ£€æµ‹åˆ°é”™è¯¯: {error_message}")
                    RESULTS[task_id]['error'] = str(error_message)
                    return
                except json.JSONDecodeError: pass
            while True:
                match = text_pattern.search(buffer)
                if not match: break
                try:
                    text_content = json.loads(f'"{match.group(1)}"')
                    if text_content: yield text_content
                except json.JSONDecodeError: pass
                buffer = buffer[match.end():]
            
            finish_match = finish_pattern.search(raw_chunk)
            if finish_match:
                reason = finish_match.group(1)
                logger.info(f"æ£€æµ‹åˆ°ä»»åŠ¡ {task_id[:8]} çš„ LMArena æµç»“æŸä¿¡å·ï¼ŒåŸå› : {reason}ã€‚")
                RESULTS[task_id]['finish_reason'] = reason
                return
        except Empty:
            logger.warning(f"ä»»åŠ¡ {task_id[:8]} çš„ç”Ÿæˆå™¨è¶…æ—¶ã€‚")
            RESULTS[task_id]['error'] = 'æµå¼å“åº”åœ¨60ç§’åè¶…æ—¶ã€‚'
            return

def _load_config():
    global CONFIG
    try:
        with open('config.jsonc', 'r', encoding='utf-8') as f:
            CONFIG = json.loads(re.sub(r'/\*.*?\*/', '', re.sub(r'//.*', '', f.read()), flags=re.DOTALL))
    except Exception as e:
        logging.error(f"æ— æ³•åŠ è½½ config.jsonc: {e}ã€‚å°†ä½¿ç”¨é»˜è®¤è®¾ç½®ã€‚")
        CONFIG = {"enable_comprehensive_logging": False}

@app.route('/v1/models', methods=['GET'])
def list_models():
    return jsonify({"object": "list", "data": [{"id": name, "object": "model", "owned_by": "local-server"} for name in MODEL_NAME_TO_ID_MAP.keys()]})

@app.route('/v1/chat/completions', methods=['POST'])
def chat_completions():
    request_data = request.json
    if CONFIG.get("log_server_requests"):
        logger.info(f"--- æ”¶åˆ° OpenAI è¯·æ±‚ ---\n{json.dumps(request_data, indent=2, ensure_ascii=False)}")
    if not request_data or "messages" not in request_data: return jsonify({"error": "è¯·æ±‚å¿…é¡»åŒ…å« 'messages'"}), 400
    request_data["messages"] = [_normalize_message_content(msg) for msg in request_data.get("messages", [])]
    if not request_data["messages"]: return jsonify({"error": "'messages' åˆ—è¡¨ä¸èƒ½ä¸ºç©º"}), 400
    if CONFIG.get("tavern_mode_enabled"):
        system_prompts = [msg['content'] for msg in request_data["messages"] if msg['role'] == 'system']
        other_messages = [msg for msg in request_data["messages"] if msg['role'] != 'system']
        merged_system_prompt = "\n\n".join(system_prompts)
        final_messages = []
        if merged_system_prompt: final_messages.append({"role": "system", "content": merged_system_prompt})
        final_messages.extend(other_messages)
        request_data["messages"] = final_messages
    messages_job = convert_openai_to_lmarena_templates(request_data)
    MESSAGES_JOBS.put(messages_job)
    task_id = str(uuid.uuid4())
    prompt_job = {"task_id": task_id, "prompt": f"[è¿™æ¡æ¶ˆæ¯ä»…èµ·å ä½ï¼Œè¯·ä»¥å¤–éƒ¨åº”ç”¨ä¸­æ˜¾ç¤ºçš„å†…å®¹ä¸ºå‡†ï¼š/{task_id}]"}
    PROMPT_JOBS.put(prompt_job)
    RESULTS[task_id] = {"status": "pending", "stream_queue": Queue(), "error": None}
    model = request_data.get("model", "default")
    use_stream = request_data.get("stream", False)
    request_id = f"chatcmpl-{uuid.uuid4()}"
    if use_stream:
        def stream_response():
            for chunk in _openai_response_generator(task_id):
                yield format_openai_chunk(chunk, model, request_id)

            if RESULTS[task_id].get('error'):
                yield format_openai_chunk(f"[LMArena è‡ªåŠ¨åŒ–å·¥å…·é”™è¯¯]: {RESULTS[task_id]['error']}", model, request_id)
                yield format_openai_finish_chunk(model, request_id)
                return

            finish_reason = RESULTS[task_id].get('finish_reason')
            if finish_reason == 'content-filter':
                yield format_openai_chunk("\n\nå“åº”è¢«ç»ˆæ­¢ï¼Œå¯èƒ½æ˜¯ä¸Šä¸‹æ–‡è¶…é™æˆ–è€…æ¨¡å‹å†…éƒ¨å®¡æŸ¥çš„åŸå› ", model, request_id)
            
            # ç¡®ä¿å³ä½¿ finish_reason ä¸º Noneï¼Œä¹Ÿä¼ é€’ 'stop'
            yield format_openai_finish_chunk(model, request_id, reason=finish_reason or 'stop')
        return Response(stream_response(), mimetype='text/event-stream')
    else:
        full_response_content = "".join(list(_openai_response_generator(task_id)))
        if RESULTS[task_id].get('error'):
            return jsonify({"error": {"message": f"[LMArena è‡ªåŠ¨åŒ–å·¥å…·é”™è¯¯]: {RESULTS[task_id]['error']}", "type": "automator_error"}}), 500
        
        finish_reason = RESULTS[task_id].get('finish_reason', 'stop')
        if finish_reason == 'content-filter':
            full_response_content += "\n\nå“åº”è¢«ç»ˆæ­¢ï¼Œå¯èƒ½æ˜¯ä¸Šä¸‹æ–‡è¶…é™æˆ–è€…æ¨¡å‹å†…éƒ¨å®¡æŸ¥çš„åŸå› "
            
        return jsonify(format_openai_non_stream_response(full_response_content, model, request_id, reason=finish_reason))

if __name__ == '__main__':
    _load_config()
    if CONFIG.get("enable_comprehensive_logging"):
        log_dir = "Debug"
        os.makedirs(log_dir, exist_ok=True)
        log_filename = os.path.join(log_dir, f"debug_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log")
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', handlers=[logging.FileHandler(log_filename, encoding='utf-8'), logging.StreamHandler()])
        logger.info(f"èšåˆæ—¥å¿—å·²å¯ç”¨ã€‚æ—¥å¿—æ–‡ä»¶ä¿å­˜è‡³: {os.path.abspath(log_filename)}")
    else:
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s', handlers=[logging.StreamHandler()])
    
    load_model_map()
    
    # æ£€æŸ¥æ›´æ–°
    check_for_updates()

    logger.info("="*60)
    logger.info("  ğŸš€ LMArena è‡ªåŠ¨åŒ–å·¥å…· - v12.2 (ä¸­æ–‡æœ¬åœ°åŒ–)")
    logger.info(f"  - ç›‘å¬åœ°å€: http://127.0.0.1:5102")
    
    # ä½¿ç”¨ä¸€ä¸ªå­—å…¸æ¥æ˜ å°„é…ç½®é”®å’Œå®ƒä»¬çš„ä¸­æ–‡åç§°
    config_keys_in_chinese = {
        "enable_auto_update": "è‡ªåŠ¨æ›´æ–°",
        "bypass_enabled": "Bypass æ¨¡å¼",
        "tavern_mode_enabled": "é…’é¦†æ¨¡å¼",
        "log_server_requests": "æœåŠ¡å™¨è¯·æ±‚æ—¥å¿—",
        "log_tampermonkey_debug": "æ²¹çŒ´è„šæœ¬è°ƒè¯•æ—¥å¿—",
        "enable_comprehensive_logging": "èšåˆæ—¥å¿—"
    }
    
    logger.info("\n  å½“å‰é…ç½®:")
    for key, name in config_keys_in_chinese.items():
        status = 'âœ… å·²å¯ç”¨' if CONFIG.get(key) else 'âŒ å·²ç¦ç”¨'
        logger.info(f"  - {name}: {status}")
        
    logger.info("\n  è¯·åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ä¸€ä¸ª LMArena çš„ Direct Chat é¡µé¢ä»¥æ¿€æ´»æ²¹çŒ´è„šæœ¬ã€‚")
    logger.info("="*60)
    
    app.run(host='0.0.0.0', port=5102, threaded=True)